#!/usr/bin/env python3
"""
Test the Lemmatizer functionality with the installed SpaCy model
"""

import sys
import os

# Add path to modules
sys.path.append(os.path.abspath('.'))

def test_spacy_model_basic():
    """Test that SpaCy model works correctly"""
    print("üß™ Testing SpaCy Model Basic Functionality")
    print("=" * 60)
    
    try:
        import spacy
        
        # Load the model
        print("Loading ru_core_news_sm model...")
        nlp = spacy.load('ru_core_news_sm')
        print("‚úÖ Model loaded successfully")
        
        # Test basic functionality
        test_text = "–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π"
        doc = nlp(test_text)
        
        print(f"\nüìù Test text: '{test_text}'")
        print("Token analysis:")
        for token in doc:
            print(f"   '{token.text}' ‚Üí lemma: '{token.lemma_}', POS: {token.pos_}, is_stop: {token.is_stop}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå SpaCy model test failed: {e}")
        return False

def test_lemmatizer_with_spacy():
    """Test the Lemmatizer class with SpaCy model"""
    print("\nüß™ Testing Lemmatizer with SpaCy Model")
    print("=" * 60)
    
    try:
        from same.text_processing.lemmatizer import Lemmatizer, LemmatizerConfig
        
        # Create config with the small model
        lemmatizer_config = LemmatizerConfig(
            model_name="ru_core_news_sm",
            preserve_technical_terms=True,
            min_token_length=2
        )
        
        print("‚úÖ LemmatizerConfig created successfully")
        
        # Create lemmatizer instance
        lemmatizer = Lemmatizer(lemmatizer_config)
        print("‚úÖ Lemmatizer instance created successfully")
        
        # Test with sample data
        test_samples = [
            "–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π –∫—Ä–µ–ø–µ–∂–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç",
            "–î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–æ—â–Ω–æ—Å—Ç—å—é 4–∫–í—Ç —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–π",
            "–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è –±–µ—Å—à–æ–≤–Ω–∞—è –¥–∏–∞–º–µ—Ç—Ä–æ–º 57–º–º —Ç–æ–ª—â–∏–Ω–æ–π —Å—Ç–µ–Ω–∫–∏ 3.5–º–º",
            "–ö–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π –≥–∏–±–∫–∏–π –º–Ω–æ–≥–æ–∂–∏–ª—å–Ω—ã–π —Å–µ—á–µ–Ω–∏–µ–º 2.5–º–º¬≤ –≤ –∏–∑–æ–ª—è—Ü–∏–∏",
            "–ù–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –≤–æ–¥—è–Ω–æ–π –ø–æ–¥–∞—á–µ–π 12.5–º¬≥/—á –Ω–∞–ø–æ—Ä–æ–º 20–º"
        ]
        
        print("\nüìù Testing lemmatization with sample data:")
        
        for i, sample in enumerate(test_samples, 1):
            try:
                result = lemmatizer.lemmatize_text(sample)
                
                print(f"\n{i}. Original: '{sample}'")
                print(f"   Lemmatized: '{result['lemmatized']}'")
                print(f"   Tokens: {len(result['tokens'])}")
                print(f"   Filtered lemmas: {len(result['filtered_lemmas'])}")
                
                # Show some details
                if result['tokens']:
                    print(f"   All tokens: {result['tokens'][:6]}{'...' if len(result['tokens']) > 6 else ''}")
                if result['filtered_lemmas']:
                    print(f"   Key lemmas: {result['filtered_lemmas'][:6]}{'...' if len(result['filtered_lemmas']) > 6 else ''}")
                    
            except Exception as e:
                print(f"   ‚ùå Error lemmatizing sample {i}: {e}")
                return False
        
        print("\n‚úÖ All lemmatization tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Lemmatizer test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_notebook_cell_simulation():
    """Simulate the exact notebook cell execution"""
    print("\nüß™ Testing Notebook Cell Simulation")
    print("=" * 60)
    
    try:
        # Import exactly as in notebook
        from same.text_processing.lemmatizer import Lemmatizer, LemmatizerConfig
        
        print("üìö –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Lemmatizer")
        print("=" * 50)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞ (exactly as in notebook)
        lemmatizer_config = LemmatizerConfig(
            model_name="ru_core_news_sm",  # Using the small model
            preserve_technical_terms=True,
            min_token_length=2
        )

        lemmatizer = Lemmatizer(lemmatizer_config)

        # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (exactly as in notebook)
        lemmatization_samples = [
            "–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π –∫—Ä–µ–ø–µ–∂–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç",
            "–î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–æ—â–Ω–æ—Å—Ç—å—é 4–∫–í—Ç —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–π",
            "–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è –±–µ—Å—à–æ–≤–Ω–∞—è –¥–∏–∞–º–µ—Ç—Ä–æ–º 57–º–º —Ç–æ–ª—â–∏–Ω–æ–π —Å—Ç–µ–Ω–∫–∏ 3.5–º–º",
            "–ö–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π –≥–∏–±–∫–∏–π –º–Ω–æ–≥–æ–∂–∏–ª—å–Ω—ã–π —Å–µ—á–µ–Ω–∏–µ–º 2.5–º–º¬≤ –≤ –∏–∑–æ–ª—è—Ü–∏–∏",
            "–ù–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –≤–æ–¥—è–Ω–æ–π –ø–æ–¥–∞—á–µ–π 12.5–º¬≥/—á –Ω–∞–ø–æ—Ä–æ–º 20–º"
        ]

        print("\nüìù –ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:")
        lemmatization_results = []

        for i, sample in enumerate(lemmatization_samples, 1):
            result = lemmatizer.lemmatize_text(sample)
            lemmatization_results.append(result)
            
            print(f"\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π: '{sample}'")
            print(f"   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '{result['lemmatized']}'")
            print(f"   –¢–æ–∫–µ–Ω–æ–≤: {len(result['tokens'])}")
            print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–µ–º–º: {len(result['filtered_lemmas'])}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if result['tokens']:
                print(f"   –í—Å–µ —Ç–æ–∫–µ–Ω—ã: {result['tokens'][:8]}{'...' if len(result['tokens']) > 8 else ''}")
            if result['filtered_lemmas']:
                print(f"   –ö–ª—é—á–µ–≤—ã–µ –ª–µ–º–º—ã: {result['filtered_lemmas'][:6]}{'...' if len(result['filtered_lemmas']) > 6 else ''}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (exactly as in notebook)
        total_tokens = sum(len(r['tokens']) for r in lemmatization_results)
        total_filtered = sum(len(r['filtered_lemmas']) for r in lemmatization_results)
        reduction_ratio = (total_tokens - total_filtered) / total_tokens * 100 if total_tokens > 0 else 0

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(lemmatization_results)}")
        print(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ª–µ–º–º: {total_filtered}")
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {reduction_ratio:.1f}%")
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ: {total_tokens/len(lemmatization_results):.1f} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ: {total_filtered/len(lemmatization_results):.1f} –ª–µ–º–º")

        print("\n‚úÖ Lemmatizer —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Notebook simulation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_configuration_options():
    """Test different configuration options"""
    print("\nüß™ Testing Configuration Options")
    print("=" * 60)
    
    try:
        from same.text_processing.lemmatizer import LemmatizerConfig, Lemmatizer
        
        # Test different configurations
        configs = [
            {
                'name': 'Default (Small Model)',
                'config': LemmatizerConfig(model_name="ru_core_news_sm"),
                'test_text': '–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70'
            },
            {
                'name': 'Technical Terms Preserved',
                'config': LemmatizerConfig(
                    model_name="ru_core_news_sm",
                    preserve_technical_terms=True,
                    min_token_length=2
                ),
                'test_text': '–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70'
            },
            {
                'name': 'Technical Terms Not Preserved',
                'config': LemmatizerConfig(
                    model_name="ru_core_news_sm",
                    preserve_technical_terms=False,
                    min_token_length=2
                ),
                'test_text': '–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70'
            },
            {
                'name': 'Longer Min Token Length',
                'config': LemmatizerConfig(
                    model_name="ru_core_news_sm",
                    preserve_technical_terms=True,
                    min_token_length=4
                ),
                'test_text': '–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70'
            }
        ]
        
        print("üìã Testing different configuration scenarios:")
        
        for i, config_info in enumerate(configs, 1):
            config = config_info['config']
            test_text = config_info['test_text']
            
            print(f"\n{i}. {config_info['name']}")
            print(f"   preserve_technical_terms: {config.preserve_technical_terms}")
            print(f"   min_token_length: {config.min_token_length}")
            
            try:
                lemmatizer = Lemmatizer(config)
                result = lemmatizer.lemmatize_text(test_text)
                
                print(f"   Input: '{test_text}'")
                print(f"   Output: '{result['lemmatized']}'")
                print(f"   Filtered lemmas: {result['filtered_lemmas']}")
                
            except Exception as e:
                print(f"   ‚ùå Configuration test failed: {e}")
                return False
        
        print("\n‚úÖ All configuration tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Configuration options test failed: {e}")
        return False

def main():
    """Run all tests"""
    print("üöÄ Testing Lemmatizer with Installed SpaCy Model")
    print("=" * 70)
    
    tests = [
        ("SpaCy Model Basic", test_spacy_model_basic),
        ("Lemmatizer with SpaCy", test_lemmatizer_with_spacy),
        ("Notebook Cell Simulation", test_notebook_cell_simulation),
        ("Configuration Options", test_configuration_options)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*70}")
        print(f"üß™ Running: {test_name}")
        print(f"{'='*70}")
        
        result = test_func()
        results.append((test_name, result))
    
    # Final summary
    print(f"\n{'='*70}")
    print("üìä FINAL TEST RESULTS")
    print(f"{'='*70}")
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{status}: {test_name}")
        if result:
            passed += 1
    
    print(f"\nOverall: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("\nüéâ ALL TESTS PASSED!")
        print("\nüìù Summary:")
        print("   ‚úÖ SpaCy ru_core_news_sm model is installed and working")
        print("   ‚úÖ LemmatizerConfig creates successfully with correct parameters")
        print("   ‚úÖ Lemmatizer processes Russian text correctly")
        print("   ‚úÖ Technical terms are preserved when configured")
        print("   ‚úÖ Notebook cell simulation works perfectly")
        
        print("\nüöÄ The SAMe_Demo.ipynb Lemmatizer section is now fully functional!")
        print("\nüìã What works now:")
        print("   ‚Ä¢ SpaCy ru_core_news_sm model is installed")
        print("   ‚Ä¢ LemmatizerConfig with correct parameter names")
        print("   ‚Ä¢ Full lemmatization functionality for Russian MTR text")
        print("   ‚Ä¢ Technical term preservation (–ú10, –∫–í—Ç, –º–º, –ì–û–°–¢)")
        print("   ‚Ä¢ Automatic stopword and punctuation filtering")
        print("   ‚Ä¢ Configurable token length filtering")
        
        print("\nüí° Model comparison:")
        print("   ‚Ä¢ ru_core_news_sm: 15.3 MB, faster, good accuracy")
        print("   ‚Ä¢ ru_core_news_lg: 513.4 MB, slower, higher accuracy")
        print("   ‚Ä¢ For MTR processing, the small model should be sufficient")
        
        return True
    else:
        print(f"\n‚ùå {len(results) - passed} test(s) failed.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
