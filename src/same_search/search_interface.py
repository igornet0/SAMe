#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã SAMe

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

import sys
import pandas as pd
from pathlib import Path
import logging
from typing import List

# Add src directory to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from same_clear.search import TokenSearchEngine, SearchResult, SearchConfig
# from excel_processor import ExcelProcessor

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SAMeSearchInterface:
    """–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã SAMe"""
    
    def __init__(self, data_file: str = None, tokenizer_config: str = "vectorized_tokenizer"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –ø–æ–∏—Å–∫–∞
        
        Args:
            data_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            tokenizer_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        """
        self.data_file = data_file
        self.tokenizer_config = tokenizer_config
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.search_engine = None
        self.processor = None
        
        # –°—Ç–∞—Ç—É—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.is_ready = False
        
        logger.info("SAMe Search Interface initialized")
    
    def initialize(self, data_file: str = None) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        
        Args:
            data_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–ª–∏ —Ñ–∞–π–ª –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if data_file:
                self.data_file = data_file
            
            if not self.data_file:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
                possible_files = [
                    "test_final_improvements_output.csv",
                    "test_vectorization_fix_output.csv", 
                    "src/data/output/proccesed2.csv"
                ]
                
                for file_path in possible_files:
                    if Path(file_path).exists():
                        self.data_file = file_path
                        logger.info(f"Using data file: {file_path}")
                        break
                
                if not self.data_file:
                    logger.error("No data file found")
                    return False
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
            logger.info("Initializing processor and vectorizer...")
            logger.warning("Processor not available, skipping...")
            # self.processor = ExcelProcessor(tokenizer_config_name=self.tokenizer_config)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
            if not hasattr(self.processor.tokenizer, '_vectorizer') or self.processor.tokenizer._vectorizer is None:
                logger.error("Vectorizer not available in tokenizer")
                return False
            
            # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ –æ–±—É—á–∏—Ç—å
            if not self.processor.tokenizer._vectorizer.is_fitted:
                logger.info("Vectorizer not fitted, attempting to train...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                df = pd.read_csv(self.data_file)
                success = self.processor.train_vectorizer_on_data(df, sample_size=1000)
                
                if not success:
                    logger.error("Failed to train vectorizer")
                    return False
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
            config = SearchConfig(
                token_id_weight=0.6,
                semantic_weight=0.4,
                similarity_threshold=0.2,
                max_results=50
            )
            
            self.search_engine = TokenSearchEngine(config)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
            success = self.search_engine.load_data(
                self.data_file,
                self.processor.tokenizer._vectorizer,
                self.processor.tokenizer
            )
            
            if success:
                self.is_ready = True
                logger.info("‚úÖ Search interface ready!")
                return True
            else:
                logger.error("Failed to load data into search engine")
                return False
                
        except Exception as e:
            logger.error(f"Error initializing search interface: {e}")
            return False
    
    def search(self, query: str, method: str = "hybrid", top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ ('token_id', 'semantic', 'hybrid', 'extended_hybrid',
                   'advanced_hybrid', 'prefix', 'inverted_index', 'tfidf', 'lsh', 'spatial')
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.is_ready:
            logger.error("Search interface not ready. Call initialize() first.")
            return []
        
        return self.search_engine.search_by_tokens(query, method, top_k)
    
    def search_similar(self, reference_code: str, method: str = "semantic", top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –∫–æ–¥—É —Ç–æ–≤–∞—Ä–∞
        
        Args:
            reference_code: –ö–æ–¥ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö
            method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π
        """
        if not self.is_ready:
            logger.error("Search interface not ready")
            return []
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å—å –ø–æ –∫–æ–¥—É
        df = self.search_engine.data_df
        reference_row = df[df['–ö–æ–¥'] == reference_code]
        
        if reference_row.empty:
            logger.warning(f"Reference code not found: {reference_code}")
            return []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –∫–∞–∫ –∑–∞–ø—Ä–æ—Å
        reference_name = reference_row.iloc[0]['Raw_Name']
        logger.info(f"Searching for items similar to: {reference_name}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫, –∏—Å–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—É—é –∑–∞–ø–∏—Å—å
        results = self.search(reference_name, method, top_k + 1)
        
        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        filtered_results = [r for r in results if r.code != reference_code]
        
        return filtered_results[:top_k]
    
    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        if not self.is_ready:
            return {'status': 'not_ready'}
        
        return self.search_engine.get_search_stats()

    def search_by_method(self, query: str, method: str, top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            method: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.is_ready:
            logger.error("Search interface not ready")
            return []

        return self.search_engine.search_by_tokens(query, method, top_k)

    def benchmark_methods(self, query: str, top_k: int = 10) -> dict:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞
        """
        if not self.is_ready:
            logger.error("Search interface not ready")
            return {}

        import time

        methods = [
            'token_id', 'semantic', 'hybrid', 'extended_hybrid', 'advanced_hybrid',
            'prefix', 'inverted_index', 'tfidf', 'lsh', 'spatial'
        ]

        benchmark_results = {}

        for method in methods:
            try:
                start_time = time.time()
                results = self.search_by_method(query, method, top_k)
                end_time = time.time()

                benchmark_results[method] = {
                    'execution_time': end_time - start_time,
                    'results_count': len(results),
                    'top_score': results[0].score if results else 0.0,
                    'available': True
                }

            except Exception as e:
                benchmark_results[method] = {
                    'execution_time': 0.0,
                    'results_count': 0,
                    'top_score': 0.0,
                    'available': False,
                    'error': str(e)
                }

        return benchmark_results

    def get_method_recommendations(self, query_type: str = "general") -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞

        Args:
            query_type: –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ ('general', 'technical', 'prefix', 'similar')

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        recommendations = {
            'general': {
                'primary': 'advanced_hybrid',
                'alternatives': ['extended_hybrid', 'hybrid'],
                'description': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤'
            },
            'technical': {
                'primary': 'token_id',
                'alternatives': ['inverted_index', 'tfidf'],
                'description': '–ü–æ–∏—Å–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π'
            },
            'prefix': {
                'primary': 'prefix',
                'alternatives': ['trie', 'inverted_index'],
                'description': '–ü–æ–∏—Å–∫ –ø–æ –Ω–∞—á–∞–ª—É –Ω–∞–∑–≤–∞–Ω–∏—è –∏–ª–∏ –∞—Ä—Ç–∏–∫—É–ª–∞'
            },
            'similar': {
                'primary': 'semantic',
                'alternatives': ['lsh', 'spatial'],
                'description': '–ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤'
            },
            'fast': {
                'primary': 'spatial',
                'alternatives': ['lsh', 'inverted_index'],
                'description': '–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö'
            }
        }

        return recommendations.get(query_type, recommendations['general'])
    
    def print_results(self, results: List[SearchResult], show_details: bool = True):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            show_details: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏ (—Ç–æ–∫–µ–Ω—ã, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
        """
        if not results:
            print("üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
        print("=" * 80)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. üì¶ {result.code}")
            print(f"   üìù {result.raw_name}")
            print(f"   üìä –û—Ü–µ–Ω–∫–∞: {result.score:.3f} ({result.match_type})")
            
            if show_details:
                if result.tokens:
                    print(f"   üî§ –¢–æ–∫–µ–Ω—ã: {result.tokens}")
                
                if result.token_vectors:
                    print(f"   üî¢ –í–µ–∫—Ç–æ—Ä—ã: {result.token_vectors}")
                
                if result.parameters:
                    print(f"   ‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {result.parameters}")
                
                if hasattr(result, 'matched_tokens') and result.matched_tokens:
                    print(f"   ‚úÖ –°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {result.matched_tokens}")
                
                if hasattr(result, 'similarity_score') and result.similarity_score > 0:
                    print(f"   üéØ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {result.similarity_score:.3f}")

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                if hasattr(result, 'method_scores') and result.method_scores:
                    print(f"   üìà –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏:")
                    for method, score in result.method_scores.items():
                        if score > 0:
                            method_name = {
                                'exact_match': '–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ',
                                'partial_match': '–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ',
                                'semantic': '–°–µ–º–∞–Ω—Ç–∏–∫–∞',
                                'subset_match': '–ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞',
                            }.get(method, method)
                            print(f"      {method_name}: {score:.3f}")

                if hasattr(result, 'technical_boost') and result.technical_boost > 0:
                    print(f"   üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –±–æ–Ω—É—Å: {result.technical_boost:.3f}")

            print("-" * 80)


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –ø–æ–∏—Å–∫–∞"""
    
    print("üöÄ SAMe Search Interface Demo")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    search_interface = SAMeSearchInterface()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
    if not search_interface.initialize():
        print("‚ùå Failed to initialize search interface")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = search_interface.get_stats()
    print(f"\nüìä Search Engine Stats:")
    print(f"   Total records: {stats.get('total_records', 'N/A')}")
    print(f"   Unique tokens: {stats.get('unique_token_ids', 'N/A')}")
    print(f"   Embeddings available: {stats.get('embeddings_available', False)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã
    advanced_methods = stats.get('advanced_search_methods', {})
    print(f"\nüöÄ Advanced Search Methods:")
    for method, available in advanced_methods.items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"   {status} {method}")

    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
    test_queries = [
        ("—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫ LED 50W", "advanced_hybrid"),
        ("–∞–≤—Ç–æ–º–∞—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ê–í–î–¢", "token_id"),
        ("–±–æ–ª—Ç –ú10—Ö50 –ì–û–°–¢", "tfidf"),
        ("–∫—Ä–µ—Å–ª–æ –¥–ª—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è", "semantic")
    ]

    for query, method in test_queries:
        print(f"\nüîç –ü–æ–∏—Å–∫: '{query}' (–º–µ—Ç–æ–¥: {method})")
        print("-" * 60)

        results = search_interface.search(query, method=method, top_k=3)
        search_interface.print_results(results, show_details=False)

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    print(f"\n‚ö° Benchmark –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫ LED':")
    print("-" * 60)
    benchmark_results = search_interface.benchmark_methods("—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫ LED", top_k=5)

    for method, result in benchmark_results.items():
        if result['available']:
            print(f"   {method:15} | {result['execution_time']:.4f}s | {result['results_count']:2d} results | score: {result['top_score']:.3f}")
        else:
            print(f"   {method:15} | ‚ùå Not available")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞:")
    print("-" * 40)
    for query_type in ['general', 'technical', 'prefix', 'similar', 'fast']:
        rec = search_interface.get_method_recommendations(query_type)
        print(f"   {query_type:10}: {rec['primary']} ({rec['description']})")


if __name__ == "__main__":
    main()
