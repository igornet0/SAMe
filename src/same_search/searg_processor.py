import pandas as pd
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows
import asyncio

from src.same_api.export import ExcelExporter, ExportConfig
from src.same_search.categorization import CategoryClassifier, CategoryClassifierConfig
from src.same_clear.search import SearchResult, SearchConfig, TokenSearchEngine
from src.same_clear import PreprocessorConfig
from src.same_search.search_interface import SAMeSearchInterface
from src.same.analog_search_engine import AnalogSearchEngine
from src.same_search.hybrid_dbscan_search import HybridDBSCANSearchEngine, HybridDBSCANConfig
from src.same_search.optimized_dbscan_search import OptimizedHybridDBSCANSearchEngine, OptimizedDBSCANConfig

import logging

logger = logging.getLogger(__name__)


@dataclass
class AnalogSearchConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"""
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    preprocessor_config: PreprocessorConfig = None
    export_config: ExportConfig = None
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
    search_method: str = "hybrid"  
    similarity_threshold: float = 0.6
    max_results_per_query: int = 100
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    batch_size: int = 100
    enable_parameter_extraction: bool = True
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    data_dir: Path = Path("data")
    models_dir: Path = Path("models")
    output_dir: Path = Path("data/output")

class AnalogSearchProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"""

    def __init__(self, search_method: str = "hybrid", similarity_threshold: float = 0.6, 
                 use_extended_search: bool = True, max_excel_results: int = 1000000):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞

        Args:
            search_method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (fuzzy, semantic, hybrid, extended_hybrid, token_id,
                          prefix, inverted_index, tfidf, lsh, spatial, advanced_hybrid)
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_extended_search: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            max_excel_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ Excel
        """
        self.search_method = search_method
        self.similarity_threshold = similarity_threshold
        self.use_extended_search = use_extended_search
        self.max_excel_results = max_excel_results

        # –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
        self.input_format = 'unknown'

        # –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞
        self.extended_search_interface = None

        # –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        self.token_search_engine = None
        self.use_token_search = self._is_token_search_method(search_method)

        # Hybrid DBSCAN –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
        self.hybrid_dbscan_engine = None
        self.use_hybrid_dbscan = search_method == "hybrid_dbscan"

        # Optimized DBSCAN –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
        self.optimized_dbscan_engine = None
        self.use_optimized_dbscan = search_method == "optimized_dbscan"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ (–æ—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)
        self.preprocessor_config = PreprocessorConfig(
            save_intermediate_steps=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            batch_size=500,  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            enable_parallel_processing=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            max_workers=1,
            parallel_threshold=10000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥
            chunk_size=25  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        )
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        self.search_config = AnalogSearchConfig(
            preprocessor_config=self.preprocessor_config,
            search_method=search_method,
            similarity_threshold=similarity_threshold,
            max_results_per_query=50,
            batch_size=100,
            enable_parameter_extraction=True
        )

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        self.token_search_config = SearchConfig(
            # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            token_id_weight=0.6,
            semantic_weight=0.4,
            similarity_threshold=similarity_threshold,
            max_results=50,

            # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞
            enable_trie_search=True,
            trie_weight=0.3,
            trie_min_prefix_length=2,

            enable_inverted_index=True,
            inverted_index_weight=0.4,

            enable_tfidf_search=True,
            tfidf_weight=0.35,
            tfidf_max_features=10000,
            tfidf_ngram_range=(1, 3),

            enable_lsh_search=True,
            lsh_weight=0.25,
            lsh_threshold=0.6,
            lsh_num_perm=128,

            enable_spatial_search=True,
            spatial_weight=0.3,
            faiss_index_type="flat",

            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã (–æ—Ç–∫–ª—é—á–µ–Ω—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
            enable_advanced_embeddings=False,
            enable_graph_search=False,

            boost_technical_terms=True,
            enable_fuzzy_matching=True
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.category_classifier = CategoryClassifier(
            CategoryClassifierConfig(
                use_keyword_matching=True,
                use_pattern_matching=True,
                min_confidence=0.3
            )
        )
        
        self.search_engine = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
        if self.use_extended_search:
            logger.info("Initializing extended search system...")
            try:
                self.extended_search_interface = SAMeSearchInterface()
                logger.info("Extended search interface created")
            except Exception as e:
                logger.warning(f"Failed to initialize extended search: {e}")
                self.use_extended_search = False
                self.extended_search_interface = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –µ—Å–ª–∏ –Ω—É–∂–Ω–∞
        if self.use_token_search:
            logger.info("Initializing token search system...")
            try:
                self.token_search_engine = TokenSearchEngine(self.token_search_config)
                logger.info("Token search engine created")
            except Exception as e:
                logger.warning(f"Failed to initialize token search: {e}")
                self.use_token_search = False
                self.token_search_engine = None

        logger.info(f"AnalogSearchProcessor initialized (extended_search: {self.use_extended_search}, token_search: {self.use_token_search})")

    def _is_token_search_method(self, method: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –º–µ—Ç–æ–¥–æ–º –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            method: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞

        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        """
        token_search_methods = {
            'token_id', 'prefix', 'inverted_index', 'tfidf',
            'lsh', 'spatial', 'advanced_hybrid'
        }
        return method in token_search_methods

    def _is_hybrid_dbscan_method(self, method: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ hybrid DBSCAN –ø–æ–∏—Å–∫–æ–º"""
        return method == "hybrid_dbscan"

    def search_by_tokens(self, query: str, method: str = None, top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.search_method)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.token_search_engine:
            logger.error("Token search engine not initialized")
            return []

        search_method = method or self.search_method

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –¥–ª—è spatial –ø–æ–∏—Å–∫–∞
        if search_method == 'spatial':
            logger.info(f"üîç Spatial search diagnostics for query: '{query[:50]}...'")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ token_search_engine
            if hasattr(self.token_search_engine, 'faiss_index'):
                logger.info(f"  FAISS index available: {self.token_search_engine.faiss_index is not None}")
                if self.token_search_engine.faiss_index:
                    logger.info(f"  FAISS index vectors: {self.token_search_engine.faiss_index.ntotal}")

            if hasattr(self.token_search_engine, 'embeddings_matrix'):
                logger.info(f"  Embeddings matrix available: {self.token_search_engine.embeddings_matrix is not None}")
                if self.token_search_engine.embeddings_matrix is not None:
                    logger.info(f"  Embeddings matrix shape: {self.token_search_engine.embeddings_matrix.shape}")

            if hasattr(self.token_search_engine, 'vectorizer'):
                logger.info(f"  Vectorizer available: {self.token_search_engine.vectorizer is not None}")
                if self.token_search_engine.vectorizer:
                    logger.info(f"  Vectorizer fitted: {getattr(self.token_search_engine.vectorizer, 'is_fitted', False)}")

            if hasattr(self.token_search_engine, 'tokenizer'):
                logger.info(f"  Tokenizer available: {self.token_search_engine.tokenizer is not None}")

        try:
            results = self.token_search_engine.search_by_tokens(
                query=query,
                method=search_method,
                top_k=top_k
            )

            logger.info(f"Token search ({search_method}) found {len(results)} results for query: {query[:50]}...")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è spatial –ø–æ–∏—Å–∫–∞
            if search_method == 'spatial' and len(results) == 0:
                logger.warning(f"‚ö†Ô∏è Spatial search returned 0 results for query: '{query[:50]}...'")
                logger.warning("  This might indicate issues with:")
                logger.warning("  - FAISS index not properly initialized")
                logger.warning("  - Embeddings matrix is empty or None")
                logger.warning("  - Vectorizer not fitted or unavailable")
                logger.warning("  - Query embedding generation failed")

            return results

        except Exception as e:
            logger.error(f"Error in token search: {e}")
            import traceback
            traceback.print_exc()
            return []

    def batch_search_by_tokens(self, queries: List[str], method: str = None, top_k: int = 10) -> Dict[str, List[SearchResult]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.search_method)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∑–∞–ø—Ä–æ—Å: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤}
        """
        if not self.token_search_engine:
            logger.error("Token search engine not initialized")
            return {}

        search_method = method or self.search_method
        results = {}

        logger.info(f"Starting batch token search for {len(queries)} queries using method: {search_method}")

        for i, query in enumerate(queries):
            try:
                query_results = self.token_search_engine.search_by_tokens(
                    query=query,
                    method=search_method,
                    top_k=top_k
                )
                results[query] = query_results

                if (i + 1) % 100 == 0:
                    logger.info(f"Processed {i + 1}/{len(queries)} queries")

            except Exception as e:
                logger.error(f"Error in batch token search for query '{query}': {e}")
                results[query] = []

        logger.info(f"Batch token search completed. Processed {len(queries)} queries")
        return results

    def validate_input_csv(self, csv_path: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫

        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∞ CSV —Ñ–∞–π–ª–æ–≤:
        1. –ë–∞–∑–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (legacy): ['–ö–æ–¥', 'Raw_Name', 'Cleaned_Name', 'Lemmatized_Name', 'Normalized_Name', 'parameters', 'Dublikat']
        2. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (advanced): –≤—Å–µ –ø–æ–ª—è –∏–∑ excel_processor_advanced.py

        Args:
            csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É

        Returns:
            True –µ—Å–ª–∏ —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω, False –∏–Ω–∞—á–µ
        """
        # –ë–∞–∑–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–º–∏–Ω–∏–º—É–º –¥–ª—è —Ä–∞–±–æ—Ç—ã)
        basic_required_columns = ['–ö–æ–¥', 'Raw_Name', 'Normalized_Name']

        # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        advanced_columns = [
            '–ö–æ–¥', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ', '–ì—Ä—É–ø–ø–∞', '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', '–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è',
            'Raw_Name', 'Cleaned_Name', 'Lemmatized_Name', 'Normalized_Name', 'BPE_Tokens', 'BPE_Tokens_Count',
            'Semantic_Category', 'Technical_Complexity', 'Parameter_Confidence', 'Embedding_Similarity',
            'Advanced_Parameters', 'ML_Validated_Parameters', 'Colors_Found', 'Colors_Count',
            'Technical_Terms_Found', 'Technical_Terms_Count', 'Dublikat', 'Processing_Status'
        ]

        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ legacy —Ñ–æ—Ä–º–∞—Ç–∞
        legacy_columns = ['–ö–æ–¥', 'Raw_Name', 'Cleaned_Name', 'Lemmatized_Name',
                         'Normalized_Name', 'parameters', 'Dublikat']

        try:
            df = pd.read_csv(csv_path)
            available_columns = set(df.columns)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            missing_basic = [col for col in basic_required_columns if col not in available_columns]
            if missing_basic:
                logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–∞–∑–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_basic}")
                return False

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
            advanced_columns_present = sum(1 for col in advanced_columns if col in available_columns)
            legacy_columns_present = sum(1 for col in legacy_columns if col in available_columns)

            if advanced_columns_present >= len(advanced_columns) * 0.8:  # 80% –∫–æ–ª–æ–Ω–æ–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                self.input_format = 'advanced'
                logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CSV —Ñ–∞–π–ª–∞ ({advanced_columns_present}/{len(advanced_columns)} –∫–æ–ª–æ–Ω–æ–∫)")
            elif legacy_columns_present >= len(legacy_columns) * 0.8:  # 80% –∫–æ–ª–æ–Ω–æ–∫ legacy —Ñ–æ—Ä–º–∞—Ç–∞
                self.input_format = 'legacy'
                logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –±–∞–∑–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç CSV —Ñ–∞–π–ª–∞ ({legacy_columns_present}/{len(legacy_columns)} –∫–æ–ª–æ–Ω–æ–∫)")
            else:
                self.input_format = 'basic'
                logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CSV —Ñ–∞–π–ª–∞ (—Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏)")

            logger.info(f"CSV —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π, —Ñ–æ—Ä–º–∞—Ç: {self.input_format}")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV —Ñ–∞–π–ª–∞: {e}")
            return False
    
    async def initialize_search_engine(self, catalog_df: pd.DataFrame):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ —Å –∫–∞—Ç–∞–ª–æ–≥–æ–º –¥–∞–Ω–Ω—ã—Ö

        Args:
            catalog_df: DataFrame —Å –∫–∞—Ç–∞–ª–æ–≥–æ–º —Ç–æ–≤–∞—Ä–æ–≤
        """
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if self.use_extended_search and self.extended_search_interface:
            try:
                logger.info("Initializing extended search interface...")
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π CSV —Ñ–∞–π–ª –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ extended search
                temp_csv_for_extended = self._prepare_token_search_data(catalog_df)
                success = self.extended_search_interface.initialize(temp_csv_for_extended)

                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if Path(temp_csv_for_extended).exists():
                    Path(temp_csv_for_extended).unlink()

                if success:
                    logger.info("‚úÖ Extended search interface initialized successfully")
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                    if (hasattr(self.extended_search_interface, 'processor') and
                        self.extended_search_interface.processor and
                        hasattr(self.extended_search_interface.processor, 'tokenizer')):
                        tokenizer = self.extended_search_interface.processor.tokenizer
                        if hasattr(tokenizer, '_vectorizer') and tokenizer._vectorizer:
                            logger.info("‚úÖ Tokenizer and vectorizer available in extended search")
                        else:
                            logger.warning("‚ö†Ô∏è Vectorizer not available in extended search tokenizer")
                    else:
                        logger.warning("‚ö†Ô∏è Processor or tokenizer not available in extended search")
                else:
                    logger.warning("‚ùå Extended search interface initialization failed, falling back to legacy search")
                    self.use_extended_search = False
            except Exception as e:
                logger.error(f"Error initializing extended search: {e}")
                import traceback
                traceback.print_exc()
                self.use_extended_search = False

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        if self.use_token_search and self.token_search_engine:
            try:
                logger.info("Initializing token search engine...")
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π CSV —Ñ–∞–π–ª –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                temp_csv_path = self._prepare_token_search_data(catalog_df)

                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                tokenizer = None
                vectorizer = None

                if self.extended_search_interface and hasattr(self.extended_search_interface, 'processor'):
                    processor = self.extended_search_interface.processor
                    if processor and hasattr(processor, 'tokenizer'):
                        tokenizer = processor.tokenizer
                        logger.info("‚úÖ Tokenizer obtained from extended search interface")
                        if hasattr(tokenizer, '_vectorizer'):
                            vectorizer = tokenizer._vectorizer
                            if vectorizer and hasattr(vectorizer, 'is_fitted') and vectorizer.is_fitted:
                                logger.info("‚úÖ Vectorizer obtained and is fitted")
                            else:
                                logger.warning("‚ö†Ô∏è Vectorizer not fitted or unavailable")
                        else:
                            logger.warning("‚ö†Ô∏è Vectorizer not found in tokenizer")
                    else:
                        logger.warning("‚ö†Ô∏è Tokenizer not found in processor")
                else:
                    logger.warning("‚ö†Ô∏è Extended search interface or processor not available")

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –¥–≤–∏–∂–æ–∫ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–º
                logger.info(f"Loading token search data with tokenizer: {tokenizer is not None}, vectorizer: {vectorizer is not None}")
                success = self.token_search_engine.load_data(temp_csv_path, vectorizer, tokenizer)
                if success:
                    logger.info("‚úÖ Token search engine initialized successfully")
                else:
                    logger.warning("‚ùå Token search engine initialization failed")
                    self.use_token_search = False

                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if Path(temp_csv_path).exists():
                    Path(temp_csv_path).unlink()

            except Exception as e:
                logger.error(f"Error initializing token search: {e}")
                self.use_token_search = False

        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if len(catalog_df) > 10000:
            logger.info(f"–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç ({len(catalog_df)} –∑–∞–ø–∏—Å–µ–π), –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏")
            # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
            sample_size = min(5000, len(catalog_df))
            catalog_sample = catalog_df.sample(n=sample_size, random_state=42)
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ {sample_size} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏")
        else:
            catalog_sample = catalog_df

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Hybrid DBSCAN –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        if self.use_hybrid_dbscan:
            try:
                logger.info("Initializing Hybrid DBSCAN search engine...")
                dbscan_config = HybridDBSCANConfig(
                    eps=0.3,
                    min_samples=2,
                    similarity_threshold=self.similarity_threshold,
                    max_features=10000,
                    batch_size=min(1000, len(catalog_df) // 10)
                )
                self.hybrid_dbscan_engine = HybridDBSCANSearchEngine(dbscan_config)
                logger.info("‚úÖ Hybrid DBSCAN search engine initialized")
            except Exception as e:
                logger.error(f"Error initializing Hybrid DBSCAN engine: {e}")
                self.use_hybrid_dbscan = False
                self.hybrid_dbscan_engine = None

        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
        self.search_engine = AnalogSearchEngine(self.search_config)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Normalized_Name –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        catalog_for_search = catalog_sample.copy()
        catalog_for_search['name'] = catalog_sample['Normalized_Name'].fillna(catalog_sample['Raw_Name'])

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤–∏–∂–æ–∫ —Å –ø–æ–¥–≤—ã–±–æ—Ä–∫–æ–π
            await self.search_engine.initialize(catalog_for_search)
            logger.info("–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            self.full_catalog = catalog_df

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞: {e}")
            # –ü—Ä–æ–±—É–µ–º —Å –µ—â–µ –º–µ–Ω—å—à–µ–π –≤—ã–±–æ—Ä–∫–æ–π
            if len(catalog_sample) > 1000:
                logger.info("–ü—Ä–æ–±—É–µ–º —Å –º–µ–Ω—å—à–µ–π –≤—ã–±–æ—Ä–∫–æ–π (1000 –∑–∞–ø–∏—Å–µ–π)")
                smaller_sample = catalog_df.sample(n=1000, random_state=42)
                catalog_for_search = smaller_sample.copy()
                catalog_for_search['name'] = smaller_sample['Normalized_Name'].fillna(smaller_sample['Raw_Name'])
                await self.search_engine.initialize(catalog_for_search)
                self.full_catalog = catalog_df
                logger.info("–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–µ–Ω—å—à–µ–π –≤—ã–±–æ—Ä–∫–æ–π")
            else:
                raise

    def _prepare_token_search_data(self, catalog_df: pd.DataFrame) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            catalog_df: DataFrame —Å –∫–∞—Ç–∞–ª–æ–≥–æ–º

        Returns:
            –ü—É—Ç—å –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π DataFrame —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            token_search_df = catalog_df.copy()

            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ['–ö–æ–¥', 'Raw_Name']
            for col in required_columns:
                if col not in token_search_df.columns:
                    token_search_df[col] = ''

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞
            if self.input_format == 'advanced':
                # –î–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º Advanced_Parameters
                if 'Advanced_Parameters' in token_search_df.columns:
                    token_search_df['parameters'] = token_search_df['Advanced_Parameters'].fillna('')
                else:
                    token_search_df['parameters'] = ''
            elif self.input_format == 'legacy':
                # –î–ª—è legacy —Ñ–æ—Ä–º–∞—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–æ–Ω–∫—É parameters
                if 'parameters' not in token_search_df.columns:
                    token_search_df['parameters'] = ''
            else:
                # –î–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –∫–æ–ª–æ–Ω–∫—É parameters
                token_search_df['parameters'] = ''

            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            if 'tokenizer' not in token_search_df.columns:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º BPE_Tokens –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
                if self.input_format == 'advanced' and 'BPE_Tokens' in token_search_df.columns:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BPE —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ —Å–ø–∏—Å–æ–∫
                    def parse_bpe_tokens(tokens_str):
                        if pd.isna(tokens_str) or tokens_str == '':
                            return []
                        try:
                            # –£–±–∏—Ä–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
                            tokens_str = str(tokens_str).strip("[]'\"")
                            if tokens_str:
                                return [token.strip("'\" ") for token in tokens_str.split(',')]
                            return []
                        except:
                            return []

                    token_search_df['tokenizer'] = token_search_df['BPE_Tokens'].apply(parse_bpe_tokens)
                else:
                    # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞–∫ fallback
                    token_search_df['tokenizer'] = token_search_df['Normalized_Name'].fillna(
                        token_search_df['Raw_Name']
                    ).str.lower().str.split()

            if 'token_vectors' not in token_search_df.columns:
                # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
                token_search_df['token_vectors'] = ''

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_path = "temp_token_search_data.csv"
            token_search_df.to_csv(temp_path, index=False)

            logger.info(f"Prepared token search data: {len(token_search_df)} records, format: {self.input_format}")
            return temp_path

        except Exception as e:
            logger.error(f"Error preparing token search data: {e}")
            raise

    async def process_catalog(self, input_csv_path: str, output_excel_path: str = None, limit_records: int = None) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        
        Args:
            input_csv_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
            output_excel_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É Excel —Ñ–∞–π–ª—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit_records: –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É Excel —Ñ–∞–π–ª—É
        """
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞: {input_csv_path}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if not self.validate_input_csv(input_csv_path):
            raise ValueError("–í—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        catalog_df = pd.read_csv(input_csv_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(catalog_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if limit_records and limit_records < len(catalog_df):
            catalog_df = catalog_df.head(limit_records)
            logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {limit_records} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Optimized DBSCAN
        if self.use_optimized_dbscan:
            logger.info("Using Optimized DBSCAN search engine for processing...")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Optimized DBSCAN –¥–≤–∏–∂–æ–∫ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
            if not self.optimized_dbscan_engine:
                try:
                    logger.info("Initializing Optimized DBSCAN search engine...")
                    optimized_config = OptimizedDBSCANConfig(
                        eps=0.4,
                        min_samples=3,
                        similarity_threshold=self.similarity_threshold,
                        max_features=5000,
                        batch_size=max(1, min(500, len(catalog_df) // 20)),
                        use_sampling=True,
                        sample_size=min(5000, len(catalog_df)),
                        memory_limit_gb=4.0
                    )
                    self.optimized_dbscan_engine = OptimizedHybridDBSCANSearchEngine(optimized_config)
                    self.optimized_dbscan_engine.set_catalog(catalog_df)
                    logger.info("‚úÖ Optimized DBSCAN search engine initialized")
                except Exception as e:
                    logger.error(f"Error initializing Optimized DBSCAN engine: {e}")
                    # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –ø–æ–∏—Å–∫—É
                    self.use_optimized_dbscan = False
                    self.optimized_dbscan_engine = None

            if self.optimized_dbscan_engine:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–ª—è CSV
                if output_excel_path:
                    csv_output_path = output_excel_path.replace('.xlsx', '.csv').replace('.xls', '.csv')
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_output_path = f"optimized_dbscan_results_{timestamp}.csv"

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ —Å –ø–æ–º–æ—â—å—é Optimized DBSCAN
                logger.info("Starting optimized processing...")

                # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                all_results = []
                batch_size = self.optimized_dbscan_engine.config.batch_size

                for i in range(0, len(catalog_df), batch_size):
                    batch_end = min(i + batch_size, len(catalog_df))
                    logger.info(f"Processing batch: items {i}-{batch_end-1}")

                    batch_tasks = []
                    for idx in range(i, batch_end):
                        batch_tasks.append(self.optimized_dbscan_engine.search_analogs_optimized(idx, max_results=5))

                    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è –±–∞—Ç—á–∞
                    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

                    for idx, results in enumerate(batch_results):
                        if isinstance(results, Exception):
                            logger.error(f"Error processing item {i + idx}: {results}")
                            continue

                        all_results.extend(results)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                if all_results:
                    results_df = pd.DataFrame(all_results)
                    results_df.to_csv(csv_output_path, index=False, encoding='utf-8')
                    logger.info(f"Results saved to {csv_output_path}")

                logger.info(f"Optimized DBSCAN processing completed. Results saved to: {csv_output_path}")
                return csv_output_path

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Hybrid DBSCAN
        elif self.use_hybrid_dbscan:
            logger.info("Using Hybrid DBSCAN search engine for processing...")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Hybrid DBSCAN –¥–≤–∏–∂–æ–∫ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
            if not self.hybrid_dbscan_engine:
                try:
                    logger.info("Initializing Hybrid DBSCAN search engine...")
                    dbscan_config = HybridDBSCANConfig(
                        eps=0.3,
                        min_samples=2,
                        similarity_threshold=self.similarity_threshold,
                        max_features=10000,
                        batch_size=max(1, min(1000, len(catalog_df)))  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ batch_size >= 1
                    )
                    self.hybrid_dbscan_engine = HybridDBSCANSearchEngine(dbscan_config)
                    logger.info("‚úÖ Hybrid DBSCAN search engine initialized")
                except Exception as e:
                    logger.error(f"Error initializing Hybrid DBSCAN engine: {e}")
                    # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –ø–æ–∏—Å–∫—É
                    self.use_hybrid_dbscan = False
                    self.hybrid_dbscan_engine = None

            if self.hybrid_dbscan_engine:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–ª—è CSV
                if output_excel_path:
                    csv_output_path = output_excel_path.replace('.xlsx', '.csv').replace('.xls', '.csv')
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    csv_output_path = f"hybrid_dbscan_results_{timestamp}.csv"

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ —Å –ø–æ–º–æ—â—å—é Hybrid DBSCAN
                results_df = await self.hybrid_dbscan_engine.process_catalog(catalog_df, csv_output_path)

                logger.info(f"Hybrid DBSCAN processing completed. Results saved to: {csv_output_path}")
                return csv_output_path

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        await self.initialize_search_engine(catalog_df)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
        results = []
        total_items = len(catalog_df)

        logger.info(f"–ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è {total_items} –∑–∞–ø–∏—Å–µ–π...")

        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
        batch_size = 100 if total_items > 10000 else 10
        processed_count = 0

        for start_idx in range(0, total_items, batch_size):
            end_idx = min(start_idx + batch_size, total_items)
            batch_df = catalog_df.iloc[start_idx:end_idx]

            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {start_idx}-{end_idx} ({processed_count}/{total_items} –∑–∞–ø–∏—Å–µ–π, {processed_count/total_items*100:.1f}%)")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
            batch_results = await self._process_batch(batch_df, catalog_df)
            results.extend(batch_results)

            processed_count += len(batch_df)

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
            if processed_count % 1000 == 0:
                import gc
                gc.collect()
                logger.info(f"–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ {processed_count} –∑–∞–ø–∏—Å–µ–π")
        
        logger.info(f"–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ Excel —Ñ–∞–π–ª–∞
        if not output_excel_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_excel_path = f"analog_search_results_{timestamp}.xlsx"
        
        excel_path = await self._create_excel_output(results, output_excel_path)
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {excel_path}")
        return excel_path

    async def _process_batch(self, batch_df: pd.DataFrame, full_catalog_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

        Args:
            batch_df: –ë–∞—Ç—á –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            full_catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Å–∞–º–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –±–∞—Ç—á–∞
        """
        batch_results = []

        for idx, row in batch_df.iterrows():
            try:
                # –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–ø–∏—Å–∏
                item_results = await self._process_single_item(row, full_catalog_df)
                batch_results.extend(item_results)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø–∏—Å–∏ {idx}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—à–∏–±–∫–µ
                error_result = self._create_error_result(row, str(e))
                batch_results.append(error_result)

        return batch_results
    
    async def _process_single_item(self, row: pd.Series, catalog_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Å–∞–º–æ–π –∑–∞–ø–∏—Å–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_query = row['Normalized_Name'] if pd.notna(row['Normalized_Name']) else row['Raw_Name']

        if not search_query or search_query.strip() == '':
            return [self._create_error_result(row, "–ü—É—Å—Ç–æ–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")]

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            if self.use_token_search and self.token_search_engine:
                return await self._process_with_token_search(row, search_query, catalog_df)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            elif self.use_extended_search and self.extended_search_interface and self.extended_search_interface.is_ready:
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤
                if self.search_method == "token_id":
                    return await self._process_with_token_id_search(row, search_query)
                else:
                    return await self._process_with_extended_search(row, search_query, catalog_df)

            # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ –ø–æ–∏—Å–∫–∞
            return await self._process_with_legacy_search(row, search_query, catalog_df)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è {row.get('–ö–æ–¥', 'unknown')}: {e}")
            return [self._create_error_result(row, str(e))]

    async def _process_with_token_search(self, row: pd.Series, search_query: str, catalog_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            search_query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            search_results = self.search_by_tokens(
                query=search_query,
                method=self.search_method,
                top_k=15  # –ë–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            original_code = row['–ö–æ–¥']

            for result in search_results:
                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if result.code == original_code:
                    continue

                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è spatial –ø–æ–∏—Å–∫–∞ (TF-IDF —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–∞—é—Ç –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏)
                effective_threshold = self.similarity_threshold
                if self.search_method == 'spatial':
                    effective_threshold = max(0.05, self.similarity_threshold * 0.3)  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è spatial

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                if result.score < effective_threshold:
                    continue

                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                processed_result = self._format_token_search_result(row, result)
                processed_results.append(processed_result)

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if not processed_results:
                no_results = self._create_no_results_entry(row)
                no_results['Search_Engine'] = f"token_search_{self.search_method}"
                no_results['Comment'] = f"–ü–æ–∏—Å–∫ –ø–æ —Ç–æ–∫–µ–Ω–∞–º ({self.search_method}) –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
                processed_results.append(no_results)

            logger.debug(f"Token search ({self.search_method}) found {len(processed_results)} results for {original_code}")
            return processed_results

        except Exception as e:
            logger.error(f"Error in token search for {row.get('–ö–æ–¥', 'unknown')}: {e}")
            # Fallback –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if self.use_extended_search and self.extended_search_interface and self.extended_search_interface.is_ready:
                return await self._process_with_extended_search(row, search_query, catalog_df)
            # –ò–Ω–∞—á–µ fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ
            return await self._process_with_legacy_search(row, search_query, catalog_df)

    async def _process_with_extended_search(self, row: pd.Series, search_query: str, catalog_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            search_query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞
            if self.search_method == "token_id":
                search_method = "token_id"
            elif self.search_method in ["hybrid", "extended_hybrid"]:
                search_method = "extended_hybrid"
            else:
                search_method = self.search_method

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
            search_results = self.extended_search_interface.search(
                search_query,
                method=search_method,
                top_k=10
            )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            original_code = row['–ö–æ–¥']

            for result in search_results:
                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if result.code == original_code:
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                if result.score < self.similarity_threshold:
                    continue

                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                processed_result = self._format_extended_search_result(row, result)
                processed_results.append(processed_result)

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if not processed_results:
                no_results = self._create_no_results_entry(row)
                no_results['Search_Engine'] = f"extended_{search_method}"
                processed_results.append(no_results)

            logger.debug(f"Extended search found {len(processed_results)} results for {original_code}")
            return processed_results

        except Exception as e:
            logger.error(f"Error in extended search for {row.get('–ö–æ–¥', 'unknown')}: {e}")
            # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ
            return await self._process_with_legacy_search(row, search_query, catalog_df)

    async def _process_with_legacy_search(self, row: pd.Series, search_query: str, catalog_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞

        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            search_query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤
        if self.search_engine is None:
            logger.warning("–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
            return [self._create_simple_search_result(row, catalog_df)]

        search_results = await self.search_engine.search_analogs_async([search_query], self.search_method)
        query_results = search_results.get(search_query, [])

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        processed_results = []
        original_code = row['–ö–æ–¥']

        for result in query_results:
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ –∫–æ–¥—É)
            candidate_row_idx = result.get('document_id')
            if candidate_row_idx is not None and candidate_row_idx < len(catalog_df):
                candidate_row = catalog_df.iloc[candidate_row_idx]
                candidate_code = candidate_row.get('–ö–æ–¥', '')

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ —Ç–∞ –∂–µ —Å–∞–º–∞—è –∑–∞–ø–∏—Å—å
                if candidate_code == original_code:
                    continue

            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            processed_result = self._format_search_result(row, result, catalog_df)
            processed_results.append(processed_result)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not processed_results:
            no_results = self._create_no_results_entry(row)
            processed_results.append(no_results)

        return processed_results

    async def _process_with_token_id_search(self, row: pd.Series, search_query: str) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–∏—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            search_query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤
        """
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤
            search_results = self.extended_search_interface.search(
                search_query,
                method="token_id",
                top_k=15  # –ë–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            processed_results = []
            original_code = row['–ö–æ–¥']

            for result in search_results:
                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if result.code == original_code:
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥)
                token_id_threshold = max(0.1, self.similarity_threshold * 0.5)  # –ü–æ–ª–æ–≤–∏–Ω–∞ –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
                if result.score < token_id_threshold:
                    continue

                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                processed_result = self._format_token_id_search_result(row, result)
                processed_results.append(processed_result)

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if not processed_results:
                no_results = self._create_no_results_entry(row)
                no_results['Search_Engine'] = "token_id_search"
                no_results['Comment'] = "–ü–æ–∏—Å–∫ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
                processed_results.append(no_results)

            logger.debug(f"Token ID search found {len(processed_results)} results for {original_code}")
            return processed_results

        except Exception as e:
            logger.error(f"Error in token ID search for {row.get('–ö–æ–¥', 'unknown')}: {e}")
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—à–∏–±–∫–µ
            error_result = self._create_error_result(row, f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤: {str(e)}")
            error_result['Search_Engine'] = "token_id_search_error"
            return [error_result]

    def _format_token_search_result(self, original_row: pd.Series, search_result) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º

        Args:
            original_row: –ò—Å—Ö–æ–¥–Ω–∞—è –∑–∞–ø–∏—Å—å –∫–∞—Ç–∞–ª–æ–≥–∞
            search_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º (SearchResult)

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        if self.input_format == 'advanced' and 'Semantic_Category' in original_row and pd.notna(original_row['Semantic_Category']):
            original_category = original_row['Semantic_Category']
        else:
            original_category, _ = self.category_classifier.classify(original_row['Raw_Name'])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_score = search_result.score
        relation_type = self._determine_relation_type(similarity_score)

        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ –ø–æ–∏—Å–∫–∞
        search_details = []
        method_name = {
            'token_id': '–ü–æ–∏—Å–∫ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤',
            'prefix': '–ü—Ä–µ—Ñ–∏–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫',
            'inverted_index': '–ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å',
            'tfidf': 'TF-IDF –ø–æ–∏—Å–∫',
            'lsh': 'LSH –ø–æ–∏—Å–∫',
            'spatial': '–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫',
            'advanced_hybrid': '–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫'
        }.get(search_result.match_type, search_result.match_type)

        search_details.append(f"–ú–µ—Ç–æ–¥: {method_name}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö
        if hasattr(search_result, 'matched_tokens') and search_result.matched_tokens:
            try:
                if isinstance(search_result.matched_tokens, list):
                    tokens_str = ', '.join(str(token) for token in search_result.matched_tokens[:5])  # –ü–µ—Ä–≤—ã–µ 5 —Ç–æ–∫–µ–Ω–æ–≤
                    if len(search_result.matched_tokens) > 5:
                        tokens_str += f" (–∏ –µ—â–µ {len(search_result.matched_tokens) - 5})"
                    search_details.append(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {tokens_str}")
                else:
                    search_details.append(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {search_result.matched_tokens}")
            except Exception:
                search_details.append("–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º —Å—Ö–æ–¥—Å—Ç–≤–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if hasattr(search_result, 'similarity_score') and search_result.similarity_score > 0:
            search_details.append(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {search_result.similarity_score:.3f}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.input_format == 'advanced':
            if 'Technical_Complexity' in original_row and pd.notna(original_row['Technical_Complexity']):
                search_details.append(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {original_row['Technical_Complexity']}")
            if 'Colors_Found' in original_row and pd.notna(original_row['Colors_Found']) and original_row['Colors_Found']:
                search_details.append(f"–¶–≤–µ—Ç–∞: {original_row['Colors_Found']}")
            if 'Technical_Terms_Found' in original_row and pd.notna(original_row['Technical_Terms_Found']) and original_row['Technical_Terms_Found']:
                search_details.append(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: {original_row['Technical_Terms_Found']}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        comment = "; ".join(search_details)

        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            '–ö–æ–¥': original_row['–ö–æ–¥'],
            'Raw_Name': original_row['Raw_Name'],
            'Candidate_Name': search_result.raw_name,
            'Similarity_Score': round(similarity_score, 4),
            'Relation_Type': relation_type,
            'Suggested_Category': original_category,
            'Final_Decision': '',  # –ü—É—Å—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            'Comment': comment,
            'Original_Category': original_category,
            'Original_Code': original_row['–ö–æ–¥'],
            'Search_Engine': f"token_search_{search_result.match_type}"
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.input_format == 'advanced':
            additional_fields = {
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': original_row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', ''),
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ': original_row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ', ''),
                '–ì—Ä—É–ø–ø–∞': original_row.get('–ì—Ä—É–ø–ø–∞', ''),
                '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã': original_row.get('–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', ''),
                '–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è': original_row.get('–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è', ''),
                'Cleaned_Name': original_row.get('Cleaned_Name', ''),
                'Lemmatized_Name': original_row.get('Lemmatized_Name', ''),
                'Normalized_Name': original_row.get('Normalized_Name', ''),
                'BPE_Tokens': original_row.get('BPE_Tokens', ''),
                'BPE_Tokens_Count': original_row.get('BPE_Tokens_Count', ''),
                'Technical_Complexity': original_row.get('Technical_Complexity', ''),
                'Parameter_Confidence': original_row.get('Parameter_Confidence', ''),
                'Embedding_Similarity': original_row.get('Embedding_Similarity', ''),
                'Advanced_Parameters': original_row.get('Advanced_Parameters', ''),
                'ML_Validated_Parameters': original_row.get('ML_Validated_Parameters', ''),
                'Colors_Found': original_row.get('Colors_Found', ''),
                'Colors_Count': original_row.get('Colors_Count', ''),
                'Technical_Terms_Found': original_row.get('Technical_Terms_Found', ''),
                'Technical_Terms_Count': original_row.get('Technical_Terms_Count', ''),
                'Dublikat': original_row.get('Dublikat', ''),
                'Processing_Status': original_row.get('Processing_Status', '')
            }
            result.update(additional_fields)

        return result

    def _format_token_id_search_result(self, original_row: pd.Series, search_result) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            original_row: –ò—Å—Ö–æ–¥–Ω–∞—è –∑–∞–ø–∏—Å—å –∫–∞—Ç–∞–ª–æ–≥–∞
            search_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        original_category, _ = self.category_classifier.classify(original_row['Raw_Name'])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_score = search_result.score
        relation_type = self._determine_relation_type(similarity_score)

        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö
        token_info = []
        if hasattr(search_result, 'matched_token_ids') and search_result.matched_token_ids:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            token_ids_str = ', '.join(str(tid) for tid in search_result.matched_token_ids)
            token_info.append(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ ID —Ç–æ–∫–µ–Ω–æ–≤: {token_ids_str}")

        if hasattr(search_result, 'matched_tokens') and search_result.matched_tokens:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã - —Å—Ç—Ä–æ–∫–∏
            tokens_str = ', '.join(str(token) for token in search_result.matched_tokens)
            token_info.append(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {tokens_str}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏
        if hasattr(search_result, 'vector_distance'):
            token_info.append(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {search_result.vector_distance:.4f}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        comment_parts = ["–ü–æ–∏—Å–∫ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤"]
        if token_info:
            comment_parts.extend(token_info)

        comment = "; ".join(comment_parts)

        return {
            '–ö–æ–¥': original_row['–ö–æ–¥'],
            'Raw_Name': original_row['Raw_Name'],
            'Candidate_Name': search_result.raw_name,
            'Similarity_Score': round(similarity_score, 4),
            'Relation_Type': relation_type,
            'Suggested_Category': original_category,
            'Final_Decision': '',  # –ü—É—Å—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            'Comment': comment,  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
            'Original_Category': original_category,
            'Original_Code': original_row['–ö–æ–¥'],
            'Search_Engine': "token_id_search"
        }

    def _format_extended_search_result(self, original_row: pd.Series, search_result) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Ç—Ä–µ–±—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç

        Args:
            original_row: –ò—Å—Ö–æ–¥–Ω–∞—è –∑–∞–ø–∏—Å—å –∫–∞—Ç–∞–ª–æ–≥–∞
            search_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã (SearchResult)

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        original_category, _ = self.category_classifier.classify(original_row['Raw_Name'])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_score = search_result.score
        relation_type = self._determine_relation_type(similarity_score)

        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–∞—Ö –ø–æ–∏—Å–∫–∞
        search_details = []
        if hasattr(search_result, 'method_scores') and search_result.method_scores:
            for method, score in search_result.method_scores.items():
                if score > 0:
                    method_name = {
                        'exact_match': '–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ',
                        'partial_match': '–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ',
                        'semantic': '–°–µ–º–∞–Ω—Ç–∏–∫–∞',
                        'subset_match': '–ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞',
                        'token_id': '–ü–æ–∏—Å–∫ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤'
                    }.get(method, method)
                    search_details.append(f"{method_name}: {score:.3f}")

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ ID —Ç–æ–∫–µ–Ω–æ–≤
        if hasattr(search_result, 'match_type') and search_result.match_type == 'token_id':
            if hasattr(search_result, 'matched_token_ids') and search_result.matched_token_ids:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ ID —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏
                token_ids_str = ', '.join(str(tid) for tid in search_result.matched_token_ids)
                token_ids_info = f"ID —Ç–æ–∫–µ–Ω–æ–≤: {token_ids_str}"
                search_details.append(token_ids_info)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º –±–æ–Ω—É—Å–µ
        technical_info = ""
        if hasattr(search_result, 'technical_boost') and search_result.technical_boost > 0:
            technical_info = f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –±–æ–Ω—É—Å: {search_result.technical_boost:.3f}"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π
        comment_parts = []
        if search_details:
            comment_parts.append("–î–µ—Ç–∞–ª–∏: " + "; ".join(search_details))
        if technical_info:
            comment_parts.append(technical_info)
        if hasattr(search_result, 'matched_tokens') and search_result.matched_tokens:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            try:
                token_count = len(search_result.matched_tokens)
                comment_parts.append(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤: {token_count}")
            except (TypeError, AttributeError):
                comment_parts.append("–°–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

        comment = "; ".join(comment_parts) if comment_parts else "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫"

        return {
            '–ö–æ–¥': original_row['–ö–æ–¥'],
            'Raw_Name': original_row['Raw_Name'],
            'Candidate_Name': search_result.raw_name,
            'Similarity_Score': round(similarity_score, 4),
            'Relation_Type': relation_type,
            'Suggested_Category': original_category,
            'Final_Decision': '',  # –ü—É—Å—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            'Comment': comment,  # –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
            'Original_Category': original_category,
            'Original_Code': original_row['–ö–æ–¥'],
            'Search_Engine': f"extended_{search_result.match_type}"
        }

    def _format_search_result(self, original_row: pd.Series, search_result: Dict[str, Any],
                            catalog_df: pd.DataFrame) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞ –≤ —Ç—Ä–µ–±—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            original_row: –ò—Å—Ö–æ–¥–Ω–∞—è –∑–∞–ø–∏—Å—å –∫–∞—Ç–∞–ª–æ–≥–∞
            search_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –æ—Ç –¥–≤–∏–∂–∫–∞
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        candidate_row_idx = search_result.get('document_id')
        if candidate_row_idx is not None and candidate_row_idx < len(catalog_df):
            candidate_row = catalog_df.iloc[candidate_row_idx]
        else:
            candidate_row = pd.Series()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        original_category, _ = self.category_classifier.classify(original_row['Raw_Name'])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_score = search_result.get('similarity_score', search_result.get('combined_score', 0))
        relation_type = self._determine_relation_type(similarity_score)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
        search_engine = search_result.get('search_method', self.search_method)
        
        return {
            '–ö–æ–¥': original_row['–ö–æ–¥'],
            'Raw_Name': original_row['Raw_Name'],
            'Candidate_Name': candidate_row.get('Raw_Name', search_result.get('document', '')),
            'Similarity_Score': round(similarity_score, 4),
            'Relation_Type': relation_type,
            'Suggested_Category': original_category,
            'Final_Decision': '',  # –ü—É—Å—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            'Comment': '',  # –ü—É—Å—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            'Original_Category': original_category,
            'Original_Code': original_row['–ö–æ–¥'],
            'Search_Engine': search_engine
        }

    def _determine_relation_type(self, similarity_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        if similarity_score >= 0.9:
            return '–¢–æ—á–Ω—ã–π –∞–Ω–∞–ª–æ–≥'
        elif similarity_score >= 0.7:
            return '–ë–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥'
        elif similarity_score >= 0.5:
            return '–í–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'
        else:
            return '–°–ª–∞–±–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ'

    def _create_error_result(self, row: pd.Series, error_message: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ–± –æ—à–∏–±–∫–µ"""
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            '–ö–æ–¥': row['–ö–æ–¥'],
            'Raw_Name': row['Raw_Name'],
            'Candidate_Name': '',
            'Similarity_Score': 0.0,
            'Relation_Type': '–û—à–∏–±–∫–∞',
            'Suggested_Category': '',
            'Final_Decision': '',
            'Comment': f'–û—à–∏–±–∫–∞: {error_message}',
            'Original_Category': '',
            'Original_Code': row['–ö–æ–¥'],
            'Search_Engine': 'error'
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.input_format == 'advanced':
            additional_fields = {
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', ''),
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ': row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ', ''),
                '–ì—Ä—É–ø–ø–∞': row.get('–ì—Ä—É–ø–ø–∞', ''),
                '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã': row.get('–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', ''),
                '–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è': row.get('–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è', ''),
                'Cleaned_Name': row.get('Cleaned_Name', ''),
                'Lemmatized_Name': row.get('Lemmatized_Name', ''),
                'Normalized_Name': row.get('Normalized_Name', ''),
                'BPE_Tokens': row.get('BPE_Tokens', ''),
                'BPE_Tokens_Count': row.get('BPE_Tokens_Count', ''),
                'Technical_Complexity': row.get('Technical_Complexity', ''),
                'Parameter_Confidence': row.get('Parameter_Confidence', ''),
                'Embedding_Similarity': row.get('Embedding_Similarity', ''),
                'Advanced_Parameters': row.get('Advanced_Parameters', ''),
                'ML_Validated_Parameters': row.get('ML_Validated_Parameters', ''),
                'Colors_Found': row.get('Colors_Found', ''),
                'Colors_Count': row.get('Colors_Count', ''),
                'Technical_Terms_Found': row.get('Technical_Terms_Found', ''),
                'Technical_Terms_Count': row.get('Technical_Terms_Count', ''),
                'Dublikat': row.get('Dublikat', ''),
                'Processing_Status': row.get('Processing_Status', '')
            }
            result.update(additional_fields)

        return result

    def _create_no_results_entry(self, row: pd.Series) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if self.input_format == 'advanced' and 'Semantic_Category' in row and pd.notna(row['Semantic_Category']):
            original_category = row['Semantic_Category']
        else:
            original_category, _ = self.category_classifier.classify(row['Raw_Name'])

        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            '–ö–æ–¥': row['–ö–æ–¥'],
            'Raw_Name': row['Raw_Name'],
            'Candidate_Name': '',
            'Similarity_Score': 0.0,
            'Relation_Type': '–ê–Ω–∞–ª–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã',
            'Suggested_Category': original_category,
            'Final_Decision': '',
            'Comment': '–ê–Ω–∞–ª–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–∞—Ç–∞–ª–æ–≥–µ',
            'Original_Category': original_category,
            'Original_Code': row['–ö–æ–¥'],
            'Search_Engine': self.search_method
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.input_format == 'advanced':
            additional_fields = {
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', ''),
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ': row.get('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ', ''),
                '–ì—Ä—É–ø–ø–∞': row.get('–ì—Ä—É–ø–ø–∞', ''),
                '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã': row.get('–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', ''),
                '–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è': row.get('–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è', ''),
                'Cleaned_Name': row.get('Cleaned_Name', ''),
                'Lemmatized_Name': row.get('Lemmatized_Name', ''),
                'Normalized_Name': row.get('Normalized_Name', ''),
                'BPE_Tokens': row.get('BPE_Tokens', ''),
                'BPE_Tokens_Count': row.get('BPE_Tokens_Count', ''),
                'Technical_Complexity': row.get('Technical_Complexity', ''),
                'Parameter_Confidence': row.get('Parameter_Confidence', ''),
                'Embedding_Similarity': row.get('Embedding_Similarity', ''),
                'Advanced_Parameters': row.get('Advanced_Parameters', ''),
                'ML_Validated_Parameters': row.get('ML_Validated_Parameters', ''),
                'Colors_Found': row.get('Colors_Found', ''),
                'Colors_Count': row.get('Colors_Count', ''),
                'Technical_Terms_Found': row.get('Technical_Terms_Found', ''),
                'Technical_Terms_Count': row.get('Technical_Terms_Count', ''),
                'Dublikat': row.get('Dublikat', ''),
                'Processing_Status': row.get('Processing_Status', '')
            }
            result.update(additional_fields)

        return result

    def _create_simple_search_result(self, row: pd.Series, catalog_df: pd.DataFrame) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        (fallback –º–µ—Ç–æ–¥ –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)

        Args:
            row: –ò—Å—Ö–æ–¥–Ω–∞—è –∑–∞–ø–∏—Å—å –∫–∞—Ç–∞–ª–æ–≥–∞
            catalog_df: –ü–æ–ª–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞
        """
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏
            search_query = row['Normalized_Name'] if pd.notna(row['Normalized_Name']) else row['Raw_Name']
            original_code = row['–ö–æ–¥']

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            query_words = set(search_query.lower().split())

            best_match = None
            best_score = 0.0

            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Å–ª–æ–≤
            for _, candidate_row in catalog_df.iterrows():
                candidate_code = candidate_row.get('–ö–æ–¥', '')

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å
                if candidate_code == original_code:
                    continue

                candidate_name = candidate_row.get('Normalized_Name', candidate_row.get('Raw_Name', ''))
                if not candidate_name:
                    continue

                candidate_words = set(candidate_name.lower().split())

                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é —Å–ª–æ–≤
                intersection = query_words.intersection(candidate_words)
                union = query_words.union(candidate_words)

                if len(union) > 0:
                    score = len(intersection) / len(union)

                    if score > best_score and score >= self.similarity_threshold:
                        best_score = score
                        best_match = candidate_row

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            original_category, _ = self.category_classifier.classify(row['Raw_Name'])

            if best_match is not None:
                return {
                    '–ö–æ–¥': original_code,
                    'Raw_Name': row['Raw_Name'],
                    'Candidate_Name': best_match.get('Raw_Name', ''),
                    'Similarity_Score': round(best_score, 4),
                    'Relation_Type': self._determine_relation_type(best_score),
                    'Suggested_Category': original_category,
                    'Final_Decision': '',
                    'Comment': '–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Å—Ç—ã–º –ø–æ–∏—Å–∫–æ–º (fallback)',
                    'Original_Category': original_category,
                    'Original_Code': original_code,
                    'Search_Engine': 'simple_fallback'
                }
            
            else:
                return {
                    '–ö–æ–¥': original_code,
                    'Raw_Name': row['Raw_Name'],
                    'Candidate_Name': '',
                    'Similarity_Score': 0.0,
                    'Relation_Type': '–ê–Ω–∞–ª–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã',
                    'Suggested_Category': original_category,
                    'Final_Decision': '',
                    'Comment': '–ê–Ω–∞–ª–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä–æ—Å—Ç—ã–º –ø–æ–∏—Å–∫–æ–º',
                    'Original_Category': original_category,
                    'Original_Code': original_code,
                    'Search_Engine': 'simple_fallback'
                }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è {row.get('–ö–æ–¥', 'unknown')}: {e}")
            return self._create_error_result(row, f"–û—à–∏–±–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")

    async def _create_excel_output(self, results: List[Dict[str, Any]], output_path: str) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ Excel —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É

        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ Excel —Ñ–∞–π–ª–∞: {output_path}")

        # Excel –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (1,048,576)
        # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞, –ø–æ—ç—Ç–æ–º—É –º–∞–∫—Å–∏–º—É–º 1,048,575 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        EXCEL_MAX_ROWS = 1048575

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –ª–∏–º–∏—Ç–æ–º –∏ –ª–∏–º–∏—Ç–æ–º Excel
        effective_max_rows = min(self.max_excel_results, EXCEL_MAX_ROWS)

        total_results = len(results)
        if total_results > effective_max_rows:
            logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({total_results}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({effective_max_rows})")

            # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
            num_files = (total_results + effective_max_rows - 1) // effective_max_rows  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ {num_files} —Ñ–∞–π–ª–æ–≤")

            created_files = []
            for i in range(num_files):
                start_idx = i * effective_max_rows
                end_idx = min((i + 1) * effective_max_rows, total_results)

                # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –Ω–æ–º–µ—Ä–æ–º —á–∞—Å—Ç–∏
                if num_files > 1:
                    base_name = output_path.rsplit('.', 1)[0]
                    extension = output_path.rsplit('.', 1)[1] if '.' in output_path else 'xlsx'
                    part_output_path = f"{base_name}_part{i+1}.{extension}"
                else:
                    part_output_path = output_path

                # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —ç—Ç–æ–π —á–∞—Å—Ç–∏
                part_results = results[start_idx:end_idx]
                part_file = await self._create_single_excel_file(part_results, part_output_path)
                created_files.append(part_file)

                logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª {i+1}/{num_files}: {part_file} ({len(part_results)} –∑–∞–ø–∏—Å–µ–π)")

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É —Ñ–∞–π–ª—É
            return created_files[0]
        else:
            # –°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω —Ñ–∞–π–ª
            return await self._create_single_excel_file(results, output_path)

    async def _create_single_excel_file(self, results: List[Dict[str, Any]], output_path: str) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ Excel —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É

        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        df = pd.DataFrame(results)

        # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        wb = Workbook()
        ws = wb.active
        ws.title = "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if self.input_format == 'advanced':
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è advanced —Ñ–æ—Ä–º–∞—Ç–∞
            column_order = [
                # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
                '–ö–æ–¥', 'Raw_Name', 'Candidate_Name', 'Similarity_Score',
                'Relation_Type', 'Suggested_Category', 'Final_Decision', 'Comment',
                'Original_Category', 'Original_Code', 'Search_Engine',

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–ü–æ–ª–Ω–æ–µ', '–ì—Ä—É–ø–ø–∞', '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', '–ï–¥–∏–Ω–∏—Ü–∞–ò–∑–º–µ—Ä–µ–Ω–∏—è',
                'Cleaned_Name', 'Lemmatized_Name', 'Normalized_Name', 'BPE_Tokens', 'BPE_Tokens_Count',
                'Semantic_Category', 'Technical_Complexity', 'Parameter_Confidence', 'Embedding_Similarity',
                'Advanced_Parameters', 'ML_Validated_Parameters', 'Colors_Found', 'Colors_Count',
                'Technical_Terms_Found', 'Technical_Terms_Count', 'Dublikat', 'Processing_Status'
            ]
        else:
            # –ë–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è legacy –∏ basic —Ñ–æ—Ä–º–∞—Ç–æ–≤
            column_order = [
                '–ö–æ–¥', 'Raw_Name', 'Candidate_Name', 'Similarity_Score',
                'Relation_Type', 'Suggested_Category', 'Final_Decision', 'Comment',
                'Original_Category', 'Original_Code', 'Search_Engine'
            ]

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞–Ω–Ω—ã—Ö
        available_columns = [col for col in column_order if col in df.columns]

        # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º DataFrame —Ç–æ–ª—å–∫–æ –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        df = df.reindex(columns=available_columns)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ª–∏—Å—Ç
        for r in dataframe_to_rows(df, index=False, header=True):
            ws.append(r)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._apply_excel_formatting(ws, len(df))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        wb.save(output_path)

        logger.info(f"Excel —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_path}")
        return output_path

    def _apply_excel_formatting(self, worksheet, num_rows: int):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫ Excel –ª–∏—Å—Ç—É

        Args:
            worksheet: –õ–∏—Å—Ç Excel –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            num_rows: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        """
        # –°—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_alignment = Alignment(horizontal="center", vertical="center")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª–∏ –∫ –∑–∞–≥–æ–ª–æ–≤–∫—É
        for cell in worksheet[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter

            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass

            adjusted_width = min(max_length + 2, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
            worksheet.column_dimensions[column_letter].width = adjusted_width

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        worksheet.auto_filter.ref = worksheet.dimensions

