"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤ —Å —Å–∏—Å—Ç–µ–º–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ SAMe
"""

import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Set
from pathlib import Path
from collections import defaultdict, Counter
import re

from .category_classifier import CategoryClassifier, CategoryClassifierConfig

logger = logging.getLogger(__name__)


class CatalogIntegrator:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self, catalog_path: str):
        self.catalog_path = Path(catalog_path)
        self.df = None
        self.category_mapping = {}
        self.quality_issues = {}
        
    def load_catalog(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤"""
        try:
            logger.info(f"Loading catalog from {self.catalog_path}")
            self.df = pd.read_excel(self.catalog_path)
            logger.info(f"Loaded {len(self.df):,} products with {len(self.df.columns)} columns")
            return True
        except Exception as e:
            logger.error(f"Failed to load catalog: {e}")
            return False
    
    def analyze_category_columns(self) -> Dict[str, Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–ª–±—Ü–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –∫–∞—Ç–∞–ª–æ–≥–µ"""
        if self.df is None:
            return {}
        
        # –ü–æ–∏—Å–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_columns = []
        for col in self.df.columns:
            if any(keyword in col.lower() for keyword in ['–≥—Ä—É–ø–ø–∞', '–∫–∞—Ç–µ–≥–æ—Ä', '–≤–∏–¥', '—Ç–∏–ø', '–∫–ª–∞—Å—Å']):
                category_columns.append(col)
        
        analysis = {}
        
        for col in category_columns:
            unique_count = self.df[col].nunique()
            non_null_count = self.df[col].notna().sum()
            top_categories = self.df[col].value_counts().head(10)
            
            analysis[col] = {
                'unique_count': unique_count,
                'non_null_count': non_null_count,
                'coverage': non_null_count / len(self.df),
                'top_categories': top_categories.to_dict(),
                'distribution': self._analyze_distribution(self.df[col])
            }
            
            logger.info(f"Column '{col}': {unique_count} unique values, "
                       f"{non_null_count:,} non-null ({non_null_count/len(self.df)*100:.1f}%)")
        
        return analysis
    
    def _analyze_distribution(self, series: pd.Series) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Å—Ç–æ–ª–±—Ü–µ"""
        value_counts = series.value_counts()
        
        return {
            'most_common': value_counts.iloc[0] if len(value_counts) > 0 else 0,
            'least_common': value_counts.iloc[-1] if len(value_counts) > 0 else 0,
            'median_frequency': value_counts.median(),
            'categories_with_1_product': (value_counts == 1).sum(),
            'categories_with_100plus_products': (value_counts >= 100).sum()
        }
    
    def check_data_quality(self) -> Dict[str, any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        if self.df is None:
            return {}
        
        # –ü–æ–∏—Å–∫ —Å—Ç–æ–ª–±—Ü–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        name_columns = [col for col in self.df.columns 
                       if any(keyword in col.lower() for keyword in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'])]
        
        if not name_columns:
            logger.warning("No name column found")
            return {'error': 'No name column found'}
        
        name_col = name_columns[0]
        category_columns = [col for col in self.df.columns 
                           if any(keyword in col.lower() for keyword in ['–≥—Ä—É–ø–ø–∞', '–≤–∏–¥'])]
        
        quality_issues = {
            'duplicate_names': 0,
            'inconsistent_categorization': {},
            'empty_categories': 0,
            'examples': {}
        }
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏–π
        duplicate_names = self.df[self.df[name_col].duplicated(keep=False)]
        quality_issues['duplicate_names'] = len(duplicate_names)
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        for cat_col in category_columns:
            inconsistent_count = 0
            examples = []
            
            for name, group in duplicate_names.groupby(name_col):
                if group[cat_col].nunique() > 1:
                    inconsistent_count += 1
                    if len(examples) < 5:
                        categories = group[cat_col].unique()
                        examples.append((name, list(categories)))
            
            quality_issues['inconsistent_categorization'][cat_col] = {
                'count': inconsistent_count,
                'examples': examples
            }
        
        # –ê–Ω–∞–ª–∏–∑ –ø—É—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for cat_col in category_columns:
            empty_count = self.df[cat_col].isna().sum()
            quality_issues['empty_categories'] += empty_count
        
        self.quality_issues = quality_issues
        return quality_issues
    
    def search_ice_cleats_products(self) -> Dict[str, any]:
        """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Ç–∏–ø–∞ –ª–µ–¥–æ—Ö–æ–¥–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if self.df is None:
            return {}
        
        name_columns = [col for col in self.df.columns 
                       if any(keyword in col.lower() for keyword in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ'])]
        
        if not name_columns:
            return {'error': 'No name column found'}
        
        name_col = name_columns[0]
        
        # –ü–æ–∏—Å–∫ –ª–µ–¥–æ—Ö–æ–¥–æ–≤
        ice_cleats_mask = self.df[name_col].str.contains('–ª–µ–¥–æ—Ö–æ–¥', case=False, na=False)
        ice_cleats_products = self.df[ice_cleats_mask]
        
        # –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        specific_mask = self.df[name_col].str.contains(
            r'–ª–µ–¥–æ—Ö–æ–¥.*–ø—Ä–æ—Ñ.*10|–ª–µ–¥–æ—Ö–æ–¥.*10.*–ø—Ä–æ—Ñ', 
            case=False, na=False, regex=True
        )
        specific_products = self.df[specific_mask]
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        category_columns = [col for col in self.df.columns 
                           if any(keyword in col.lower() for keyword in ['–≥—Ä—É–ø–ø–∞', '–≤–∏–¥'])]
        
        categorization = {}
        for cat_col in category_columns:
            if len(ice_cleats_products) > 0:
                categories = ice_cleats_products[cat_col].value_counts()
                categorization[cat_col] = categories.to_dict()
        
        return {
            'total_ice_cleats': len(ice_cleats_products),
            'specific_matches': len(specific_products),
            'examples': ice_cleats_products[name_col].head(10).tolist() if len(ice_cleats_products) > 0 else [],
            'specific_examples': specific_products[name_col].tolist() if len(specific_products) > 0 else [],
            'categorization': categorization
        }
    
    def map_to_same_categories(self) -> Dict[str, Dict]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π SAMe"""
        if self.df is None:
            return {}
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ SAMe
        same_categories = {
            '—Å—Ä–µ–¥—Å—Ç–≤–∞_–∑–∞—â–∏—Ç—ã': ['–ª–µ–¥–æ—Ö–æ–¥', '–∫–∞—Å–∫–∞', '–ø–µ—Ä—á–∞—Ç–∫–∏', '—Å–∞–ø–æ–≥–∏', '–∂–∏–ª–µ—Ç', '–æ—á–∫–∏', '—Ä–µ—Å–ø–∏—Ä–∞—Ç–æ—Ä', '–º–∞—Å–∫–∞', '—à–ª–µ–º', '–∑–∞—â–∏—Ç–Ω'],
            '—Ö–∏–º–∏—è': ['—Å–æ–ª—å–≤–µ–Ω—Ç', '—Ä–∞—Å—Ç–≤–æ—Ä–∏—Ç–µ–ª—å', '–∫—Ä–∞—Å–∫–∞', '–ª–∞–∫', '–≥—Ä—É–Ω—Ç–æ–≤–∫–∞', '—ç–º–∞–ª—å', '–∫–ª–µ–π', '—Ö–∏–º–∏—á'],
            '–º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç': ['—à–≤–µ–ª–ª–µ—Ä', '—É–≥–æ–ª–æ–∫', '–±–∞–ª–∫–∞', '–ª–∏—Å—Ç', '—Ç—Ä—É–±–∞', '–ø—Ä–æ—Ñ–∏–ª—å', '–∞—Ä–º–∞—Ç—É—Ä–∞', '–º–µ—Ç–∞–ª–ª'],
            '–∫—Ä–µ–ø–µ–∂': ['–±–æ–ª—Ç', '–≥–∞–π–∫–∞', '–≤–∏–Ω—Ç', '—Å–∞–º–æ—Ä–µ–∑', '—à—É—Ä—É–ø', '–∑–∞–∫–ª–µ–ø–∫–∞', '–¥—é–±–µ–ª—å', '–∫—Ä–µ–ø–µ–∂'],
            '—ç–ª–µ–∫—Ç—Ä–∏–∫–∞': ['–∫–∞–±–µ–ª—å', '–ø—Ä–æ–≤–æ–¥', '—Ä–æ–∑–µ—Ç–∫–∞', '–≤—ã–∫–ª—é—á–∞—Ç–µ–ª—å', '–ª–∞–º–ø–∞', '—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫', '—ç–ª–µ–∫—Ç—Ä'],
            '—Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–∞': ['–∫—Ä–∞–Ω', '–≤–µ–Ω—Ç–∏–ª—å', '—Ñ–∏—Ç–∏–Ω–≥', '–Ω–∞—Å–æ—Å', '—Ñ–∏–ª—å—Ç—Ä', '—Å–º–µ—Å–∏—Ç–µ–ª—å', '—Å–∞–Ω—Ç–µ—Ö–Ω'],
            '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã': ['–æ—Ç–≤–µ—Ä—Ç–∫–∞', '–∫–ª—é—á', '–º–æ–ª–æ—Ç–æ–∫', '–ø–∏–ª–∞', '–¥—Ä–µ–ª—å', '–±–æ–ª–≥–∞—Ä–∫–∞', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç'],
            '—Ç–µ–∫—Å—Ç–∏–ª—å': ['—Ç–∫–∞–Ω—å', '–±—Ä–µ–∑–µ–Ω—Ç', '—Ç–µ–Ω—Ç', '–ø–æ–ª–æ–≥', '–º–µ—à–æ–∫', '–≤–µ—Ä–µ–≤–∫–∞', '—Ç–µ–∫—Å—Ç–∏–ª—å']
        }
        
        name_columns = [col for col in self.df.columns 
                       if any(keyword in col.lower() for keyword in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ'])]
        category_columns = [col for col in self.df.columns 
                           if any(keyword in col.lower() for keyword in ['–≥—Ä—É–ø–ø–∞', '–≤–∏–¥'])]
        
        if not name_columns:
            return {}
        
        name_col = name_columns[0]
        mapping_results = {}
        
        for cat_col in category_columns:
            category_mapping = {}
            unique_categories = self.df[cat_col].dropna().unique()
            
            for dataset_category in unique_categories:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–≤–∞—Ä—ã –∏–∑ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                category_products = self.df[self.df[cat_col] == dataset_category][name_col].dropna()
                
                if len(category_products) == 0:
                    continue
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
                best_match = None
                max_score = 0
                
                for same_cat, keywords in same_categories.items():
                    score = 0
                    for keyword in keywords:
                        matches = category_products.str.contains(keyword, case=False, na=False).sum()
                        score += matches
                    
                    if score > max_score:
                        max_score = score
                        best_match = same_cat
                
                if best_match and max_score > 0:
                    confidence = max_score / len(category_products)
                    category_mapping[dataset_category] = {
                        'same_category': best_match,
                        'confidence': confidence,
                        'matches': max_score,
                        'total_products': len(category_products)
                    }
            
            mapping_results[cat_col] = category_mapping
        
        self.category_mapping = mapping_results
        return mapping_results
    
    def generate_enhanced_classifier_rules(self) -> Dict[str, Set[str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–∞–ª–æ–≥–∞"""
        if self.df is None or not self.category_mapping:
            return {}
        
        enhanced_rules = defaultdict(set)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        name_columns = [col for col in self.df.columns 
                       if any(keyword in col.lower() for keyword in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ'])]
        
        if not name_columns:
            return {}
        
        name_col = name_columns[0]
        
        for cat_col, mappings in self.category_mapping.items():
            for dataset_cat, mapping_info in mappings.items():
                same_category = mapping_info['same_category']
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–≤–∞—Ä—ã –∏–∑ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                category_products = self.df[self.df[cat_col] == dataset_cat][name_col].dropna()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
                all_words = []
                for product_name in category_products:
                    words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', product_name.lower())
                    all_words.extend(words)
                
                # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ (–∏—Å–∫–ª—é—á–∞—è –æ–±—â–∏–µ)
                word_counts = Counter(all_words)
                common_words = {'–¥–ª—è', '–∏–ª–∏', '–ø–æ–¥', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø—Ä–æ', '–≤—Å–µ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '—Ç–æ—Ç'}
                
                for word, count in word_counts.most_common(10):
                    if word not in common_words and count >= 3:
                        enhanced_rules[same_category].add(word)
        
        return dict(enhanced_rules)
    
    def create_integration_report(self) -> Dict[str, any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if self.df is None:
            return {'error': 'Catalog not loaded'}
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã
        category_analysis = self.analyze_category_columns()
        quality_analysis = self.check_data_quality()
        ice_cleats_analysis = self.search_ice_cleats_products()
        mapping_analysis = self.map_to_same_categories()
        enhanced_rules = self.generate_enhanced_classifier_rules()
        
        # –û—Ü–µ–Ω–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏
        suitability_score = 0
        max_score = 5
        
        if len(category_analysis) > 0:
            suitability_score += 1
        
        if ice_cleats_analysis.get('total_ice_cleats', 0) > 0:
            suitability_score += 1
        
        if len(mapping_analysis) > 0:
            suitability_score += 1
        
        if len(self.df) > 1000:
            suitability_score += 1
        
        if quality_analysis.get('duplicate_names', 0) < len(self.df) * 0.1:
            suitability_score += 1
        
        return {
            'dataset_info': {
                'total_products': len(self.df),
                'columns': list(self.df.columns),
                'suitability_score': f"{suitability_score}/{max_score}"
            },
            'category_analysis': category_analysis,
            'quality_analysis': quality_analysis,
            'ice_cleats_analysis': ice_cleats_analysis,
            'mapping_analysis': mapping_analysis,
            'enhanced_rules': enhanced_rules,
            'recommendations': self._generate_recommendations(suitability_score)
        }
    
    def _generate_recommendations(self, suitability_score: int) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        recommendations = []
        
        if suitability_score >= 4:
            recommendations.extend([
                "‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                "üîß –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å enhanced_rules –≤ CategoryClassifier",
                "üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏",
                "üéØ –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ '–ª–µ–¥–æ—Ö–æ–¥'"
            ])
        elif suitability_score >= 3:
            recommendations.extend([
                "‚ö†Ô∏è –ö–∞—Ç–∞–ª–æ–≥ –ø–æ–¥—Ö–æ–¥–∏—Ç —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∞–º–∏",
                "üîß –û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–µ–π",
                "üìä –î–æ–ø–æ–ª–Ω–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Ä—É—á–Ω—É—é",
                "üéØ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
            ])
        else:
            recommendations.extend([
                "‚ùå –ö–∞—Ç–∞–ª–æ–≥ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏",
                "üîß –ü—Ä–æ–≤–µ—Å—Ç–∏ –≥–ª—É–±–æ–∫—É—é –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö",
                "üìä –°–æ–∑–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏",
                "üéØ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö"
            ])
        
        return recommendations
