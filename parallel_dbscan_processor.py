#!/usr/bin/env python3
"""
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å DBSCAN
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import pandas as pd
import numpy as np
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import re
from collections import Counter, defaultdict
import gc
import psutil
import os
from datetime import datetime
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import tempfile
import shutil

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def normalize_text_fast(text: str) -> str:
    """–ë—ã—Å—Ç—Ä–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not text or not isinstance(text, str):
        return ""
    
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = " ".join(filter(lambda x: len(x) > 2, text.split()))
    
    return text


def jaccard_similarity(text1: str, text2: str) -> float:
    """–ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ñ–∞–∫–∫–∞—Ä–∞"""
    if not text1 or not text2:
        return 0.0
    
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0


def determine_relation_type(similarity: float) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"""
    if similarity >= 0.9:
        return "–¥—É–±–ª—å"
    elif similarity >= 0.7:
        return "–∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.6:
        return "–±–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.4:
        return "–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.2:
        return "–ø–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä"
    else:
        return "–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤"


def create_word_index(df_chunk: pd.DataFrame, text_column: str) -> Dict[str, List[int]]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —á–∞–Ω–∫–∞"""
    word_index = defaultdict(list)
    
    for idx, row in df_chunk.iterrows():
        text = normalize_text_fast(str(row.get(text_column, '')))
        words = text.split()
        
        for word in words:
            word_index[word].append(idx)
    
    return dict(word_index)


def find_candidates_fast(query_text: str, word_index: Dict[str, List[int]], 
                        df_chunk: pd.DataFrame, max_candidates: int = 50) -> List[int]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
    query_words = normalize_text_fast(query_text).split()
    
    if not query_words:
        return []
    
    candidate_scores = Counter()
    
    for word in query_words:
        if word in word_index:
            for idx in word_index[word]:
                if idx < len(df_chunk):
                    candidate_scores[idx] += 1
    
    candidates = [idx for idx, score in candidate_scores.most_common(max_candidates)]
    return candidates


def process_chunk_parallel(args: Tuple[pd.DataFrame, int, str, float, int]) -> List[Dict[str, Any]]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    chunk_df, chunk_num, text_column, similarity_threshold, max_candidates = args
    
    # –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —á–∞–Ω–∫–∞
    word_index = create_word_index(chunk_df, text_column)
    
    results = []
    processed_count = 0
    
    for idx, row in chunk_df.iterrows():
        query_text = str(row.get(text_column, ''))
        
        if not query_text or query_text == 'nan':
            continue
        
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = find_candidates_fast(query_text, word_index, chunk_df, max_candidates)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        for candidate_idx in candidates:
            if candidate_idx == idx:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º —Ç–æ–≤–∞—Ä
                continue
            
            if candidate_idx >= len(chunk_df):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
                continue
            
            candidate_row = chunk_df.iloc[candidate_idx]
            candidate_text = str(candidate_row.get(text_column, ''))
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = jaccard_similarity(
                normalize_text_fast(query_text),
                normalize_text_fast(candidate_text)
            )
            
            if similarity >= similarity_threshold:
                relation_type = determine_relation_type(similarity)
                
                result = {
                    '–ö–æ–¥': row.get('–ö–æ–¥', ''),
                    'Raw_Name': row.get('Raw_Name', ''),
                    'Candidate_Name': candidate_row.get('Raw_Name', ''),
                    'Similarity_Score': round(similarity, 4),
                    'Relation_Type': relation_type,
                    'Suggested_Category': candidate_row.get('–ì—Ä—É–ø–ø–∞', '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'),
                    'Final_Decision': '',
                    'Comment': f"Chunk {chunk_num}; Parallel; –ñ–∞–∫–∫–∞—Ä: {similarity:.3f}",
                    'Original_Category': row.get('–ì—Ä—É–ø–ø–∞', ''),
                    'Candidate_–ö–æ–¥': candidate_row.get('–ö–æ–¥', ''),
                    'Original_Code': row.get('–ö–æ–¥', ''),
                    'Search_Engine': 'ParallelDBSCAN'
                }
                
                results.append(result)
        
        processed_count += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
        if processed_count % 1000 == 0:
            print(f"Chunk {chunk_num}: processed {processed_count}/{len(chunk_df)} records, found {len(results)} relationships")
    
    print(f"Chunk {chunk_num} completed: {len(results)} relationships found")
    return results


class ParallelDBSCANProcessor:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    def __init__(self, 
                 chunk_size: int = 8000,
                 similarity_threshold: float = 0.5,
                 max_candidates: int = 30,
                 n_workers: int = None,
                 overlap_size: int = 1000):
        
        self.chunk_size = chunk_size
        self.similarity_threshold = similarity_threshold
        self.max_candidates = max_candidates
        self.n_workers = n_workers or min(mp.cpu_count(), 8)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 8 –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        self.overlap_size = overlap_size  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        logger.info(f"ParallelDBSCANProcessor initialized:")
        logger.info(f"  Chunk size: {chunk_size}")
        logger.info(f"  Overlap size: {overlap_size}")
        logger.info(f"  Similarity threshold: {similarity_threshold}")
        logger.info(f"  Max candidates per query: {max_candidates}")
        logger.info(f"  Workers: {self.n_workers}")
    
    def create_overlapping_chunks(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è"""
        chunks = []
        total_records = len(df)
        
        for start_idx in range(0, total_records, self.chunk_size):
            # –û—Å–Ω–æ–≤–Ω–æ–π —á–∞–Ω–∫
            end_idx = min(start_idx + self.chunk_size, total_records)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
            overlap_start = max(0, start_idx - self.overlap_size // 2)
            overlap_end = min(total_records, end_idx + self.overlap_size // 2)
            
            chunk = df.iloc[overlap_start:overlap_end].copy().reset_index(drop=True)
            chunks.append(chunk)
        
        logger.info(f"Created {len(chunks)} overlapping chunks")
        return chunks
    
    def process_dataset_parallel(self, input_file: str, output_file: str, 
                               text_column: str = 'Normalized_Name') -> str:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        logger.info(f"üöÄ Starting parallel processing: {input_file}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info("Loading dataset...")
        df = pd.read_csv(input_file)
        total_records = len(df)
        logger.info(f"Loaded {total_records} records")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
        memory_gb = psutil.virtual_memory().available / (1024**3)
        logger.info(f"Available memory: {memory_gb:.1f}GB")
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
        chunks = self.create_overlapping_chunks(df)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        chunk_args = []
        for i, chunk in enumerate(chunks):
            args = (chunk, i + 1, text_column, self.similarity_threshold, self.max_candidates)
            chunk_args.append(args)
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        logger.info(f"Starting parallel processing with {self.n_workers} workers...")
        all_results = []
        
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏
            future_to_chunk = {
                executor.submit(process_chunk_parallel, args): i 
                for i, args in enumerate(chunk_args)
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            completed_chunks = 0
            for future in as_completed(future_to_chunk):
                chunk_num = future_to_chunk[future]
                try:
                    chunk_results = future.result()
                    all_results.extend(chunk_results)
                    completed_chunks += 1
                    
                    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    progress = completed_chunks / len(chunks) * 100
                    current_memory = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
                    
                    logger.info(f"Progress: {completed_chunks}/{len(chunks)} chunks ({progress:.1f}%), "
                               f"Memory: {current_memory:.1f}GB, "
                               f"Total results: {len(all_results)}")
                    
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_num}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∏–∑-–∑–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π)
        logger.info("Removing duplicate results from overlapping chunks...")
        results_df = pd.DataFrame(all_results)
        
        if not results_df.empty:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º
            before_dedup = len(results_df)
            results_df = results_df.drop_duplicates(
                subset=['–ö–æ–¥', 'Candidate_–ö–æ–¥', 'Similarity_Score'], 
                keep='first'
            )
            after_dedup = len(results_df)
            logger.info(f"Removed {before_dedup - after_dedup} duplicate results")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"Saving {len(results_df)} results to {output_file}")
        
        if not results_df.empty:
            results_df.to_csv(output_file, index=False, encoding='utf-8')
            logger.info(f"‚úÖ Results saved to: {output_file}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            relation_counts = results_df['Relation_Type'].value_counts()
            logger.info(f"üìä Results summary:")
            for relation_type, count in relation_counts.items():
                logger.info(f"  {relation_type}: {count}")
        else:
            logger.warning("No results found!")
        
        logger.info(f"üéâ Parallel processing completed!")
        return output_file


def main():
    parser = argparse.ArgumentParser(description='Parallel DBSCAN processor for large datasets')
    parser.add_argument('input_file', help='Input CSV file')
    parser.add_argument('-o', '--output', help='Output CSV file')
    parser.add_argument('-c', '--chunk-size', type=int, default=8000, help='Chunk size for processing')
    parser.add_argument('-t', '--threshold', type=float, default=0.5, help='Similarity threshold')
    parser.add_argument('--max-candidates', type=int, default=30, help='Max candidates per query')
    parser.add_argument('-j', '--workers', type=int, help='Number of parallel workers')
    parser.add_argument('--overlap', type=int, default=1000, help='Overlap size between chunks')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    if args.output:
        output_file = args.output
    else:
        input_path = Path(args.input_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"parallel_dbscan_results_{input_path.stem}_{timestamp}.csv"
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = ParallelDBSCANProcessor(
        chunk_size=args.chunk_size,
        similarity_threshold=args.threshold,
        max_candidates=args.max_candidates,
        n_workers=args.workers,
        overlap_size=args.overlap
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    result_file = processor.process_dataset_parallel(args.input_file, output_file)
    print(f"‚úÖ Parallel processing completed! Results saved to: {result_file}")


if __name__ == "__main__":
    main()
