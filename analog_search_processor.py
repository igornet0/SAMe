#!/usr/bin/env python3
"""
–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ SAMe

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ data/input/main_dataset.xlsx
2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ SAMe
3. –ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∞–Ω–∞–ª–æ–≥–∏
4. –°–æ–∑–¥–∞–µ—Ç Excel –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ analog_analysis_report

"""

import sys
import os
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import asyncio
from dataclasses import dataclass
import gc
import psutil
import time
import glob
import re

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.append('src')

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe
try:
    from same.text_processing.text_cleaner import TextCleaner, CleaningConfig
    from same.text_processing.lemmatizer import Lemmatizer, LemmatizerConfig
    from same.text_processing.normalizer import TextNormalizer, NormalizerConfig
    from same.text_processing.preprocessor import TextPreprocessor, PreprocessorConfig
    from same.search_engine.fuzzy_search import FuzzySearchEngine, FuzzySearchConfig
    from same.search_engine.semantic_search import SemanticSearchEngine, SemanticSearchConfig
    from same.search_engine.hybrid_search import HybridSearchEngine, HybridSearchConfig
    from same.analog_search_engine import AnalogSearchEngine, AnalogSearchConfig
    from same.data_manager.DataManager import DataManager
    from same.data_manager import data_helper
    print("‚úÖ –ú–æ–¥—É–ª–∏ SAMe —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π SAMe: {e}")
    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ src/same/")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(name)-45s:%(lineno)-3d - %(levelname)-7s - %(message)s',
    handlers=[
        logging.FileHandler('analog_search.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    total_records: int = 0
    processed_records: int = 0
    duplicates_found: int = 0
    analogs_found: int = 0
    processing_errors: int = 0
    start_time: datetime = None
    end_time: datetime = None
    memory_usage_mb: float = 0.0
    batch_count: int = 0
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    preloaded_duplicates: int = 0
    preloaded_analogs: int = 0
    preloaded_processed_names: int = 0
    existing_report_path: str = None


class AnalogSearchProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.data_input_path = Path("src/data/input/main_dataset.xlsx")
        self.data_output_dir = Path("src/data/output")
        self.data_output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.preprocessor = None
        self.search_engines = {}
        self.data = None
        self.processed_data = None
        self.stats = ProcessingStats()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.similarity_thresholds = {
            'duplicate': 0.95,  # –ü–æ—Ä–æ–≥ –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            'close_analog': 0.85,  # –ë–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥
            'analog': 0.70,  # –ê–Ω–∞–ª–æ–≥
            'possible_analog': 0.60,  # –í–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥
            'similar': 0.50  # –ü–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä
        }

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.batch_size = 1000  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.memory_limit_mb = 4000  # –õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ –≤ MB
        self.use_processed_cache = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
        self.analog_index = {}  # –ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤
        self.processed_pairs = set()  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –∞–Ω–∞–ª–æ–≥–æ–≤

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.preloaded_results = []  # –†–∞–Ω–µ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.preloaded_processed_names = set()  # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
        self.preloaded_analog_pairs = set()  # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∞–Ω–∞–ª–æ–≥–æ–≤
        self.load_existing_results = False  # –§–ª–∞–≥ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    def _get_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ MB"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0

    def _check_memory_limit(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –ø–∞–º—è—Ç–∏"""
        current_memory = self._get_memory_usage()
        self.stats.memory_usage_mb = current_memory
        return current_memory > self.memory_limit_mb

    def _cleanup_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        gc.collect()
        logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏: {self._get_memory_usage():.1f} MB")

    def _load_processed_data_if_exists(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        if not self.use_processed_cache:
            return False

        try:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            processed_files = list(self.data_output_dir.parent.glob("processed/processed_data_*.parquet"))
            processed_files.extend(list(self.data_output_dir.parent.glob("processed/processed_data_*.csv")))

            if processed_files:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                processed_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                latest_file = processed_files[0]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
                if latest_file.stat().st_size > 0:
                    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {latest_file}")

                    if latest_file.suffix == '.parquet':
                        self.processed_data = pd.read_parquet(latest_file)
                    else:
                        self.processed_data = pd.read_csv(latest_file)

                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.processed_data)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
                    return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {e}")

        return False

    def _save_processed_data(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        if not self.use_processed_cache or self.processed_data is None:
            return

        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞
            cache_dir = self.data_output_dir.parent / "processed"
            cache_dir.mkdir(parents=True, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            cache_file = cache_dir / f"processed_data_{timestamp}.parquet"

            self.processed_data.to_parquet(cache_file, index=False)
            logger.info(f"üíæ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à: {cache_file}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à: {e}")

    def _find_latest_report(self) -> Optional[Path]:
        """–ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ output"""
        try:
            # –ò—â–µ–º —Ñ–∞–π–ª—ã –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
            pattern = str(self.data_output_dir / "analog_analysis_report_*.xlsx")
            report_files = glob.glob(pattern)

            if not report_files:
                logger.info("üìÇ –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç—á–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return None

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–π - –ø–µ—Ä–≤—ã–π)
            report_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            latest_report = Path(report_files[0])

            logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á–µ—Ç: {latest_report.name}")
            return latest_report

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ç—á–µ—Ç–æ–≤: {e}")
            return None

    def _load_existing_results(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        if not self.load_existing_results:
            return False

        latest_report = self._find_latest_report()
        if not latest_report or not latest_report.exists():
            logger.info("üìÇ –ù–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return False

        try:
            logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {latest_report.name}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª
            existing_df = pd.read_excel(latest_report)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞
            required_columns = ['Raw_Name', 'Candidate_Name', 'Relation_Type', 'Similarity_Score']
            missing_columns = [col for col in required_columns if col not in existing_df.columns]

            if missing_columns:
                logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ: {missing_columns}")
                return False

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è—Ö
            processed_names = set()
            analog_pairs = set()

            for _, row in existing_df.iterrows():
                raw_name = str(row['Raw_Name']).strip()
                candidate_name = str(row['Candidate_Name']).strip()
                relation_type = str(row['Relation_Type']).strip()

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π
                processed_names.add(raw_name)

                # –î–ª—è –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä—ã (–∏—Å–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)
                if relation_type != '–¥—É–±–ª—å' and not candidate_name.startswith('–î–£–ë–õ–ò–ö–ê–¢:'):
                    processed_names.add(candidate_name)
                    analog_pairs.add((raw_name, candidate_name))
                    analog_pairs.add((candidate_name, raw_name))  # –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å

            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
            optimized_results = self._optimize_preloaded_duplicates(existing_df)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            self.preloaded_results = optimized_results
            self.preloaded_processed_names = processed_names
            self.preloaded_analog_pairs = analog_pairs

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats.preloaded_duplicates = len(existing_df[existing_df['Relation_Type'] == '–¥—É–±–ª—å'])
            self.stats.preloaded_analogs = len(existing_df[existing_df['Relation_Type'] != '–¥—É–±–ª—å'])
            self.stats.preloaded_processed_names = len(processed_names)
            self.stats.existing_report_path = str(latest_report)

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞:")
            logger.info(f"   üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(existing_df)}")
            logger.info(f"   üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats.preloaded_duplicates}")
            logger.info(f"   üîç –ê–Ω–∞–ª–æ–≥–æ–≤: {self.stats.preloaded_analogs}")
            logger.info(f"   üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π: {self.stats.preloaded_processed_names}")
            logger.info(f"   üîó –ü–∞—Ä –∞–Ω–∞–ª–æ–≥–æ–≤: {len(analog_pairs)//2}")  # –î–µ–ª–∏–º –Ω–∞ 2, —Ç.–∫. –ø–∞—Ä—ã –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ

            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return False

    def _optimize_preloaded_duplicates(self, existing_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π"""
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

        optimized_results = []
        processed_duplicate_names = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö Raw_Name –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –ù–ï –¥—É–±–ª–∏–∫–∞—Ç—ã
        non_duplicates = existing_df[existing_df['Relation_Type'] != '–¥—É–±–ª—å']
        for _, row in non_duplicates.iterrows():
            optimized_results.append(row.to_dict())

        # –ó–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å –ø–æ–ª–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –ø–æ Raw_Name
        duplicates = existing_df[existing_df['Relation_Type'] == '–¥—É–±–ª—å']

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ Raw_Name –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        duplicate_groups = duplicates.groupby('Raw_Name')

        for raw_name, group in duplicate_groups:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ
            group_size = len(group)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –≥—Ä—É–ø–ø–µ –∑–∞–ø–∏—Å–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ candidate_name
            unique_candidates = group['Candidate_Name'].unique()

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ —Å "–î–£–ë–õ–ò–ö–ê–¢: X –∑–∞–ø–∏—Å–∏", –∏–∑–≤–ª–µ–∫–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ
            max_count = group_size
            for candidate in unique_candidates:
                if candidate.startswith('–î–£–ë–õ–ò–ö–ê–¢:'):
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏ "–î–£–ë–õ–ò–ö–ê–¢: X –∑–∞–ø–∏—Å–∏"
                        import re
                        match = re.search(r'–î–£–ë–õ–ò–ö–ê–¢: (\d+)', candidate)
                        if match:
                            count = int(match.group(1))
                            max_count = max(max_count, count)
                    except:
                        pass

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
            if max_count > 1:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å –∏–∑ –≥—Ä—É–ø–ø—ã –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è
                representative_row = group.iloc[0]
                row_dict = representative_row.to_dict()

                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                row_dict['Candidate_Name'] = f"–î–£–ë–õ–ò–ö–ê–¢: {max_count} –∑–∞–ø–∏—Å–∏"
                row_dict['Comment'] = f'–¢–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç, –Ω–∞–π–¥–µ–Ω–æ {max_count} –∑–∞–ø–∏—Å–∏'
                row_dict['Similarity_Score'] = 1.0

                optimized_results.append(row_dict)
                processed_duplicate_names.add(raw_name)

                # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                if group_size > 1:
                    logger.debug(f"üîß –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ –≥—Ä—É–ø–ø–∞: {raw_name} ({group_size} -> 1 –∑–∞–ø–∏—Å—å, —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã: {max_count})")
            else:
                # –õ–û–ñ–ù–´–ô –î–£–ë–õ–ò–ö–ê–¢: –≠—Ç–æ –æ–¥–∏–Ω–æ—á–Ω–∞—è –∑–∞–ø–∏—Å—å, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç
                # –ò—Å–∫–ª—é—á–∞–µ–º –µ—ë –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                logger.debug(f"‚è≠Ô∏è –ò—Å–∫–ª—é—á–µ–Ω –ª–æ–∂–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç: {raw_name} (—Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã: {max_count})")
                # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –≤ optimized_results - —ç—Ç–∞ –∑–∞–ø–∏—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–∞–∫ –æ–±—ã—á–Ω–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

        original_count = len(existing_df)
        optimized_count = len(optimized_results)
        deduplicated_count = original_count - optimized_count

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        original_duplicates = len(duplicates)
        optimized_duplicates = len([r for r in optimized_results if r['Relation_Type'] == '–¥—É–±–ª—å'])
        false_duplicates_removed = original_duplicates - optimized_duplicates

        if deduplicated_count > 0 or false_duplicates_removed > 0:
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
            logger.info(f"   üìä –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {original_count}")
            logger.info(f"   üìä –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {optimized_count}")
            logger.info(f"   üîß –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–≤—Ç–æ—Ä–æ–≤: {deduplicated_count}")
            logger.info(f"   üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–æ: {original_duplicates} -> –ø–æ—Å–ª–µ: {optimized_duplicates}")
            if false_duplicates_removed > 0:
                logger.info(f"   ‚ùå –ò—Å–∫–ª—é—á–µ–Ω–æ –ª–æ–∂–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {false_duplicates_removed}")
        else:
            logger.info(f"‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è - –¥–∞–Ω–Ω—ã–µ —É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

        return optimized_results

    def _configure_engine_for_small_dataset(self, search_engine, num_documents: int):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ –¥–ª—è –º–∞–ª—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            if hasattr(search_engine, 'fuzzy_engine'):
                fuzzy_engine = search_engine.fuzzy_engine
                if hasattr(fuzzy_engine, 'config'):
                    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ TF-IDF –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    fuzzy_engine.config.tfidf_min_df = 1
                    fuzzy_engine.config.tfidf_max_df = 1.0  # –£–±–∏—Ä–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max_df
                    fuzzy_engine.config.cosine_threshold = 0.1  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
                    fuzzy_engine.config.tfidf_max_features = min(1000, num_documents * 50)
                    logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–µ–Ω –Ω–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è {num_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            if hasattr(search_engine, 'config'):
                if hasattr(search_engine.config, 'min_fuzzy_score'):
                    search_engine.config.min_fuzzy_score = 0.1
                if hasattr(search_engine.config, 'min_semantic_score'):
                    search_engine.config.min_semantic_score = 0.1

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            if hasattr(search_engine, 'semantic_engine'):
                semantic_engine = search_engine.semantic_engine
                if hasattr(semantic_engine, 'config'):
                    semantic_engine.config.similarity_threshold = 0.1
                    logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–µ–Ω —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è {num_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            return search_engine

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–≤–∏–∂–æ–∫ –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return search_engine

    def _simple_search_fallback(self, unique_data, processed_names: set, analog_pairs: set) -> List[Dict[str, Any]]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –ø–æ–∏—Å–∫ –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞...")

        analogs = []
        documents = unique_data['processed_name'].tolist()

        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        for idx, row in unique_data.iterrows():
            query = row['processed_name']

            if query in processed_names:
                continue

            best_match = None
            best_score = 0.0

            # –ò—â–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for other_idx, other_row in unique_data.iterrows():
                if idx == other_idx:
                    continue

                candidate = other_row['processed_name']
                if candidate in processed_names:
                    continue

                # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                from difflib import SequenceMatcher
                score = SequenceMatcher(None, query, candidate).ratio()

                if score > best_score and score > 0.6:  # –ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    best_score = score
                    best_match = other_row

            if best_match is not None:
                analog_info = {
                    'Raw_Name': row['Raw_Name'],
                    'Cleaned_Name': row['Cleaned_Name'],
                    'Lemmatized_Name': row['Lemmatized_Name'],
                    'Normalized_Name': row['Normalized_Name'],
                    'Candidate_Name': best_match['Raw_Name'],
                    'Similarity_Score': f"{best_score:.3f}",
                    'Relation_Type': '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥',
                    'Suggested_Category': best_match.get('–ì—Ä—É–ø–ø–∞', ''),
                    'Final_Decision': '–¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏',
                    'Comment': f'–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Å—Ç—ã–º –ø–æ–∏—Å–∫–æ–º (–º–∞–ª—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö), —Å—Ö–æ–∂–µ—Å—Ç—å: {best_score:.3f}',
                    'Original_Category': row.get('–ì—Ä—É–ø–ø–∞', ''),
                    'Original_Code': row.get('–ö–æ–¥', ''),
                    'Search_Engine': 'simple_fallback'
                }
                analogs.append(analog_info)
                processed_names.add(query)
                processed_names.add(best_match['processed_name'])

        logger.info(f"‚úÖ –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(analogs)} –∞–Ω–∞–ª–æ–≥–æ–≤")
        return analogs
        
    def initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ SAMe...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        preprocessor_config = PreprocessorConfig(
            save_intermediate_steps=True,
            enable_parallel_processing=True,
            max_workers=4
        )
        
        self.preprocessor = TextPreprocessor(preprocessor_config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
        try:
            # Fuzzy Search
            fuzzy_config = FuzzySearchConfig(
                similarity_threshold=0.6,
                fuzzy_threshold=60,
                max_candidates=100
            )
            self.search_engines['fuzzy'] = FuzzySearchEngine(fuzzy_config)

            # Semantic Search (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            try:
                semantic_config = SemanticSearchConfig(
                    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                    use_gpu=False,
                    similarity_threshold=0.5
                )
                self.search_engines['semantic'] = SemanticSearchEngine(semantic_config)
            except Exception as e:
                logger.warning(f"Semantic search –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

            # Hybrid Search
            if 'semantic' in self.search_engines:
                hybrid_config = HybridSearchConfig(
                    fuzzy_weight=0.4,
                    semantic_weight=0.6,
                    fuzzy_config=fuzzy_config,
                    semantic_config=semantic_config
                )
                self.search_engines['hybrid'] = HybridSearchEngine(hybrid_config)
            
            logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–≤–∏–∂–∫–∏: {list(self.search_engines.keys())}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤: {e}")
            raise
    
    def load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–∞"""
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {self.data_input_path}")
        
        if not self.data_input_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.data_input_path}")
        
        try:
            self.data = pd.read_excel(self.data_input_path)
            self.stats.total_records = len(self.data)
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(self.data.columns)}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏
            name_columns = [col for col in self.data.columns 
                          if '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' in col.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in col.lower()]
            
            if name_columns:
                self.main_name_column = name_columns[0]
            else:
                self.main_name_column = '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            
            logger.info(f"üìù –û—Å–Ω–æ–≤–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: '{self.main_name_column}'")
            
            return self.data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def preprocess_data(self) -> pd.DataFrame:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
        logger.info("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        self.stats.start_time = datetime.now()

        if self.data is None:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—ã–∑–æ–≤–∏—Ç–µ load_data() —Å–Ω–∞—á–∞–ª–∞.")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if self._load_processed_data_if_exists():
            self.stats.processed_records = len(self.processed_data)
            return self.processed_data

        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            total_records = len(self.data)
            processed_chunks = []

            logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_records} –∑–∞–ø–∏—Å–µ–π –±–∞—Ç—á–∞–º–∏ –ø–æ {self.batch_size}")

            for i in range(0, total_records, self.batch_size):
                batch_end = min(i + self.batch_size, total_records)
                batch_data = self.data.iloc[i:batch_end].copy()

                logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//self.batch_size + 1}/{(total_records + self.batch_size - 1)//self.batch_size} ({i+1}-{batch_end})")

                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
                processed_batch = self.preprocessor.preprocess_dataframe(
                    batch_data,
                    self.main_name_column,
                    output_columns={
                        'cleaned': 'Cleaned_Name',
                        'normalized': 'Normalized_Name',
                        'lemmatized': 'Lemmatized_Name',
                        'final': 'processed_name'
                    }
                )

                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–∞–∫ Raw_Name
                processed_batch['Raw_Name'] = processed_batch[self.main_name_column]

                processed_chunks.append(processed_batch)
                self.stats.batch_count += 1

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                if self._check_memory_limit():
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {self.stats.memory_usage_mb:.1f} MB")
                    self._cleanup_memory()

                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –±–∞—Ç—á–∞
                del batch_data

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏
            logger.info("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π...")
            self.processed_data = pd.concat(processed_chunks, ignore_index=True)

            # –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            del processed_chunks
            self._cleanup_memory()

            self.stats.processed_records = len(self.processed_data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._save_processed_data()

            logger.info(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(self.processed_data)} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {self._get_memory_usage():.1f} MB")

            return self.processed_data

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            self.stats.processing_errors += 1
            raise
    
    def find_duplicates(self) -> Tuple[List[Dict[str, Any]], Dict[str, List[int]]]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º"""
        logger.info("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")

        duplicates = []
        duplicate_groups = {}  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º
        grouped = self.processed_data.groupby('processed_name')

        for processed_name, group in grouped:
            if len(group) > 1:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä—É–ø–ø—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
                duplicate_groups[processed_name] = group.index.tolist()

                # –°–æ–∑–¥–∞–µ–º –û–î–ù–£ –∑–∞–ø–∏—Å—å –¥–ª—è –≤—Å–µ–π –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è –≥—Ä—É–ø–ø—ã
                representative_row = group.iloc[0]

                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –≤ –≥—Ä—É–ø–ø–µ
                all_names = group['Raw_Name'].tolist()

                duplicate_info = {
                    'index': representative_row.name,
                    'Raw_Name': representative_row['Raw_Name'],
                    'Cleaned_Name': representative_row['Cleaned_Name'],
                    'Lemmatized_Name': representative_row['Lemmatized_Name'],
                    'Normalized_Name': representative_row['Normalized_Name'],
                    'Candidate_Name': f"–î–£–ë–õ–ò–ö–ê–¢: {len(group)} –∑–∞–ø–∏—Å–∏",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: "–∑–∞–ø–∏—Å–∏" –≤–º–µ—Å—Ç–æ "–∑–∞–ø–∏—Å–µ–π"
                    'Similarity_Score': 1.000,
                    'Relation_Type': '–¥—É–±–ª—å',
                    'Suggested_Category': representative_row.get('–ì—Ä—É–ø–ø–∞', ''),
                    'Final_Decision': '–î—É–±–ª–∏–∫–∞—Ç - –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å',
                    'Comment': f'–¢–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç, –Ω–∞–π–¥–µ–Ω–æ {len(group)} –∑–∞–ø–∏—Å–∏',  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: "–∑–∞–ø–∏—Å–∏" –≤–º–µ—Å—Ç–æ "–∑–∞–ø–∏—Å–µ–π"
                    'Original_Category': representative_row.get('–ì—Ä—É–ø–ø–∞', ''),
                    'Original_Code': representative_row.get('–ö–æ–¥', ''),
                    'Search_Engine': 'exact_match',
                    'Duplicate_Count': len(group),
                    'All_Duplicate_Names': '; '.join(all_names[:5]) + (f' –∏ –µ—â–µ {len(all_names)-5}' if len(all_names) > 5 else '')
                }
                duplicates.append(duplicate_info)

        self.stats.duplicates_found = len(duplicates)
        total_duplicate_records = sum(len(group) for group in duplicate_groups.values())
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ({total_duplicate_records} –∑–∞–ø–∏—Å–µ–π)")

        return duplicates, duplicate_groups

    def search_analogs(self, exclude_duplicates: bool = True, duplicate_groups: Dict[str, List[int]] = None) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        logger.info("üîç –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤...")

        analogs = []
        processed_items = set()  # –î–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        processed_names = set(self.preloaded_processed_names)  # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
        analog_pairs = set(self.preloaded_analog_pairs)  # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∞–Ω–∞–ª–æ–≥–æ–≤

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if self.preloaded_processed_names:
            logger.info(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            logger.info(f"   üìù –ò—Å–∫–ª—é—á–µ–Ω–æ –∏–∑ –ø–æ–∏—Å–∫–∞: {len(self.preloaded_processed_names)} –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π")
            logger.info(f"   üîó –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ –ø–∞—Ä –∞–Ω–∞–ª–æ–≥–æ–≤: {len(self.preloaded_analog_pairs)//2}")  # –î–µ–ª–∏–º –Ω–∞ 2, —Ç.–∫. –ø–∞—Ä—ã –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        initial_data_size = len(self.processed_data)
        logger.info(f"üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   üìã –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ processed_data: {initial_data_size}")

        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–∏—Å–∫–ª—é—á–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if exclude_duplicates:
            unique_data = self.processed_data.drop_duplicates(subset=['processed_name'])
            logger.info(f"   üìã –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ processed_name: {len(unique_data)}")
        else:
            unique_data = self.processed_data
            logger.info(f"   üìã –ë–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_data)}")

        # –ò—Å–∫–ª—é—á–∞–µ–º –∑–∞–ø–∏—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
        if duplicate_groups:
            duplicate_indices = set()
            for group_indices in duplicate_groups.values():
                duplicate_indices.update(group_indices)

            before_duplicate_filter = len(unique_data)
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
            unique_data = unique_data[~unique_data.index.isin(duplicate_indices)]
            logger.info(f"   üìã –ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_data)} (–∏—Å–∫–ª—é—á–µ–Ω–æ {before_duplicate_filter - len(unique_data)})")
            logger.info(f"üîÑ –ò—Å–∫–ª—é—á–µ–Ω–æ {len(duplicate_indices)} –∑–∞–ø–∏—Å–µ–π-–¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤")

        # –ò—Å–∫–ª—é—á–∞–µ–º –∑–∞–ø–∏—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–∞—Ö
        if self.preloaded_processed_names:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ Raw_Name, –∏—Å–∫–ª—é—á–∞—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            before_filter = len(unique_data)
            unique_data = unique_data[~unique_data['Raw_Name'].isin(self.preloaded_processed_names)]
            after_filter = len(unique_data)
            excluded_count = before_filter - after_filter

            logger.info(f"   üìã –ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: {after_filter} (–∏—Å–∫–ª—é—á–µ–Ω–æ {excluded_count})")

            if excluded_count > 0:
                logger.info(f"üîÑ –ò—Å–∫–ª—é—á–µ–Ω–æ {excluded_count} –∑–∞–ø–∏—Å–µ–π, —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–∞—Ö")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            if excluded_count > before_filter * 0.9:  # –ï—Å–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–æ –±–æ–ª–µ–µ 90%
                logger.warning(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ò—Å–∫–ª—é—á–µ–Ω–æ {excluded_count}/{before_filter} ({excluded_count/before_filter*100:.1f}%) –∑–∞–ø–∏—Å–µ–π!")
                logger.warning(f"   –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π")

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞
        min_documents_required = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

        if len(unique_data) < min_documents_required:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤: {len(unique_data)} –∑–∞–ø–∏—Å–µ–π")
            logger.warning(f"   –ú–∏–Ω–∏–º—É–º —Ç—Ä–µ–±—É–µ—Ç—Å—è: {min_documents_required} –∑–∞–ø–∏—Å–µ–π")

            if len(unique_data) == 0:
                logger.info("‚úÖ –í—Å–µ –∑–∞–ø–∏—Å–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–∞—Ö")
                return []

            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –Ω–æ –æ–Ω–∏ –µ—Å—Ç—å - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            logger.info(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(unique_data)} –∑–∞–ø–∏—Å–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é")

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–≤–∏–∂–æ–∫
        search_engine_name = self._get_best_search_engine()
        search_engine = self.search_engines[search_engine_name]

        logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤–∏–∂–æ–∫: {search_engine_name}")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        documents = unique_data['processed_name'].tolist()

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ –¥–ª—è –º–∞–ª—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        if len(documents) < 50:
            logger.info(f"üîß –ú–∞–ª—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ({len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) - –ø—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
            search_engine = self._configure_engine_for_small_dataset(search_engine, len(documents))

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...")
            if hasattr(search_engine, 'fit'):
                search_engine.fit(documents)
            elif hasattr(search_engine, 'build_index'):
                search_engine.build_index(documents)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–≤–∏–∂–∫–∞: {e}")
            logger.error(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
            logger.error(f"   –¢–∏–ø –¥–≤–∏–∂–∫–∞: {type(search_engine).__name__}")

            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback –¥–≤–∏–∂–æ–∫ –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if len(documents) < 10:
                logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –º–∞–ª–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö...")
                return self._simple_search_fallback(unique_data, processed_names, analog_pairs)

            return []

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        total_records = len(unique_data)
        batch_size = min(self.batch_size, 500)  # –ú–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –ø–æ–∏—Å–∫–∞

        logger.info(f"üìä –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è {total_records} –∑–∞–ø–∏—Å–µ–π –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}")

        for batch_start in range(0, total_records, batch_size):
            batch_end = min(batch_start + batch_size, total_records)
            batch_data = unique_data.iloc[batch_start:batch_end]

            logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –ø–æ–∏—Å–∫–∞ {batch_start//batch_size + 1}/{(total_records + batch_size - 1)//batch_size} ({batch_start+1}-{batch_end})")

            # –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
            for idx, row in batch_data.iterrows():
                if idx in processed_items:
                    continue

                query = row['processed_name']

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                if query in processed_names:
                    continue

                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                    results = search_engine.search(query, top_k=10)

                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    if not results:
                        logger.debug(f"–ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query[:30]}...'")
                        continue

                    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    validated_results = self._validate_search_results(results, query)
                    if not validated_results:
                        logger.debug(f"–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è '{query[:30]}...'")
                        continue

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É—á–µ—Ç–æ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤
                    best_analog = self._process_search_results_optimized(
                        row, validated_results, unique_data, search_engine_name, processed_names, analog_pairs
                    )

                    if best_analog:
                        analogs.append(best_analog)
                        processed_items.add(idx)
                        processed_names.add(query)

                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä—É –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        candidate_name = best_analog.get('processed_candidate_name')
                        if candidate_name:
                            analog_pairs.add((query, candidate_name))
                            analog_pairs.add((candidate_name, query))  # –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
                            processed_names.add(candidate_name)  # –ò—Å–∫–ª—é—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏–∑ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø–æ–∏—Å–∫–∞

                except KeyError as e:
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è KeyError (–≤–∫–ª—é—á–∞—è 'hybrid_score')
                    logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á {e} –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query[:50]}...'. –î–≤–∏–∂–æ–∫: {search_engine_name}")
                    self.stats.processing_errors += 1
                    continue
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query[:50]}...': {type(e).__name__}: {e}")
                    self.stats.processing_errors += 1
                    continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            if self._check_memory_limit():
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {self.stats.memory_usage_mb:.1f} MB")
                self._cleanup_memory()

        self.stats.analogs_found = len(analogs)
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(analogs)} –∞–Ω–∞–ª–æ–≥–æ–≤")
        logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {self._get_memory_usage():.1f} MB")

        return analogs

    def _get_best_search_engine(self) -> str:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        if 'hybrid' in self.search_engines:
            return 'hybrid'
        elif 'semantic' in self.search_engines:
            return 'semantic'
        elif 'fuzzy' in self.search_engines:
            return 'fuzzy'
        else:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤")

    def _validate_search_results(self, results: List[Dict], query: str) -> List[Dict]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        if not results:
            return []

        validated_results = []
        for i, result in enumerate(results):
            if not isinstance(result, dict):
                logger.warning(f"–†–µ–∑—É–ª—å—Ç–∞—Ç {i} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query[:30]}...'")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π
            required_keys = ['document_id', 'document']
            missing_keys = [key for key in required_keys if key not in result]
            if missing_keys:
                logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏ {missing_keys} –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ {i} –¥–ª—è '{query[:30]}...'")
                continue

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä—ã - –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ
            result_copy = result.copy()

            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Å–∫–æ—Ä
            score_keys = ['hybrid_score', 'similarity_score', 'combined_score', 'score']
            has_score = any(key in result_copy for key in score_keys)

            if not has_score:
                logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—Å–µ —Å–∫–æ—Ä—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ {i} –¥–ª—è '{query[:30]}...'. –î–æ–±–∞–≤–ª—è–µ–º default_score=0.5")
                result_copy['hybrid_score'] = 0.5  # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–∫–æ—Ä

            validated_results.append(result_copy)

        return validated_results

    def _process_search_results_optimized(self, query_row: pd.Series, results: List[Dict],
                              data: pd.DataFrame, engine_name: str, processed_names: set, analog_pairs: set) -> Optional[Dict[str, Any]]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""

        if not results:
            return None

        query_text = query_row['processed_name']

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: –∏—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
        filtered_results = []
        for r in results:
            candidate_text = r.get('document', '')

            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å
            if candidate_text == query_text:
                continue

            # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
            if candidate_text in processed_names:
                continue

            # –ò—Å–∫–ª—é—á–∞–µ–º –µ—Å–ª–∏ —ç—Ç–∞ –ø–∞—Ä–∞ —É–∂–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏
            if (candidate_text, query_text) in analog_pairs:
                continue

            filtered_results.append(r)

        if not filtered_results:
            return None

        # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        best_result = filtered_results[0]

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∫–æ—Ä–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        similarity_score = 0.0
        score_source = "default"

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫–æ—Ä–∞
        score_keys = ['hybrid_score', 'similarity_score', 'combined_score', 'score']
        for key in score_keys:
            if key in best_result and best_result[key] is not None:
                try:
                    similarity_score = float(best_result[key])
                    score_source = key
                    break
                except (ValueError, TypeError):
                    continue

        # –õ–æ–≥–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤–∞–ª–∏–¥–Ω—ã–π —Å–∫–æ—Ä
        if similarity_score == 0.0:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–∫–æ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(best_result.keys())}")
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ª—é–±–æ–µ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            for key, value in best_result.items():
                if isinstance(value, (int, float)) and 0 <= value <= 1:
                    similarity_score = float(value)
                    score_source = f"{key}_fallback"
                    break

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏
        relation_type = self._determine_relation_type(similarity_score)

        if relation_type == '–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤':
            return None

        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –≤ –¥–∞–Ω–Ω—ã—Ö
        candidate_text = best_result.get('document', '')
        candidate_rows = data[data['processed_name'] == candidate_text]

        if len(candidate_rows) == 0:
            return None

        candidate_row = candidate_rows.iloc[0]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        analog_info = {
            'Raw_Name': query_row['Raw_Name'],
            'Cleaned_Name': query_row['Cleaned_Name'],
            'Lemmatized_Name': query_row['Lemmatized_Name'],
            'Normalized_Name': query_row['Normalized_Name'],
            'Candidate_Name': candidate_row['Raw_Name'],
            'Similarity_Score': f"{similarity_score:.3f}",
            'Relation_Type': relation_type,
            'Suggested_Category': candidate_row.get('–ì—Ä—É–ø–ø–∞', ''),
            'Final_Decision': self._get_decision(relation_type, similarity_score),
            'Comment': self._generate_comment(query_row, candidate_row, similarity_score),
            'Original_Category': query_row.get('–ì—Ä—É–ø–ø–∞', ''),
            'Original_Code': query_row.get('–ö–æ–¥', ''),
            'Search_Engine': engine_name,
            'processed_candidate_name': candidate_text  # –î–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        }

        return analog_info

    def _process_search_results(self, query_row: pd.Series, results: List[Dict],
                              data: pd.DataFrame, engine_name: str) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏"""

        if not results:
            return None

        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É –∑–∞–ø–∏—Å—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        query_text = query_row['processed_name']
        filtered_results = [r for r in results if r.get('document', '') != query_text]

        if not filtered_results:
            return None

        # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        best_result = filtered_results[0]

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∫–æ—Ä–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–µ—Ä–≤–æ–º—É –º–µ—Ç–æ–¥—É)
        similarity_score = 0.0

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫–æ—Ä–∞
        score_keys = ['hybrid_score', 'similarity_score', 'combined_score', 'score']
        for key in score_keys:
            if key in best_result and best_result[key] is not None:
                try:
                    similarity_score = float(best_result[key])
                    break
                except (ValueError, TypeError):
                    continue

        # Fallback: –ø–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ª—é–±–æ–µ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if similarity_score == 0.0:
            for key, value in best_result.items():
                if isinstance(value, (int, float)) and 0 <= value <= 1:
                    similarity_score = float(value)
                    break

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏
        relation_type = self._determine_relation_type(similarity_score)

        if relation_type == '–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤':
            return None

        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –≤ –¥–∞–Ω–Ω—ã—Ö
        candidate_text = best_result.get('document', '')
        candidate_row = data[data['processed_name'] == candidate_text].iloc[0] if len(data[data['processed_name'] == candidate_text]) > 0 else None

        if candidate_row is None:
            return None

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        analog_info = {
            'Raw_Name': query_row['Raw_Name'],
            'Cleaned_Name': query_row['Cleaned_Name'],
            'Lemmatized_Name': query_row['Lemmatized_Name'],
            'Normalized_Name': query_row['Normalized_Name'],
            'Candidate_Name': candidate_row['Raw_Name'],
            'Similarity_Score': f"{similarity_score:.3f}",
            'Relation_Type': relation_type,
            'Suggested_Category': candidate_row.get('–ì—Ä—É–ø–ø–∞', ''),
            'Final_Decision': self._get_decision(relation_type, similarity_score),
            'Comment': self._generate_comment(query_row, candidate_row, similarity_score),
            'Original_Category': query_row.get('–ì—Ä—É–ø–ø–∞', ''),
            'Original_Code': query_row.get('–ö–æ–¥', ''),
            'Search_Engine': engine_name
        }

        return analog_info

    def _determine_relation_type(self, similarity_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        if similarity_score >= self.similarity_thresholds['duplicate']:
            return '–¥—É–±–ª—å'
        elif similarity_score >= self.similarity_thresholds['close_analog']:
            return '–±–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥'
        elif similarity_score >= self.similarity_thresholds['analog']:
            return '–∞–Ω–∞–ª–æ–≥'
        elif similarity_score >= self.similarity_thresholds['possible_analog']:
            return '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'
        elif similarity_score >= self.similarity_thresholds['similar']:
            return '–ø–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä'
        else:
            return '–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤'

    def _get_decision(self, relation_type: str, similarity_score: float) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–Ω—è—Ç–∏—é —Ä–µ—à–µ–Ω–∏—è"""
        decisions = {
            '–¥—É–±–ª—å': '–î—É–±–ª–∏–∫–∞—Ç - –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å',
            '–±–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥': '–ë–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å',
            '–∞–Ω–∞–ª–æ–≥': '–ê–Ω–∞–ª–æ–≥ - —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∑–∞–º–µ–Ω—É',
            '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥': '–í–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥ - —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞',
            '–ø–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä': '–ü–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä - –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞–º–µ–Ω—ã',
            '–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤': '–ê–Ω–∞–ª–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'
        }
        return decisions.get(relation_type, '–¢—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏')

    def _generate_comment(self, query_row: pd.Series, candidate_row: pd.Series,
                         similarity_score: float) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        comments = []

        # –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        if similarity_score >= 0.9:
            comments.append("–í—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å")
        elif similarity_score >= 0.7:
            comments.append("–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å")
        else:
            comments.append("–ù–∏–∑–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        query_category = query_row.get('–ì—Ä—É–ø–ø–∞', '')
        candidate_category = candidate_row.get('–ì—Ä—É–ø–ø–∞', '')

        if query_category and candidate_category:
            if query_category != candidate_category:
                comments.append(f"–†–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {query_category} vs {candidate_category}")
            else:
                comments.append("–û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        comments.append(f"–ü–æ–∏—Å–∫–æ–≤—ã–π —Å–∫–æ—Ä: {similarity_score:.3f}")

        return "; ".join(comments)

    def create_excel_report(self, duplicates: List[Dict], analogs: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ analog_analysis_report —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞...")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–æ–≤—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∞–Ω–∞–ª–æ–≥–∏
        new_results = duplicates + analogs

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        all_results = self.preloaded_results + new_results

        if not all_results:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞")
            return None

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        if self.preloaded_results:
            logger.info(f"üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
            logger.info(f"   üìÇ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: {len(self.preloaded_results)}")
            logger.info(f"   üÜï –ù–æ–≤—ã—Ö: {len(new_results)}")
            logger.info(f"   üìä –ò—Ç–æ–≥–æ: {len(all_results)}")

        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –¥–ª—è {len(all_results)} –∑–∞–ø–∏—Å–µ–π")

        try:
            # –°–æ–∑–¥–∞–µ–º DataFrame –±–∞—Ç—á–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if len(all_results) > 10000:
                logger.info("üìä –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–Ω–∏–µ DataFrame –±–∞—Ç—á–∞–º–∏...")
                batch_size = 5000
                df_chunks = []

                for i in range(0, len(all_results), batch_size):
                    batch_end = min(i + batch_size, len(all_results))
                    batch_results = all_results[i:batch_end]
                    df_chunk = pd.DataFrame(batch_results)
                    df_chunks.append(df_chunk)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
                    if self._check_memory_limit():
                        self._cleanup_memory()

                results_df = pd.concat(df_chunks, ignore_index=True)
                del df_chunks  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
            else:
                results_df = pd.DataFrame(all_results)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É —Å–≤—è–∑–∏ –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
            type_order = ['–¥—É–±–ª—å', '–±–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥', '–∞–Ω–∞–ª–æ–≥', '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥', '–ø–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä', '–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤']
            results_df['type_order'] = results_df['Relation_Type'].apply(
                lambda x: type_order.index(x) if x in type_order else len(type_order)
            )
            results_df = results_df.sort_values(['type_order', 'Similarity_Score'], ascending=[True, False])
            results_df = results_df.drop('type_order', axis=1)

            # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.data_output_dir / f"analog_analysis_report_{timestamp}.xlsx"

            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞: {output_path}")

            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # –û—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
                if len(results_df) > 50000:
                    logger.info("üìä –ë–æ–ª—å—à–æ–π –æ—Ç—á–µ—Ç, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–∞–º–∏...")
                    # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    main_columns = ['Raw_Name', 'Cleaned_Name', 'Lemmatized_Name', 'Normalized_Name',
                                  'Candidate_Name', 'Similarity_Score', 'Relation_Type',
                                  'Final_Decision', 'Comment']
                    available_columns = [col for col in main_columns if col in results_df.columns]
                    results_df[available_columns].to_excel(writer, sheet_name='Analog_Analysis', index=False)
                else:
                    results_df.to_excel(writer, sheet_name='Analog_Analysis', index=False)

                # –õ–∏—Å—Ç —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ —Ç–∏–ø–∞–º —Å–≤—è–∑–µ–π
                relation_stats = results_df['Relation_Type'].value_counts().reset_index()
                relation_stats.columns = ['Relation_Type', 'Count']
                relation_stats['Percentage'] = (relation_stats['Count'] / len(results_df) * 100).round(2)
                relation_stats.to_excel(writer, sheet_name='Relation_Statistics', index=False)

                # –õ–∏—Å—Ç —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                if 'Original_Category' in results_df.columns:
                    category_stats = results_df['Original_Category'].value_counts().head(20).reset_index()
                    category_stats.columns = ['Category', 'Count']
                    category_stats.to_excel(writer, sheet_name='Category_Statistics', index=False)

                # –õ–∏—Å—Ç —Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
                general_stats = pd.DataFrame([
                    ['–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ', self.stats.total_records],
                    ['–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤', self.stats.duplicates_found],
                    ['–ù–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤', self.stats.analogs_found],
                    ['–ë–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ', self.stats.batch_count],
                    ['–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ (MB)', f"{self.stats.memory_usage_mb:.1f}"],
                    ['–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º–∏–Ω)', self._get_processing_time()],
                    ['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞', datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
                ], columns=['–ü–∞—Ä–∞–º–µ—Ç—Ä', '–ó–Ω–∞—á–µ–Ω–∏–µ'])
                general_stats.to_excel(writer, sheet_name='General_Statistics', index=False)

            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            del results_df
            self._cleanup_memory()

            logger.info(f"‚úÖ Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {output_path}")
            logger.info(f"üíæ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {output_path.stat().st_size / 1024 / 1024:.1f} MB")

            return str(output_path)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel –æ—Ç—á–µ—Ç–∞: {e}")
            raise

    def _get_processing_time(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö"""
        if self.stats.start_time and self.stats.end_time:
            delta = self.stats.end_time - self.stats.start_time
            return round(delta.total_seconds() / 60, 2)
        return 0.0

    def run_full_analysis(self, sample_size: Optional[int] = None) -> str:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤...")

        try:
            # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            self.initialize_components()

            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if self.load_existing_results:
                self._load_existing_results()

            # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.load_data()

            # 4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
            if sample_size and sample_size < len(self.data):
                logger.info(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞: {sample_size} –∏–∑ {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
                self.data = self.data.sample(n=sample_size, random_state=42)
                self.stats.total_records = len(self.data)

            # 5. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.preprocess_data()

            # 6. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            duplicates, duplicate_groups = self.find_duplicates()

            # 7. –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ (–∏—Å–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)
            analogs = self.search_analogs(exclude_duplicates=True, duplicate_groups=duplicate_groups)

            # 8. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            self.stats.end_time = datetime.now()
            report_path = self.create_excel_report(duplicates, analogs)

            # 9. –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._print_final_statistics()

            return report_path

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            raise

    def _print_final_statistics(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        logger.info("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"   üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {self.stats.total_records}")
        logger.info(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {self.stats.processed_records}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        total_duplicates = self.stats.duplicates_found + self.stats.preloaded_duplicates
        total_analogs = self.stats.analogs_found + self.stats.preloaded_analogs

        logger.info(f"   üîÑ –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats.duplicates_found} (–Ω–æ–≤—ã—Ö) + {self.stats.preloaded_duplicates} (–ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö) = {total_duplicates}")
        logger.info(f"   üîç –ù–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤: {self.stats.analogs_found} (–Ω–æ–≤—ã—Ö) + {self.stats.preloaded_analogs} (–ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö) = {total_analogs}")

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"   üì¶ –ë–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats.batch_count}")
        logger.info(f"   üíæ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {self.stats.memory_usage_mb:.1f} MB")
        logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self._get_processing_time()} –º–∏–Ω")
        logger.info(f"   ‚ùå –û—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.stats.processing_errors}")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        if self.stats.preloaded_processed_names > 0:
            logger.info(f"   üìÇ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π: {self.stats.preloaded_processed_names}")
            logger.info(f"   üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {Path(self.stats.existing_report_path).name if self.stats.existing_report_path else 'N/A'}")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.stats.memory_usage_mb > self.memory_limit_mb:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å batch_size –∏–ª–∏ memory_limit_mb")

        if self.stats.processing_errors > 0:
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
        if not self.load_existing_results and self._find_latest_report():
            logger.info(f"üí° –°–æ–≤–µ—Ç: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --load-existing-results –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description='SAMe Analog Search Processor')
    parser.add_argument('--sample-size', type=int, default=None,
                       help='–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç)')
    parser.add_argument('--input-file', type=str, default='src/data/input/main_dataset.xlsx',
                       help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É Excel')
    parser.add_argument('--output-dir', type=str, default='src/data/output',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1000)')
    parser.add_argument('--memory-limit', type=int, default=4000,
                       help='–õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ –≤ MB (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4000)')
    parser.add_argument('--disable-cache', action='store_true',
                       help='–û—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--force-reprocess', action='store_true',
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à)')
    parser.add_argument('--load-existing-results', action='store_true',
                       help='–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏')

    args = parser.parse_args()

    print("üöÄ SAMe Analog Search Processor")
    print("=" * 50)
    print(f"üìÇ –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.input_file}")
    print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.output_dir}")
    if args.sample_size:
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {args.sample_size}")
    else:
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
    SAMPLE_SIZE = args.sample_size

    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AnalogSearchProcessor()

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if args.input_file != 'src/data/input/main_dataset.xlsx':
            processor.data_input_path = Path(args.input_file)
        if args.output_dir != 'src/data/output':
            processor.data_output_dir = Path(args.output_dir)
            processor.data_output_dir.mkdir(parents=True, exist_ok=True)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        processor.batch_size = args.batch_size
        processor.memory_limit_mb = args.memory_limit
        processor.use_processed_cache = not args.disable_cache
        processor.load_existing_results = args.load_existing_results

        # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞, –æ—á–∏—â–∞–µ–º –∫—ç—à
        if args.force_reprocess:
            processor.use_processed_cache = False
            logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∫—ç—à –æ—Ç–∫–ª—é—á–µ–Ω)")

        logger.info(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
        logger.info(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {processor.batch_size}")
        logger.info(f"   üíæ –õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {processor.memory_limit_mb} MB")
        logger.info(f"   üóÑÔ∏è –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: {'–≤–∫–ª—é—á–µ–Ω–æ' if processor.use_processed_cache else '–æ—Ç–∫–ª—é—á–µ–Ω–æ'}")
        logger.info(f"   üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {'–≤–∫–ª—é—á–µ–Ω–æ' if processor.load_existing_results else '–æ—Ç–∫–ª—é—á–µ–Ω–æ'}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        report_path = processor.run_full_analysis(sample_size=SAMPLE_SIZE)

        if report_path:
            print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            print(f"\nüí° –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á–µ—Ç–∞:")
            print(f"   ‚Ä¢ Raw_Name - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
            print(f"   ‚Ä¢ Cleaned_Name - –û—á–∏—â–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
            print(f"   ‚Ä¢ Lemmatized_Name - –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
            print(f"   ‚Ä¢ Normalized_Name - –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
            print(f"   ‚Ä¢ Candidate_Name - –ù–∞–π–¥–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–æ–≥/–¥—É–±–ª–∏–∫–∞—Ç")
            print(f"   ‚Ä¢ Similarity_Score - –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)")
            print(f"   ‚Ä¢ Relation_Type - –¢–∏–ø —Å–≤—è–∑–∏ (–¥—É–±–ª—å/–∞–Ω–∞–ª–æ–≥/–ø–æ—Ö–æ–∂–∏–π)")
            print(f"   ‚Ä¢ Suggested_Category - –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è")
            print(f"   ‚Ä¢ Final_Decision - –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ")
            print(f"   ‚Ä¢ Comment - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å–∏—Å—Ç–µ–º—ã")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç")

    except KeyboardInterrupt:
        print("\nüëã –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
