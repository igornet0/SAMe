
import asyncio
import argparse
import logging
import pandas as pd
from pathlib import Path
import sys
import gc
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np
from typing import List, Dict, Tuple, Any, Optional
import pickle
import time
import tempfile
import os
import re
from datetime import datetime
from tqdm import tqdm
from dataclasses import dataclass

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent / "src"))

from data_manager.DataManager import DataManager
from same_clear.text_processing.enhanced_preprocessor import EnhancedPreprocessor, EnhancedPreprocessorConfig
from same_clear.parameter_extraction.regex_extractor import RegexParameterExtractor
from same_clear.parameter_extraction.enhanced_parameter_extractor import EnhancedParameterExtractor
from same_search.performance.optimized_search_engine import OptimizedSearchEngine, PerformanceConfig
from same_search.categorization.category_classifier import CategoryClassifier, CategoryClassifierConfig
from same_search.duplicate_analog_search import (
    DuplicateAnalogSearchEngine, 
    DuplicateSearchConfig, 
    AnalogSearchConfig
)
from same_search.multi_engine_search import MultiEngineSearch, MultiEngineConfig
from same_search.simple_multi_engine_search import SimpleMultiEngineSearch, SimpleMultiEngineConfig
from same_search.improved_multi_engine_search import ImprovedMultiEngineSearch, ImprovedMultiEngineConfig
from same_search.batch_processor import BatchProcessor
from same_search.tree_optimizer import TreeOptimizer
from same_api.export.excel_exporter import ExcelExporter

logger = logging.getLogger(__name__)

# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
MODEL_BRANDS = [
    'neox', 'osairous', 'yealink', 'sanfor', '—Å–∞–Ω—Ñ–æ—Ä', '–±–∏–æ–ª–∞–Ω', '–Ω—ç—Ñ–∏—Å',
    '–ø–µ—Ä—Å–∏–ª', 'dallas', '–ø—Ä–µ–º–∏—É–º', '–º–∞—è–∫', 'chint', 'andeli', 'grass',
    'kraft', 'reoflex', '–∫–µ—Ä—Ö–µ—Ä', 'huawei', 'honor', '–í–´–°–û–¢–ê', 'ugreen',
    'alisafox', '–º–∞—è–∫–∞–≤—Ç–æ', '—Ç–µ—Ö–Ω–æ–∞–≤–∏–∞', '–≤–æ—Å—Ç–æ–∫-—Å–µ—Ä–≤–∏—Å', 'attache', '–∫–∞–º–∞–∑',
    '–∑—É–±—Ä', 'hp', 'ekf', 'dexp', 'matrix', 'siemens', '–∫–æ–º—É—Å', 'gigant',
    'hyundai', 'iveco', 'stayer', 'brauberg', 'makita', 'bentec', '—Å–∏–±—Ä—Ç–µ—Ö',
    'bosch', 'rexant', 'sampa', 'kyocera', 'avrora', 'derrick', 'cummins',
    'economy', 'samsung', 'ofite', 'professional', 'caterpillar', 'intel',
    'proxima', 'core', 'shantui', 'king', 'office', '–ø–µ—Ç—Ä–æ–ª–µ—É–º', '—Ç—Ä–µ–π–ª',
    'skf', '—Ñ–æ—Ä–≤–µ–ª–¥', '—Å–∫–∞–π–º–∞—Å—Ç–µ—Ä', 'tony', 'kentek', '—Ä–µ—Å–∞–Ω—Ç–∞', 'dexter',
    'electric', '–æ—Ç—Ç–º'
]

@dataclass
class ProcessingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    similarity_threshold: float = 0.4
    duplicate_threshold: float = 0.95
    analog_threshold: float = 0.7
    possible_analog_threshold: float = 0.5
    batch_size: int = 1000
    max_workers: int = 4

@dataclass
class DuplicateGroup:
    """–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    main_index: int
    main_name: str
    duplicate_indices: List[int]
    duplicate_names: List[str]
    similarity_scores: List[float]

@dataclass
class AnalogGroup:
    """–ì—Ä—É–ø–ø–∞ –∞–Ω–∞–ª–æ–≥–æ–≤"""
    reference_index: int
    reference_name: str
    analogs: List[Dict[str, Any]]  # index, name, similarity, type

@dataclass
class ProductTree:
    """–î–µ—Ä–µ–≤–æ —Ç–æ–≤–∞—Ä–æ–≤"""
    root_index: int
    root_name: str
    duplicates: List[Dict[str, Any]]
    exact_analogs: List[Dict[str, Any]]
    close_analogs: List[Dict[str, Any]]
    possible_analogs: List[Dict[str, Any]]

class DuplicateAnalogProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–Ω–∞–ª–æ–≥–æ–≤"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.data_manager = DataManager()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.preprocessor = EnhancedPreprocessor(EnhancedPreprocessorConfig())
        self.parameter_extractor = RegexParameterExtractor()
        self.enhanced_parameter_extractor = EnhancedParameterExtractor()
        self.category_classifier = CategoryClassifier(CategoryClassifierConfig())
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞
        performance_config = PerformanceConfig(
            max_workers=4,
            chunk_size=1000,
            cache_size=10000,
            enable_caching=True,
            enable_parallel_processing=True,
            enable_memory_optimization=True
        )
        self.optimized_engine = OptimizedSearchEngine(performance_config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        duplicate_config = DuplicateSearchConfig(
            fuzzy_match_threshold=0.70,  # –°–Ω–∏–∂–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            parameter_similarity_threshold=0.60,
            semantic_similarity_threshold=0.50,
            enable_semantic_check=True,
            enable_parameter_check=True,
            enable_brand_check=True
        )
        analog_config = AnalogSearchConfig(
            exact_analog_threshold=0.70,
            close_analog_threshold=0.50,
            possible_analog_threshold=0.30,  # –°–Ω–∏–∂–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            max_analogs_per_item=25,
            enable_hierarchical_search=True,
            enable_parameter_priority=True,
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            semantic_weight=0.35,
            fuzzy_weight=0.30,
            parameter_weight=0.20,
            brand_weight=0.10,
            category_weight=0.05
        )
        self.search_engine = DuplicateAnalogSearchEngine(duplicate_config, analog_config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        simple_multi_engine_config = SimpleMultiEngineConfig(
            min_similarity_threshold=0.4,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
            final_top_k=50,
            enable_parallel_search=True
        )
        self.simple_multi_engine_search = SimpleMultiEngineSearch(simple_multi_engine_config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        improved_multi_engine_config = ImprovedMultiEngineConfig(
            min_similarity_threshold=0.4,
            fuzzy_threshold=40,
            semantic_threshold=0.7,
            fuzzy_weight=0.6,
            semantic_weight=0.4,
            enable_category_filtering=True,
            enable_token_filtering=True,
            enable_brand_filtering=True
        )
        self.improved_multi_engine_search = ImprovedMultiEngineSearch(improved_multi_engine_config)
        
        # –≠–∫—Å–ø–æ—Ä—Ç
        self.excel_exporter = ExcelExporter()
        
        # –î–∞–Ω–Ω—ã–µ
        self.catalog_df = None
        self.processed_df = None
        self.duplicate_groups = []
        self.analog_groups = []
        self.product_trees = []
    
    def update_multi_engine_config(self, threshold: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        self.simple_multi_engine_search.config.min_similarity_threshold = threshold
        self.improved_multi_engine_search.config.min_similarity_threshold = threshold
        
        logger.info("DuplicateAnalogProcessor initialized")
    
    def extract_model_brand(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏/–±—Ä–µ–Ω–¥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        
        text_lower = text.lower()
        words = text_lower.split()
        
        for brand in MODEL_BRANDS:
            if brand in text:
                return brand
    
        return None
    
    async def load_and_preprocess_data(self, input_file: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"Loading data from {input_file}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if input_file.endswith('.csv'):
            self.catalog_df = pd.read_csv(input_file)
        elif input_file.endswith('.xlsx'):
            self.catalog_df = pd.read_excel(input_file)
        else:
            raise ValueError(f"Unsupported file format: {input_file}")
        
        logger.info(f"Loaded {len(self.catalog_df)} records")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ø–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processed_df = self.catalog_df.copy()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        self.processed_df['processed_name'] = ''
        self.processed_df['model_brand'] = ''
        self.processed_df['extracted_parameters'] = ''
        self.processed_df['category'] = ''
        self.processed_df['duplicate_count'] = 0
        self.processed_df['duplicate_indices'] = ''
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        logger.info("Starting text preprocessing...")
        for idx in tqdm(self.processed_df.index, desc="Preprocessing text"):
            try:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
                name_column = None
                for col in self.processed_df.columns:
                    if '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' in col.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in col.lower() or 'name' in col.lower():
                        name_column = col
                        break
                
                if name_column is None:
                    # Fallback –Ω–∞ –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
                    name_column = self.processed_df.columns[0]
                
                original_name = str(self.processed_df.loc[idx, name_column])
                
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                processed_result = self.preprocessor.preprocess_text(original_name)
                processed_text = processed_result.get('normalized', original_name)
                self.processed_df.loc[idx, 'processed_name'] = processed_text
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏/–±—Ä–µ–Ω–¥–∞
                model_brand = self.extract_model_brand(original_name)
                self.processed_df.loc[idx, 'model_brand'] = model_brand if model_brand else ''
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ)
                basic_parameters = self.parameter_extractor.extract_parameters(original_name)
                enhanced_parameters = self.enhanced_parameter_extractor.extract_parameters(original_name)
                
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                all_parameters = basic_parameters + enhanced_parameters
                param_str = '; '.join([f"{p.name}: {p.value}" for p in all_parameters])
                self.processed_df.loc[idx, 'extracted_parameters'] = param_str
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                try:
                    category, confidence = self.category_classifier.classify(original_name)
                    self.processed_df.loc[idx, 'category'] = category
                except Exception as e:
                    logger.warning(f"Error classifying category for row {idx}: {e}")
                    self.processed_df.loc[idx, 'category'] = '–æ–±—â–∏–µ_—Ç–æ–≤–∞—Ä—ã'
                
            except Exception as e:
                logger.error(f"Error processing row {idx}: {e}")
                continue
        
        logger.info("Text preprocessing completed")
        return self.processed_df
    
    async def find_duplicates_and_analogs(self, use_optimized: bool = False, use_multi_engine: bool = False, use_improved: bool = False) -> Tuple[List[Any], List[Any]]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        logger.info("Starting duplicate and analog detection...")
        
        if use_improved:
            logger.info("Using improved multi-engine search with enhanced filtering...")
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
            results_df = await self.improved_multi_engine_search.process_catalog(self.processed_df)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ª–æ–≥–∏–∫–æ–π
            results = {
                'duplicates': [],
                'analogs': [],
                'trees': []
            }
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö
            if 'query_index' in results_df.columns and len(results_df) > 0:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏
                valid_results = results_df.dropna(subset=['query_index', 'candidate_idx'])
                
                if len(valid_results) > 0:
                    query_groups = valid_results.groupby('query_index')
                    for query_idx, group in query_groups:
                        analogs = []
                        for _, row in group.iterrows():
                            candidate_idx = row.get('candidate_idx')
                            similarity_score = row.get('multi_engine_score', 0.0)
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                            if candidate_idx is not None and not pd.isna(candidate_idx) and similarity_score > 0:
                                analog_result = {
                                    'original_index': int(query_idx),
                                    'similar_index': int(candidate_idx),
                                    'similarity_score': float(similarity_score),
                                    'analog_type': self._determine_analog_type(similarity_score)
                                }
                                analogs.append(analog_result)
                        
                        if analogs:
                            analog_group = {
                                'main_index': int(query_idx),
                                'analogs': analogs
                            }
                            results['analogs'].append(analog_group)
                        
            logger.info(f"Improved multi-engine search found {len(results['analogs'])} analog groups")
            
        elif use_multi_engine:
            logger.info("Using simple multi-engine search for maximum coverage...")
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
            results_df = await self.simple_multi_engine_search.process_catalog(self.processed_df)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ª–æ–≥–∏–∫–æ–π
            results = {
                'duplicates': [],
                'analogs': [],
                'trees': []
            }
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
            if 'query_index' in results_df.columns:
                query_groups = results_df.groupby('query_index')
                for query_idx, group in query_groups:
                    analogs = []
                    for _, row in group.iterrows():
                        analog_result = {
                            'original_index': query_idx,
                            'similar_index': row.get('document_id', row.get('index', 0)),
                            'similarity_score': row.get('multi_engine_score', 0.0),
                            'analog_type': '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'  # –£–ø—Ä–æ—â–∞–µ–º —Ç–∏–ø
                        }
                        analogs.append(analog_result)
                    
                    if analogs:
                        results['analogs'].append({
                            'main_index': query_idx,
                            'analogs': analogs
                        })
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç query_index, —Å–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–¥–µ–∫—Å–æ–≤
                for idx, row in results_df.iterrows():
                    analog_result = {
                        'original_index': idx,
                        'similar_index': row.get('document_id', row.get('index', 0)),
                        'similarity_score': row.get('multi_engine_score', 0.0),
                        'analog_type': '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'
                    }
                    
                    results['analogs'].append({
                        'main_index': idx,
                        'analogs': [analog_result]
                    })
            
        elif use_optimized and len(self.processed_df) > 1000:
            logger.info("Using optimized engine for large dataset...")
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            results = self.optimized_engine.optimize_for_large_dataset(self.processed_df)
        else:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞
            results = await self.search_engine.process_catalog(self.processed_df)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        for dup_result in results.get('duplicates', []):
            if isinstance(dup_result, dict):
                main_index = dup_result.get('main_index', 0)
                duplicate_indices = dup_result.get('duplicate_indices', [])
            else:
                main_index = dup_result.main_index
                duplicate_indices = dup_result.duplicate_indices
            
            self.processed_df.loc[main_index, 'duplicate_count'] = len(duplicate_indices)
            self.processed_df.loc[main_index, 'duplicate_indices'] = ','.join(map(str, duplicate_indices))
            
            # –û—Ç–º–µ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
            for dup_idx in duplicate_indices:
                self.processed_df.loc[dup_idx, 'duplicate_count'] = -1
        
        self.duplicate_groups = results.get('duplicates', [])
        self.analog_groups = results.get('analogs', [])
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –¥–µ—Ä–µ–≤—å—è –¥–ª—è –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if (use_multi_engine or use_improved) and not results.get('trees'):
            self.product_trees = self._create_simple_trees_from_analogs()
        else:
            self.product_trees = results.get('trees', [])
        
        logger.info(f"Found {len(self.duplicate_groups)} duplicate groups and {len(self.analog_groups)} analog groups")
        return self.duplicate_groups, self.analog_groups
    
    def _create_simple_trees_from_analogs(self) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–æ–≥–æ–≤"""
        logger.info("Creating optimized trees from analog results...")
        
        if not self.analog_groups:
            logger.warning("No analog groups found, returning empty trees list")
            return []
        
        logger.info(f"Processing {len(self.analog_groups)} analog groups for tree creation")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ –¥–µ—Ä–µ–≤—å—è –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        simple_trees = []
        processed_groups = 0
        valid_groups = 0
        
        for group in self.analog_groups:
            processed_groups += 1
            if isinstance(group, dict):
                main_index = group.get('main_index', 0)
                analogs = group.get('analogs', [])
            else:
                main_index = group.reference_index
                analogs = group.analogs
            
            if analogs:
                valid_groups += 1
                tree = {
                    'root_index': main_index,
                    'exact_analogs': [],
                    'close_analogs': [],
                    'possible_analogs': [],
                    'tree_depth': 1,
                    'total_nodes': len(analogs) + 1
                }
                
                for analog in analogs:
                    if isinstance(analog, dict):
                        analog_index = analog.get('similar_index', analog.get('index', 0))
                        similarity = analog.get('similarity_score', analog.get('similarity', 0.0))
                        analog_type = analog.get('analog_type', analog.get('type', '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'))
                    else:
                        analog_index = analog.index
                        similarity = analog.similarity
                        analog_type = analog.type
                    
                    analog_data = {
                        'index': analog_index,
                        'similarity': similarity
                    }
                    
                    if similarity >= 0.8:
                        tree['exact_analogs'].append(analog_data)
                    elif similarity >= 0.6:
                        tree['close_analogs'].append(analog_data)
                    else:
                        tree['possible_analogs'].append(analog_data)
                
                simple_trees.append(tree)
        
        logger.info(f"Tree creation stats: processed {processed_groups} groups, {valid_groups} had analogs, created {len(simple_trees)} simple trees")
        
        # –ï—Å–ª–∏ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥—Ä—É–ø–ø, –ø–æ–ø—Ä–æ–±—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        if len(self.analog_groups) > 5:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ü–∏–∫–ª–æ–≤
                tree_optimizer = TreeOptimizer(max_tree_depth=4, min_similarity_for_parent=0.3)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞—Ñ –∞–Ω–∞–ª–æ–≥–æ–≤
                graph_analysis = tree_optimizer.create_graph_analysis(self.analog_groups)
                logger.info(f"Graph analysis: {graph_analysis['total_nodes']} nodes, "
                           f"{graph_analysis['total_edges']} edges, "
                           f"{graph_analysis['connected_components']} components, "
                           f"{graph_analysis['components_with_cycles']} cycles detected")
                
                # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è
                optimized_trees = tree_optimizer.optimize_trees(self.analog_groups, self.processed_df)
                
                if optimized_trees:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    converted_trees = []
                    for tree in optimized_trees:
                        converted_tree = {
                            'root_index': tree['root_index'],
                            'exact_analogs': tree.get('exact_analogs', []),
                            'close_analogs': tree.get('close_analogs', []),
                            'possible_analogs': tree.get('possible_analogs', []),
                            'tree_depth': tree.get('tree_depth', 0),
                            'total_nodes': tree.get('total_nodes', 0)
                        }
                        converted_trees.append(converted_tree)
                    
                    logger.info(f"Created {len(converted_trees)} optimized trees "
                               f"(reduced from {len(self.analog_groups)} analog groups)")
                    return converted_trees
                    
            except Exception as e:
                logger.warning(f"Tree optimization failed: {e}, using simple trees")
        
        logger.info(f"Created {len(simple_trees)} simple trees from {len(self.analog_groups)} analog groups")
        return simple_trees
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
        if not text1 or not text2:
            return 0.0
        
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        if text1_lower == text2_lower:
            return 1.0
        
        # –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤
        words1 = set(text1_lower.split())
        words2 = set(text2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _determine_analog_type(self, similarity: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∞–Ω–∞–ª–æ–≥–∞ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        if similarity >= self.config.duplicate_threshold:
            return "–¥—É–±–ª—å"
        elif similarity >= self.config.analog_threshold:
            return "–∞–Ω–∞–ª–æ–≥"
        elif similarity >= self.config.possible_analog_threshold:
            return "–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥"
        else:
            return "–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤"
    
    async def save_results(self, output_dir: Path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        logger.info(f"Saving results to {output_dir}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
        date_folder = output_dir / datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        date_folder.mkdir(parents=True, exist_ok=True)
        
        # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ë–ï–ó –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å duplicate_count = -1 (—ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç—ã)
        unique_df = self.processed_df[self.processed_df['duplicate_count'] != -1].copy()
        
        processed_file = date_folder / "processed_data_with_duplicates.csv"
        unique_df.to_csv(processed_file, index=False)
        logger.info(f"Saved processed data (without duplicates) to {processed_file}")
        logger.info(f"Excluded {len(self.processed_df) - len(unique_df)} duplicate records from output")
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        name_column = None
        for col in self.processed_df.columns:
            if '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' in col.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in col.lower() or 'name' in col.lower():
                name_column = col
                break
        
        if name_column is None:
            name_column = self.processed_df.columns[0]
        
        analogs_data = []
        for group in self.analog_groups:
            if isinstance(group, dict):
                main_index = group.get('main_index', 0)
                analogs = group.get('analogs', [])
            else:
                main_index = group.reference_index
                analogs = group.analogs
            
            for analog in analogs:
                if isinstance(analog, dict):
                    analog_index = analog.get('similar_index', analog.get('index', 0))
                    similarity = analog.get('similarity_score', analog.get('similarity', 0.0))
                    analog_type = analog.get('analog_type', analog.get('type', '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'))
                else:
                    analog_index = analog.index
                    similarity = analog.similarity
                    analog_type = analog.type
                
                analogs_data.append({
                    'index': analog_index,
                    'original_name': self.processed_df.loc[main_index, name_column],
                    'similar_name': self.processed_df.loc[analog_index, name_column],
                    'similarity_coefficient': similarity,
                    'type': analog_type
                })
        
        analogs_df = pd.DataFrame(analogs_data)
        analogs_file = date_folder / "analogs_search_results.csv"
        analogs_df.to_csv(analogs_file, index=False)
        logger.info(f"Saved analogs data to {analogs_file}")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–µ—Ä–µ–≤—å—è–º–∏
        trees_file = date_folder / "product_trees.txt"
        logger.info(f"Saving {len(self.product_trees)} product trees to {trees_file}")
        
        with open(trees_file, 'w', encoding='utf-8') as f:
            f.write("–î–ï–†–ï–í–¨–Ø –ê–ù–ê–õ–û–ì–û–í\n")
            f.write("=" * 60 + "\n\n")
            
            if not self.product_trees:
                f.write("‚ùå –î–ï–†–ï–í–¨–Ø –ù–ï –ù–ê–ô–î–ï–ù–´\n")
                f.write("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n")
                f.write("- –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π\n")
                f.write("- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤\n")
                f.write("- –û—à–∏–±–∫–∞ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏\n\n")
                logger.warning("No product trees generated - writing diagnostic information")
            
            for i, tree in enumerate(self.product_trees, 1):
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
                original_name = self.processed_df.loc[tree['root_index'], name_column]
                tree_depth = tree.get('tree_depth', 0)
                total_nodes = tree.get('total_nodes', 0)
                
                f.write(f"–î–ï–†–ï–í–û {i} (–≥–ª—É–±–∏–Ω–∞: {tree_depth}, —É–∑–ª–æ–≤: {total_nodes})\n")
                f.write(f"‚îî‚îÄ‚îÄ [–ö–û–†–ï–ù–¨] {tree['root_index']} | {original_name}\n")
                
                # –î—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
                duplicates = tree.get('duplicates', [])
                if duplicates:
                    f.write("    ‚îú‚îÄ‚îÄ [–î–£–ë–õ–ò–ö–ê–¢–´]\n")
                    for j, dup in enumerate(duplicates):
                        dup_name = self.processed_df.loc[dup['index'], name_column]
                        connector = "‚îú‚îÄ‚îÄ" if j < len(duplicates) - 1 else "‚îî‚îÄ‚îÄ"
                        f.write(f"    ‚îÇ   {connector} {dup['index']} | {dup_name} ({dup['similarity']:.4f})\n")
                
                # –¢–æ—á–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏
                exact_analogs = tree.get('exact_analogs', [])
                if exact_analogs:
                    f.write("    ‚îú‚îÄ‚îÄ [–¢–û–ß–ù–´–ï –ê–ù–ê–õ–û–ì–ò]\n")
                    for j, analog in enumerate(exact_analogs):
                        analog_name = self.processed_df.loc[analog['index'], name_column]
                        connector = "‚îú‚îÄ‚îÄ" if j < len(exact_analogs) - 1 else "‚îî‚îÄ‚îÄ"
                        f.write(f"    ‚îÇ   {connector} {analog['index']} | {analog_name} ({analog['similarity']:.4f})\n")
                
                # –ë–ª–∏–∑–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏
                close_analogs = tree.get('close_analogs', [])
                if close_analogs:
                    f.write("    ‚îú‚îÄ‚îÄ [–ë–õ–ò–ó–ö–ò–ï –ê–ù–ê–õ–û–ì–ò]\n")
                    for j, analog in enumerate(close_analogs):
                        analog_name = self.processed_df.loc[analog['index'], name_column]
                        connector = "‚îú‚îÄ‚îÄ" if j < len(close_analogs) - 1 else "‚îî‚îÄ‚îÄ"
                        f.write(f"    ‚îÇ   {connector} {analog['index']} | {analog_name} ({analog['similarity']:.4f})\n")
                
                # –í–æ–∑–º–æ–∂–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏
                possible_analogs = tree.get('possible_analogs', [])
                if possible_analogs:
                    f.write("    ‚îî‚îÄ‚îÄ [–í–û–ó–ú–û–ñ–ù–´–ï –ê–ù–ê–õ–û–ì–ò]\n")
                    for j, analog in enumerate(possible_analogs):
                        analog_name = self.processed_df.loc[analog['index'], name_column]
                        connector = "‚îú‚îÄ‚îÄ" if j < len(possible_analogs) - 1 else "‚îî‚îÄ‚îÄ"
                        f.write(f"        {connector} {analog['index']} | {analog_name} ({analog['similarity']:.4f})\n")
                
                f.write("\n" + "-" * 60 + "\n\n")
        
        logger.info(f"Saved product trees to {trees_file}")
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report_file = date_folder / "processing_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("–û–¢–ß–ï–¢ –û–ë –û–ë–†–ê–ë–û–¢–ö–ï –ö–ê–¢–ê–õ–û–ì–ê\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ: {len(self.processed_df)}\n")
            f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤): {len(unique_df)}\n")
            f.write(f"–ò—Å–∫–ª—é—á–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(self.processed_df) - len(unique_df)}\n")
            f.write(f"–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(self.duplicate_groups)}\n")
            f.write(f"–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –∞–Ω–∞–ª–æ–≥–æ–≤: {len(self.analog_groups)}\n")
            f.write(f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–æ –¥–µ—Ä–µ–≤—å–µ–≤: {len(self.product_trees)}\n\n")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–æ–≥–æ–≤
            type_counts = {}
            for group in self.analog_groups:
                if isinstance(group, dict):
                    analogs = group.get('analogs', [])
                else:
                    analogs = group.analogs if hasattr(group, 'analogs') else []
                
                for analog in analogs:
                    if isinstance(analog, dict):
                        analog_type = analog.get('analog_type', analog.get('type', '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'))
                    else:
                        analog_type = analog.type if hasattr(analog, 'type') else '–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥'
                    type_counts[analog_type] = type_counts.get(analog_type, 0) + 1
            
            f.write("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–æ–≥–æ–≤:\n")
            for analog_type, count in type_counts.items():
                f.write(f"  {analog_type}: {count}\n")
        
        logger.info(f"Saved processing report to {report_file}")
        
        return date_folder

async def generate_search_trees(input_file: str, similarity_threshold: float, max_records: int, batch_size: int, use_optimized: bool = False, use_multi_engine: bool = False, use_improved: bool = False, force_batch: bool = False, force_full: bool = False) -> bool:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ—Ä–µ–≤—å–µ–≤ –ø–æ–∏—Å–∫–∞"""
    try:
        logger.info("Starting duplicate and analog search system")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = ProcessingConfig(
            similarity_threshold=similarity_threshold,
            batch_size=batch_size
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = DuplicateAnalogProcessor(config)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        processed_df = await processor.load_and_preprocess_data(input_file)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if force_full:
            logger.info(f"üöÄ FORCE FULL MODE: Processing ALL {len(processed_df):,} records without limits!")
            logger.warning("This may take several hours and use significant memory.")
        elif max_records and len(processed_df) > max_records:
            processed_df = processed_df.head(max_records)
            processor.processed_df = processed_df
            logger.info(f"Limited to {max_records} records")
        elif len(processed_df) > 50000 or force_batch:
            # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–∫–µ—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            logger.warning(f"Very large dataset detected ({len(processed_df)} records). Using batch processing.")
            batch_processor = BatchProcessor(batch_size=2000, overlap_size=100)
            
            # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            time_estimate = batch_processor.estimate_processing_time(len(processed_df))
            logger.info(f"Estimated processing time: {time_estimate['estimated_time_hours']:.1f} hours")
            
            # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            batch_results = await batch_processor.process_large_dataset(
                processed_df, processor, similarity_threshold
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processor.processed_df = batch_results.get('final_processed_df', pd.DataFrame())
            processor.duplicate_groups = batch_results.get('duplicate_groups', [])
            processor.analog_groups = batch_results.get('analog_groups', [])
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –¥–µ—Ä–µ–≤—å—è
            processor.product_trees = processor._create_simple_trees_from_analogs()
            
            logger.info(f"Batch processing completed. Found {len(processor.duplicate_groups)} duplicate groups and {len(processor.analog_groups)} analog groups")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            output_dir = Path("src/data/output")
            result_folder = await processor.save_results(output_dir)
            
            logger.info(f"Processing completed successfully. Results saved to {result_folder}")
            return True
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if use_multi_engine:
            processor.update_multi_engine_config(similarity_threshold)
        
        # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–Ω–∞–ª–æ–≥–æ–≤
        duplicate_groups, analog_groups = await processor.find_duplicates_and_analogs(use_optimized, use_multi_engine, use_improved)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_dir = Path("src/data/output")
        result_folder = await processor.save_results(output_dir)
        
        logger.info(f"Processing completed successfully. Results saved to {result_folder}")
        return True
        
    except Exception as e:
        logger.error(f"Error in generate_search_trees: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(
        description='SAMe - –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–Ω–∞–ª–æ–≥–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤',
        epilog='''
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s catalog.xlsx --improved                    # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º
  %(prog)s data.xlsx -t 0.3 -l 1000 --improved      # –° –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
  %(prog)s big_file.xlsx --batch --improved          # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
  
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: —Å–º. MAIN_PY_USAGE.md
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('input_file', 
                       help='–í—Ö–æ–¥–Ω–æ–π CSV/Excel —Ñ–∞–π–ª —Å –∫–∞—Ç–∞–ª–æ–≥–æ–º —Ç–æ–≤–∞—Ä–æ–≤')
    parser.add_argument('-t', '--threshold', type=float, default=0.4, 
                       help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.4)')
    parser.add_argument('-l', '--limit', type=int, 
                       help='–õ–∏–º–∏—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∞–≤—Ç–æ–ª–∏–º–∏—Ç: 10,000)')
    parser.add_argument('-s', '--batch-size', type=int, default=1000, 
                       help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1000)')
    parser.add_argument('-o', '--optimized', action='store_true', 
                       help='–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤')
    parser.add_argument('-m', '--multi-engine', action='store_true', 
                       help='–ü—Ä–æ—Å—Ç–æ–π –º—É–ª—å—Ç–∏-–¥–≤–∏–∂–∫–æ–≤—ã–π –ø–æ–∏—Å–∫')
    parser.add_argument('--improved', action='store_true', 
                       help='üî• –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π')
    parser.add_argument('-b', '--batch', action='store_true', 
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞')
    parser.add_argument('--force-full', action='store_true',
                       help='üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–°–ï –∑–∞–ø–∏—Å–∏ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π (–º–µ–¥–ª–µ–Ω–Ω–æ!)')
    
    args = parser.parse_args()
    
    logger.info(f"Starting processing with parameters:")
    logger.info(f"  Input file: {args.input_file}")
    logger.info(f"  Similarity threshold: {args.threshold}")
    logger.info(f"  Record limit: {args.limit}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Optimized engine: {args.optimized}")
    
    result = asyncio.run(generate_search_trees(
        input_file=args.input_file, 
        similarity_threshold=args.threshold, 
        max_records=args.limit, 
        batch_size=args.batch_size,
        use_optimized=args.optimized,
        use_multi_engine=args.multi_engine,
        use_improved=args.improved,
        force_batch=args.batch,
        force_full=args.force_full
    ))
    
    if result:
        logger.info("Duplicate and analog search completed successfully")
    else:
        logger.error("Duplicate and analog search failed")

if __name__ == "__main__":
    main()