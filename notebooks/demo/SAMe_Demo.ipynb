{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMe System - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥—É–ª–µ–π\n",
    "\n",
    "**Search Analog Model Engine** - —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥—É–ª–µ–π —Å–∏—Å—Ç–µ–º—ã SAMe:\n",
    "- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "- –ü–æ–∏—Å–∫–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã\n",
    "- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "- –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç—ã\n",
    "\n",
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ —Å–∏—Å—Ç–µ–º—ã SAMe –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.8.0/ru_core_news_lg-3.8.0-py3-none-any.whl (513.4 MB)\n",
      "\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m450.9/513.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:41\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Wheel 'ru-core-news-lg' located at /private/var/folders/h8/fhfvbn0j38nd4r3z3s719g1m0000gn/T/pip-unpack-vglp2me7/ru_core_news_lg-3.8.0-py3-none-any.whl is invalid.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "üìÅ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: /Users/igor/Desktop/PythonProjects/SAMe\n",
      "üïê –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: 2025-07-17 21:21:50\n"
     ]
    }
   ],
   "source": [
    "# –°–∏—Å—Ç–µ–º–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º SAMe\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "sys.path.append(os.path.abspath('./core'))\n",
    "\n",
    "print(\"‚úÖ –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"üìÅ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")\n",
    "print(f\"üïê –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥—É–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "try:\n",
    "    from core.text_processing.text_cleaner import TextCleaner, CleaningConfig\n",
    "    from core.text_processing.lemmatizer import Lemmatizer, LemmatizerConfig\n",
    "    from core.text_processing.normalizer import TextNormalizer, NormalizerConfig\n",
    "    from core.text_processing.preprocessor import TextPreprocessor, PreprocessorConfig\n",
    "    print(\"‚úÖ –ú–æ–¥—É–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ core/text_processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥—É–ª–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe - –ü–æ–∏—Å–∫–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã\n",
    "try:\n",
    "    from core.search_engine.fuzzy_search import FuzzySearchEngine, FuzzySearchConfig\n",
    "    from core.search_engine.semantic_search import SemanticSearchEngine, SemanticSearchConfig\n",
    "    from core.search_engine.hybrid_search import HybridSearchEngine, HybridSearchConfig\n",
    "    from core.search_engine.indexer import SearchIndexer, IndexConfig\n",
    "    print(\"‚úÖ –ú–æ–¥—É–ª–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ core/search_engine/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥—É–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "try:\n",
    "    from core.parameter_extraction.regex_extractor import (\n",
    "        RegexParameterExtractor, ParameterPattern, ParameterType, ExtractedParameter\n",
    "    )\n",
    "    from core.parameter_extraction.ml_extractor import MLParameterExtractor, MLExtractorConfig\n",
    "    from core.parameter_extraction.parameter_parser import ParameterParser, ParameterParserConfig\n",
    "    print(\"‚úÖ –ú–æ–¥—É–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ core/parameter_extraction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥—É–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe - –≠–∫—Å–ø–æ—Ä—Ç\n",
    "try:\n",
    "    from core.export.excel_exporter import ExcelExporter, ExcelExportConfig\n",
    "    from core.export.report_generator import ReportGenerator, ReportConfig\n",
    "    print(\"‚úÖ –ú–æ–¥—É–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ core/export/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ–∑–¥–∞–Ω–æ 37 –æ–±—Ä–∞–∑—Ü–æ–≤ –ú–¢–† –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
      "\n",
      "üìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\n",
      "1. –ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π\n",
      "2. –ë–æ–ª—Ç —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π –ú12√ó60 DIN 933 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å A2\n",
      "3. –í–∏–Ω—Ç –ú8√ó30 —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫–æ–º –ì–û–°–¢ 11738-84\n",
      "4. –ì–∞–π–∫–∞ –ú10 —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è –ì–û–°–¢ 5915-70 –∫–ª–∞—Å—Å –ø—Ä–æ—á–Ω–æ—Å—Ç–∏ 8\n",
      "5. –ì–∞–π–∫–∞ –ú12 DIN 934 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å A4\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ú–¢–†\n",
    "def create_sample_mtr_data():\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ú–¢–†\"\"\"\n",
    "    \n",
    "    sample_data = [\n",
    "        # –ö—Ä–µ–ø–µ–∂–Ω—ã–µ –∏–∑–¥–µ–ª–∏—è\n",
    "        \"–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π\",\n",
    "        \"–ë–æ–ª—Ç —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π –ú12√ó60 DIN 933 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å A2\",\n",
    "        \"–í–∏–Ω—Ç –ú8√ó30 —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫–æ–º –ì–û–°–¢ 11738-84\",\n",
    "        \"–ì–∞–π–∫–∞ –ú10 —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è –ì–û–°–¢ 5915-70 –∫–ª–∞—Å—Å –ø—Ä–æ—á–Ω–æ—Å—Ç–∏ 8\",\n",
    "        \"–ì–∞–π–∫–∞ –ú12 DIN 934 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å A4\",\n",
    "        \"–®–∞–π–±–∞ –ø–ª–æ—Å–∫–∞—è 10 –ì–û–°–¢ 11371-78 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è\",\n",
    "        \"–®–∞–π–±–∞ –ø—Ä—É–∂–∏–Ω–Ω–∞—è (–≥—Ä–æ–≤–µ—Ä) 12 –ì–û–°–¢ 6402-70\",\n",
    "        \"–®–ø–∏–ª—å–∫–∞ —Ä–µ–∑—å–±–æ–≤–∞—è –ú16√ó100 –ì–û–°–¢ 22032-76\",\n",
    "        \n",
    "        # –≠–ª–µ–∫—Ç—Ä–æ–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ\n",
    "        \"–î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ê–ò–†80–í2 1.5–∫–í—Ç 3000–æ–±/–º–∏–Ω 220/380–í\",\n",
    "        \"–≠–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å 4–ê–ú100L4 4–∫–í—Ç 1500–æ–±/–º–∏–Ω –ª–∞–ø—ã\",\n",
    "        \"–î–≤–∏–≥–∞—Ç–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ç–æ–∫–∞ –ü-31 0.37–∫–í—Ç 1500–æ–±/–º–∏–Ω 110–í\",\n",
    "        \"–ö–∞–±–µ–ª—å –í–í–ì 3√ó2.5 –º–º¬≤ 0.66–∫–í –º–µ–¥–Ω—ã–π\",\n",
    "        \"–ü—Ä–æ–≤–æ–¥ –ü–í–° 2√ó1.5 –º–º¬≤ –≥–∏–±–∫–∏–π –º–µ–¥–Ω—ã–π\",\n",
    "        \"–ö–∞–±–µ–ª—å —Å–∏–ª–æ–≤–æ–π –ê–í–ë–±–®–≤ 4√ó25 –º–º¬≤ 10–∫–í –∞–ª—é–º–∏–Ω–∏–µ–≤—ã–π\",\n",
    "        \"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–∫–ª—é—á–∞—Ç–µ–ª—å –í–ê47-29 –°16 1–ü IEK\",\n",
    "        \"–ö–æ–Ω—Ç–∞–∫—Ç–æ—Ä –ö–ú–ò-23210 25–ê 220–í AC3 IEK\",\n",
    "        \n",
    "        # –¢—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–Ω–∞—è –∞—Ä–º–∞—Ç—É—Ä–∞\n",
    "        \"–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57√ó3.5 –ì–û–°–¢ 8732-78 –±–µ—Å—à–æ–≤–Ω–∞—è\",\n",
    "        \"–¢—Ä—É–±–∞ –ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤–∞—è PN20 32√ó5.4 –¥–ª—è –≥–æ—Ä—è—á–µ–π –≤–æ–¥—ã\",\n",
    "        \"–§–ª–∞–Ω–µ—Ü –ø–ª–æ—Å–∫–∏–π –î–£50 –†–£16 –ì–û–°–¢ 12820-80 —Å—Ç–∞–ª—å 20\",\n",
    "        \"–ö–ª–∞–ø–∞–Ω —à–∞—Ä–æ–≤–æ–π –î–£25 –†–£40 –º—É—Ñ—Ç–æ–≤—ã–π –ª–∞—Ç—É–Ω—å\",\n",
    "        \"–ó–∞–¥–≤–∏–∂–∫–∞ —á—É–≥—É–Ω–Ω–∞—è –î–£100 –†–£16 —Ñ–ª–∞–Ω—Ü–µ–≤–∞—è 30—á76–±—Ä\",\n",
    "        \"–í–µ–Ω—Ç–∏–ª—å –∑–∞–ø–æ—Ä–Ω—ã–π –î–£15 –†–£16 –º—É—Ñ—Ç–æ–≤—ã–π 15–∫—á11—Ä\",\n",
    "        \"–û–±—Ä–∞—Ç–Ω—ã–π –∫–ª–∞–ø–∞–Ω –î–£32 –†–£16 –º—É—Ñ—Ç–æ–≤—ã–π 16–∫—á9–ø\",\n",
    "        \n",
    "        # –ü–æ–¥—à–∏–ø–Ω–∏–∫–∏ –∏ –¥–µ—Ç–∞–ª–∏ –º–∞—à–∏–Ω\n",
    "        \"–ü–æ–¥—à–∏–ø–Ω–∏–∫ —à–∞—Ä–∏–∫–æ–≤—ã–π 6205 2RS (25√ó52√ó15) –ì–û–°–¢ 8338-75\",\n",
    "        \"–ü–æ–¥—à–∏–ø–Ω–∏–∫ —Ä–æ–ª–∏–∫–æ–≤—ã–π –∫–æ–Ω–∏—á–µ—Å–∫–∏–π 7208–ê (40√ó80√ó18)\",\n",
    "        \"–†–µ–º–µ–Ω—å –∫–ª–∏–Ω–æ–≤–æ–π –ê-1000 –ì–û–°–¢ 1284.1-89\",\n",
    "        \"–¶–µ–ø—å –ø—Ä–∏–≤–æ–¥–Ω–∞—è –ü–†-15.875-2300 (1 –¥—é–π–º) –ì–û–°–¢ 13568-97\",\n",
    "        \"–ú—É—Ñ—Ç–∞ —É–ø—Ä—É–≥–∞—è –ú–£–í–ü-4 d=40–º–º\",\n",
    "        \"–†–µ–¥—É–∫—Ç–æ—Ä —á–µ—Ä–≤—è—á–Ω—ã–π –ß-80 –ø–µ—Ä–µ–¥–∞—Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ 40\",\n",
    "        \n",
    "        # –ò–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–±–æ—Ä—ã\n",
    "        \"–ú–∞–Ω–æ–º–µ—Ç—Ä –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π –ú–ü3-–£ 0-10 –∫–≥—Å/—Å–º¬≤ –ú20√ó1.5\",\n",
    "        \"–¢–µ—Ä–º–æ–º–µ—Ç—Ä –±–∏–º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–π –¢–ë-63 0-120¬∞C L=100–º–º\",\n",
    "        \"–†–∞—Å—Ö–æ–¥–æ–º–µ—Ä —ç–ª–µ–∫—Ç—Ä–æ–º–∞–≥–Ω–∏—Ç–Ω—ã–π –ü–†–≠-10 –î–£50\",\n",
    "        \"–î–∞—Ç—á–∏–∫ –¥–∞–≤–ª–µ–Ω–∏—è –ú–µ—Ç—Ä–∞–Ω-100 0-1.6–ú–ü–∞ 4-20–º–ê\",\n",
    "        \n",
    "        # –ù–∞—Å–æ—Å—ã –∏ –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä—ã\n",
    "        \"–ù–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –ö50-32-125 –ø–æ–¥–∞—á–∞ 12.5–º¬≥/—á –Ω–∞–ø–æ—Ä 20–º\",\n",
    "        \"–ù–∞—Å–æ—Å –ø–æ–≥—Ä—É–∂–Ω–æ–π –≠–¶–í6-10-110 –ø–æ–¥–∞—á–∞ 10–º¬≥/—á –Ω–∞–ø–æ—Ä 110–º\",\n",
    "        \"–ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä –ø–æ—Ä—à–Ω–µ–≤–æ–π –°–û-7–ë –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 0.7–º¬≥/–º–∏–Ω\",\n",
    "        \"–í–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä —Ä–∞–¥–∏–∞–ª—å–Ω—ã–π –í–†-80-75 ‚Ññ5 1500–æ–±/–º–∏–Ω\"\n",
    "    ]\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "sample_mtr_data = create_sample_mtr_data()\n",
    "\n",
    "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sample_mtr_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –ú–¢–† –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\")\n",
    "print(\"\\nüìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "for i, item in enumerate(sample_mtr_data[:5], 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "–ú–æ–¥—É–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ –æ—á–∏—Å—Ç–∫—É, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π –ú–¢–†."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 TextCleaner - –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "–£–¥–∞–ª—è–µ—Ç HTML —Ç–µ–≥–∏, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextCleaner\n",
      "==================================================\n",
      "\n",
      "üìù –ü—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\n",
      "\n",
      "1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '<p>–ë–æ–ª—Ç –ú10√ó50 @#$% –ì–û–°–¢ 7798-70</p>'\n",
      "   –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–ë–æ–ª—Ç –ú10 50 –ì–û–°–¢ 7798-70'\n",
      "   –°–∂–∞—Ç–∏–µ: 36 ‚Üí 24 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "\n",
      "2. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–î–≤–∏–≥–∞—Ç–µ–ª—å    –ê–ò–†80–í2     1.5–∫–í—Ç   !!!   3000–æ–±/–º–∏–Ω'\n",
      "   –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–î–≤–∏–≥–∞—Ç–µ–ª—å –ê–ò–†80–í2 1.5–∫–í—Ç 3000–æ–± –º–∏–Ω'\n",
      "   –°–∂–∞—Ç–∏–µ: 50 ‚Üí 35 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "\n",
      "3. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–ö–ê–ë–ï–õ–¨ –í–í–ì 3√ó2.5 –º–º¬≤ &nbsp; 0.66–∫–í <b>–º–µ–¥–Ω—ã–π</b>'\n",
      "   –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–ö–ê–ë–ï–õ–¨ –í–í–ì 3 2.5 –º–º¬≤ 0.66–∫–í –º–µ–¥–Ω—ã–π'\n",
      "   –°–∂–∞—Ç–∏–µ: 48 ‚Üí 34 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "\n",
      "4. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57√ó3.5    –ì–û–°–¢ 8732-78   (–±–µ—Å—à–æ–≤–Ω–∞—è)'\n",
      "   –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57 3.5 –ì–û–°–¢ 8732-78 (–±–µ—Å—à–æ–≤–Ω–∞—è)'\n",
      "   –°–∂–∞—Ç–∏–µ: 51 ‚Üí 46 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏:\n",
      "–û–±—â–µ–µ —Å–∂–∞—Ç–∏–µ: 24.9%\n",
      "–°–∏–º–≤–æ–ª–æ–≤ —É–¥–∞–ª–µ–Ω–æ: 46\n"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextCleaner\n",
    "print(\"üßπ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextCleaner\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä –æ—á–∏—Å—Ç–∏—Ç–µ–ª—è\n",
    "cleaning_config = CleaningConfig(\n",
    "    remove_html=True,\n",
    "    remove_special_chars=True,\n",
    "    remove_extra_spaces=True,\n",
    "    remove_numbers=False\n",
    ")\n",
    "\n",
    "text_cleaner = TextCleaner(cleaning_config)\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å \"–≥—Ä—è–∑–Ω—ã–º\" —Ç–µ–∫—Å—Ç–æ–º\n",
    "dirty_samples = [\n",
    "    \"<p>–ë–æ–ª—Ç –ú10√ó50 @#$% –ì–û–°–¢ 7798-70</p>\",\n",
    "    \"–î–≤–∏–≥–∞—Ç–µ–ª—å    –ê–ò–†80–í2     1.5–∫–í—Ç   !!!   3000–æ–±/–º–∏–Ω\",\n",
    "    \"–ö–ê–ë–ï–õ–¨ –í–í–ì 3√ó2.5 –º–º¬≤ &nbsp; 0.66–∫–í <b>–º–µ–¥–Ω—ã–π</b>\",\n",
    "    \"–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57√ó3.5    –ì–û–°–¢ 8732-78   (–±–µ—Å—à–æ–≤–Ω–∞—è)\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "cleaning_results = []\n",
    "\n",
    "for i, dirty_text in enumerate(dirty_samples, 1):\n",
    "    result = text_cleaner.clean_text(dirty_text)\n",
    "    cleaning_results.append(result)\n",
    "    \n",
    "    print(f\"\\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
    "    print(f\"   '{dirty_text}'\")\n",
    "    print(f\"   –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
    "    print(f\"   '{result['normalized']}'\")\n",
    "    print(f\"   –°–∂–∞—Ç–∏–µ: {len(dirty_text)} ‚Üí {len(result['normalized'])} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏\n",
    "print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏:\")\n",
    "total_original = sum(len(r['raw']) for r in cleaning_results)\n",
    "total_cleaned = sum(len(r['normalized']) for r in cleaning_results)\n",
    "compression_ratio = (total_original - total_cleaned) / total_original * 100\n",
    "\n",
    "print(f\"–û–±—â–µ–µ —Å–∂–∞—Ç–∏–µ: {compression_ratio:.1f}%\")\n",
    "print(f\"–°–∏–º–≤–æ–ª–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {total_original - total_cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TextNormalizer - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
    "\n",
    "–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextNormalizer\n",
      "==================================================\n",
      "\n",
      "üìù –ü—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
      "\n",
      "1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '—ç–ª –¥–≤–∏–≥–∞—Ç–µ–ª—å 4 –∫–∏–ª–æ–≤–∞—Ç—Ç–∞ 1500 –æ–±–æ—Ä–æ—Ç–æ–≤ –≤ –º–∏–Ω—É—Ç—É'\n",
      "   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π:\n",
      "   '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π –¥–≤–∏–≥–∞—Ç–µ–ª—å 4 –∫–í—Ç 1500 –æ–±–æ—Ä–æ—Ç–æ–≤ –≤ –º–∏–Ω—É—Ç—É'\n",
      "\n",
      "2. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '—Ç—Ä—É–±–∞ –¥–∏–∞–º–µ—Ç—Ä 57 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤ —Ç–æ–ª—â–∏–Ω–∞ 3.5 –º–º'\n",
      "   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π:\n",
      "   '—Ç—Ä—É–±–∞ –¥–∏–∞–º–µ—Ç—Ä 57 –º–º —Ç–æ–ª—â–∏–Ω–∞ 3.5 –º–º'\n",
      "\n",
      "3. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–∫–∞–±–µ–ª—å —Å–µ—á–µ–Ω–∏–µ 2.5 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ 660 –≤–æ–ª—å—Ç'\n",
      "   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π:\n",
      "   '–∫–∞–±–µ–ª—å —Å–µ—á–µ–Ω–∏–µ 2.5 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–º –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ 660 –í'\n",
      "\n",
      "4. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
      "   '–Ω–∞—Å–æ—Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 12.5 –∫—É–±–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–æ–≤ –≤ —á–∞—Å'\n",
      "   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π:\n",
      "   '–Ω–∞—Å–æ—Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 12.5 –∫—É–±–∏—á–µ—Å–∫–∏—Ö –º –≤ —á–∞—Å'\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: 4\n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: 51.0\n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: 45.8\n"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextNormalizer\n",
    "print(\"üîß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextNormalizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "normalizer_config = NormalizerConfig(\n",
    "\n",
    "    normalize_abbreviations=True,\n",
    "    unify_technical_terms=True\n",
    ")\n",
    "\n",
    "text_normalizer = TextNormalizer(normalizer_config)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "normalization_samples = [\n",
    "    \"—ç–ª –¥–≤–∏–≥–∞—Ç–µ–ª—å 4 –∫–∏–ª–æ–≤–∞—Ç—Ç–∞ 1500 –æ–±–æ—Ä–æ—Ç–æ–≤ –≤ –º–∏–Ω—É—Ç—É\",\n",
    "    \"—Ç—Ä—É–±–∞ –¥–∏–∞–º–µ—Ç—Ä 57 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤ —Ç–æ–ª—â–∏–Ω–∞ 3.5 –º–º\",\n",
    "    \"–∫–∞–±–µ–ª—å —Å–µ—á–µ–Ω–∏–µ 2.5 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ 660 –≤–æ–ª—å—Ç\",\n",
    "    \"–Ω–∞—Å–æ—Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 12.5 –∫—É–±–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–æ–≤ –≤ —á–∞—Å\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\")\n",
    "normalization_results = []\n",
    "\n",
    "for i, text in enumerate(normalization_samples, 1):\n",
    "    result = text_normalizer.normalize_text(text)\n",
    "    normalization_results.append(result)\n",
    "    \n",
    "    print(f\"\\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
    "    print(f\"   '{text}'\")\n",
    "    print(f\"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π:\")\n",
    "    print(f\"   '{result['final_normalized']}'\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\n",
    "    specs = text_normalizer.extract_technical_specs(text)\n",
    "    if specs:\n",
    "        print(f\"   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {', '.join(specs)}\")\n",
    "\n",
    "print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(normalization_results)}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {np.mean([len(r['original']) for r in normalization_results]):.1f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {np.mean([len(r['final_normalized']) for r in normalization_results]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Lemmatizer - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∫ –∏—Ö –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Lemmatizer\n",
      "==================================================\n",
      "\n",
      "üìù –ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\n",
      "\n",
      "1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '–±–æ–ª—Ç—ã —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–º–∏ –≥–æ–ª–æ–≤–∫–∞–º–∏'\n",
      "   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '–±–æ–ª—Ç —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–º–∏ –≥–æ–ª–æ–≤–∫–∞'\n",
      "   –¢–æ–∫–µ–Ω—ã: ['–±–æ–ª—Ç—ã', '—Å', '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–º–∏', '–≥–æ–ª–æ–≤–∫–∞–º–∏']\n",
      "   –õ–µ–º–º—ã: ['–±–æ–ª—Ç', '—Å', '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–º–∏', '–≥–æ–ª–æ–≤–∫–∞']\n",
      "\n",
      "2. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '–¥–≤–∏–≥–∞—Ç–µ–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–µ'\n",
      "   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '–¥–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–µ'\n",
      "   –¢–æ–∫–µ–Ω—ã: ['–¥–≤–∏–≥–∞—Ç–µ–ª–∏', '–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ', '—Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–µ']\n",
      "   –õ–µ–º–º—ã: ['–¥–≤–∏–≥–∞—Ç–µ–ª—å', '–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π', '—Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–µ']\n",
      "\n",
      "3. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '—Ç—Ä—É–±—ã —Å—Ç–∞–ª—å–Ω—ã–µ –±–µ—Å—à–æ–≤–Ω—ã–µ'\n",
      "   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '—Ç—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–æ–π –±–µ—Å—à–æ–≤–Ω—ã–π'\n",
      "   –¢–æ–∫–µ–Ω—ã: ['—Ç—Ä—É–±—ã', '—Å—Ç–∞–ª—å–Ω—ã–µ', '–±–µ—Å—à–æ–≤–Ω—ã–µ']\n",
      "   –õ–µ–º–º—ã: ['—Ç—Ä—É–±–∞', '—Å—Ç–∞–ª—å–Ω–æ–π', '–±–µ—Å—à–æ–≤–Ω—ã–π']\n",
      "\n",
      "4. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '–Ω–∞—Å–æ—Å—ã —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–µ –ø–æ–≥—Ä—É–∂–Ω—ã–µ'\n",
      "   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '–Ω–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –ø–æ–≥—Ä—É–∂–Ω–æ–π'\n",
      "   –¢–æ–∫–µ–Ω—ã: ['–Ω–∞—Å–æ—Å—ã', '—Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–µ', '–ø–æ–≥—Ä—É–∂–Ω—ã–µ']\n",
      "   –õ–µ–º–º—ã: ['–Ω–∞—Å–æ—Å', '—Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π', '–ø–æ–≥—Ä—É–∂–Ω–æ–π']\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\n",
      "–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: 13\n",
      "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º: 13\n",
      "–°–∂–∞—Ç–∏–µ —Å–ª–æ–≤–∞—Ä—è: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Lemmatizer\n",
    "print(\"üìö –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Lemmatizer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞\n",
    "lemmatizer_config = LemmatizerConfig(\n",
    "    model_name=\"ru_core_news_sm\",\n",
    "    preserve_technical_terms=True,\n",
    "    min_token_length=2\n",
    ")\n",
    "\n",
    "try:\n",
    "    lemmatizer = Lemmatizer(lemmatizer_config)\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "    lemmatization_samples = [\n",
    "        \"–±–æ–ª—Ç—ã —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–º–∏ –≥–æ–ª–æ–≤–∫–∞–º–∏\",\n",
    "        \"–¥–≤–∏–≥–∞—Ç–µ–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–µ\",\n",
    "        \"—Ç—Ä—É–±—ã —Å—Ç–∞–ª—å–Ω—ã–µ –±–µ—Å—à–æ–≤–Ω—ã–µ\",\n",
    "        \"–Ω–∞—Å–æ—Å—ã —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–µ –ø–æ–≥—Ä—É–∂–Ω—ã–µ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
    "    lemmatization_results = []\n",
    "    \n",
    "    for i, text in enumerate(lemmatization_samples, 1):\n",
    "        result = lemmatizer.lemmatize_text(text)\n",
    "        lemmatization_results.append(result)\n",
    "        \n",
    "        print(f\"\\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{text}'\")\n",
    "        print(f\"   –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '{result['lemmatized']}'\")\n",
    "        print(f\"   –¢–æ–∫–µ–Ω—ã: {result['tokens']}\")\n",
    "        print(f\"   –õ–µ–º–º—ã: {result['lemmas']}\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "    print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:\")\n",
    "    total_tokens = sum(len(r['tokens']) for r in lemmatization_results)\n",
    "    unique_lemmas = set()\n",
    "    for r in lemmatization_results:\n",
    "        unique_lemmas.update(r['lemmas'])\n",
    "    \n",
    "    print(f\"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}\")\n",
    "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º: {len(unique_lemmas)}\")\n",
    "    print(f\"–°–∂–∞—Ç–∏–µ —Å–ª–æ–≤–∞—Ä—è: {(1 - len(unique_lemmas)/total_tokens)*100:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å SpaCy: python -m spacy download ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TextPreprocessor - –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "\n",
    "–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –µ–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SpaCy model ru_core_news_lg not found. Please install it:\n",
      "python -m spacy download ru_core_news_lg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextPreprocessor (–ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω)\n",
      "============================================================\n",
      "‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: [E050] Can't find model 'ru_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ –º–æ–¥—É–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "print(\"üîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è TextPreprocessor (–ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "cleaning_config = CleaningConfig(\n",
    "    remove_html=True,\n",
    "    remove_special_chars=True,\n",
    "    remove_extra_spaces=True,\n",
    "    remove_numbers=False,\n",
    "    preserve_technical_terms=True\n",
    ")\n",
    "\n",
    "lemmatizer_config = LemmatizerConfig(\n",
    "    model_name=\"ru_core_news_lg\",\n",
    "    preserve_technical_terms=True,\n",
    "    min_token_length=2,\n",
    "    preserve_numbers=True\n",
    ")\n",
    "\n",
    "normalizer_config = NormalizerConfig(\n",
    "    standardize_units=True,\n",
    "    normalize_abbreviations=True,\n",
    "    unify_technical_terms=True,\n",
    "    remove_brand_names=False,\n",
    "    standardize_numbers=True\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    cleaning_config=cleaning_config,\n",
    "    lemmatizer_config=lemmatizer_config,\n",
    "    normalizer_config=normalizer_config,\n",
    "    save_intermediate_steps=True,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "try:\n",
    "    text_preprocessor = TextPreprocessor(preprocessor_config)\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –Ω–∞—à–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    preprocessing_samples = sample_mtr_data[:8]\n",
    "    \n",
    "    print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "    preprocessing_results = []\n",
    "    \n",
    "    for i, text in enumerate(preprocessing_samples, 1):\n",
    "        start_time = time.time()\n",
    "        result = text_preprocessor.preprocess_text(text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        preprocessing_results.append(result)\n",
    "        \n",
    "        print(f\"\\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
    "        print(f\"   '{text}'\")\n",
    "        print(f\"   –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\")\n",
    "        print(f\"   '{result['final_text']}'\")\n",
    "        print(f\"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f}–º—Å\")\n",
    "        print(f\"   –£—Å–ø–µ—à–Ω–æ: {'‚úÖ' if result['processing_successful'] else '‚ùå'}\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    df_preprocessing = pd.DataFrame({\n",
    "        'original': [r['original'] for r in preprocessing_results],\n",
    "        'final_text': [r['final_text'] for r in preprocessing_results],\n",
    "        'success': [r['processing_successful'] for r in preprocessing_results],\n",
    "        'original_length': [len(r['original']) for r in preprocessing_results],\n",
    "        'final_length': [len(r['final_text']) for r in preprocessing_results]\n",
    "    })\n",
    "    \n",
    "    df_preprocessing['compression_ratio'] = (\n",
    "        (df_preprocessing['original_length'] - df_preprocessing['final_length']) / \n",
    "        df_preprocessing['original_length'] * 100\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "    print(f\"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {df_preprocessing['success'].sum()}/{len(df_preprocessing)}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ —Å–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞: {df_preprocessing['compression_ratio'].mean():.1f}%\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ: {df_preprocessing['original_length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ: {df_preprocessing['final_length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    ax1.scatter(df_preprocessing['original_length'], df_preprocessing['final_length'], alpha=0.7)\n",
    "    ax1.plot([0, df_preprocessing['original_length'].max()], [0, df_preprocessing['original_length'].max()], 'r--', alpha=0.5)\n",
    "    ax1.set_xlabel('–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞')\n",
    "    ax1.set_ylabel('–î–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞')\n",
    "    ax1.set_title('–°–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–∂–∞—Ç–∏—è\n",
    "    ax2.hist(df_preprocessing['compression_ratio'], bins=8, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è (%)')\n",
    "    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤')\n",
    "    ax2.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–∂–∞—Ç–∏—è')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}\")\n",
    "    print(\"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ –º–æ–¥—É–ª–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n",
    "\n",
    "–°–∏—Å—Ç–µ–º–∞ SAMe –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç—Ä–∏ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞: –Ω–µ—á–µ—Ç–∫–∏–π, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –ú–¢–† –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –ú–¢–†\n",
      "==================================================\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ 46 –ø–æ–∑–∏—Ü–∏–π\n",
      "üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–∞:\n",
      "   id                                               name category  \\\n",
      "0   1              –ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π      –ú–¢–†   \n",
      "1   2  –ë–æ–ª—Ç —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π –ú12√ó60 DIN 933 –Ω–µ...      –ú–¢–†   \n",
      "2   3  –í–∏–Ω—Ç –ú8√ó30 —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫–æ–º –ì–û–°–¢ 11...      –ú–¢–†   \n",
      "3   4  –ì–∞–π–∫–∞ –ú10 —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è –ì–û–°–¢ 5915-70 –∫–ª–∞—Å—Å –ø—Ä–æ—á...      –ú–¢–†   \n",
      "4   5             –ì–∞–π–∫–∞ –ú12 DIN 934 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞–ª—å A4      –ú–¢–†   \n",
      "5   6        –®–∞–π–±–∞ –ø–ª–æ—Å–∫–∞—è 10 –ì–û–°–¢ 11371-78 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è      –ú–¢–†   \n",
      "6   7           –®–∞–π–±–∞ –ø—Ä—É–∂–∏–Ω–Ω–∞—è (–≥—Ä–æ–≤–µ—Ä) 12 –ì–û–°–¢ 6402-70      –ú–¢–†   \n",
      "7   8            –®–ø–∏–ª—å–∫–∞ —Ä–µ–∑—å–±–æ–≤–∞—è –ú16√ó100 –ì–û–°–¢ 22032-76      –ú–¢–†   \n",
      "8   9  –î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ê–ò–†80–í2 1.5–∫–í—Ç 3000–æ–±/–º–∏...      –ú–¢–†   \n",
      "9  10     –≠–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å 4–ê–ú100L4 4–∫–í—Ç 1500–æ–±/–º–∏–Ω –ª–∞–ø—ã      –ú–¢–†   \n",
      "\n",
      "                                         description  \n",
      "0  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞...  \n",
      "1  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ë–æ–ª—Ç —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π –ú12√ó...  \n",
      "2  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –í–∏–Ω—Ç –ú8√ó30 —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —à–µ—Å—Ç–∏–≥—Ä–∞–Ω...  \n",
      "3  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ì–∞–π–∫–∞ –ú10 —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è –ì–û–°–¢ 5915-...  \n",
      "4  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ì–∞–π–∫–∞ –ú12 DIN 934 –Ω–µ—Ä–∂–∞–≤–µ—é—â–∞—è —Å—Ç–∞...  \n",
      "5  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –®–∞–π–±–∞ –ø–ª–æ—Å–∫–∞—è 10 –ì–û–°–¢ 11371-78 –æ—Ü...  \n",
      "6  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –®–∞–π–±–∞ –ø—Ä—É–∂–∏–Ω–Ω–∞—è (–≥—Ä–æ–≤–µ—Ä) 12 –ì–û–°–¢ ...  \n",
      "7  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –®–ø–∏–ª—å–∫–∞ —Ä–µ–∑—å–±–æ–≤–∞—è –ú16√ó100 –ì–û–°–¢ 22...  \n",
      "8  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ê–ò–†80–í2 1.5...  \n",
      "9  –û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –≠–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å 4–ê–ú100L4 4–∫–í—Ç 15...  \n",
      "\n",
      "üîç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞:\n",
      "–î–æ–∫—É–º–µ–Ω—Ç–æ–≤: 46\n",
      "ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: 46\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞\n",
    "print(\"üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –ú–¢–†\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "def create_extended_catalog():\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –ú–¢–† —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\"\"\"\n",
    "    \n",
    "    catalog_data = []\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    for i, item in enumerate(sample_mtr_data):\n",
    "        catalog_data.append({\n",
    "            'id': i + 1,\n",
    "            'name': item,\n",
    "            'category': '–ú–¢–†',\n",
    "            'description': f'–û–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è {item}'\n",
    "        })\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞\n",
    "    additional_items = [\n",
    "        \"–ë–æ–ª—Ç –ú10 –¥–ª–∏–Ω–∞ 50–º–º –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π\",\n",
    "        \"–ë–æ–ª—Ç –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π 10—Ö50 –ì–û–°–¢\",\n",
    "        \"–í–∏–Ω—Ç –ú10—Ö50 —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π\",\n",
    "        \"–î–≤–∏–≥–∞—Ç–µ–ª—å —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π 1.5–∫–í—Ç 3000–æ–±/–º–∏–Ω\",\n",
    "        \"–ú–æ—Ç–æ—Ä –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π 1500–í—Ç —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω—ã–π\",\n",
    "        \"–≠–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å 1.5–∫–í—Ç –ê–ò–† —Å–µ—Ä–∏–∏\",\n",
    "        \"–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–º–µ—Ç—Ä 57–º–º\",\n",
    "        \"–¢—Ä—É–±–∫–∞ –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∞—è 57—Ö3.5\",\n",
    "        \"–¢—Ä—É–±–∞ –±–µ—Å—à–æ–≤–Ω–∞—è —Å—Ç–∞–ª—å 57*3.5–º–º\"\n",
    "    ]\n",
    "    \n",
    "    for i, item in enumerate(additional_items):\n",
    "        catalog_data.append({\n",
    "            'id': len(sample_mtr_data) + i + 1,\n",
    "            'name': item,\n",
    "            'category': '–ú–¢–†',\n",
    "            'description': f'–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è {item}'\n",
    "        })\n",
    "    \n",
    "    return catalog_data\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥\n",
    "catalog_data = create_extended_catalog()\n",
    "catalog_df = pd.DataFrame(catalog_data)\n",
    "\n",
    "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ {len(catalog_data)} –ø–æ–∑–∏—Ü–∏–π\")\n",
    "print(f\"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–∞:\")\n",
    "print(catalog_df.head(10))\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤\n",
    "documents = catalog_df['name'].tolist()\n",
    "document_ids = catalog_df['id'].tolist()\n",
    "\n",
    "print(f\"\\nüîç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞:\")\n",
    "print(f\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\")\n",
    "print(f\"ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(document_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 FuzzySearchEngine - –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è FuzzySearchEngine\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FuzzySearchConfig.__init__() got an unexpected keyword argument 'similarity_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m fuzzy_config = \u001b[43mFuzzySearchConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfidf_max_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_stemming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –¥–≤–∏–∂–æ–∫ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\u001b[39;00m\n\u001b[32m     15\u001b[39m     fuzzy_engine = FuzzySearchEngine(fuzzy_config)\n",
      "\u001b[31mTypeError\u001b[39m: FuzzySearchConfig.__init__() got an unexpected keyword argument 'similarity_threshold'"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "print(\"üîç –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è FuzzySearchEngine\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "fuzzy_config = FuzzySearchConfig(\n",
    "    tfidf_max_features=5000,\n",
    "    similarity_threshold=0.1,\n",
    "    max_results=5,\n",
    "    use_stemming=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –¥–≤–∏–∂–æ–∫ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    fuzzy_engine = FuzzySearchEngine(fuzzy_config)\n",
    "    \n",
    "    print(\"üîÑ –û–±—É—á–µ–Ω–∏–µ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...\")\n",
    "    start_time = time.time()\n",
    "    fuzzy_engine.fit(documents, document_ids)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {fit_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n",
    "    test_queries = [\n",
    "        \"–±–æ–ª—Ç –º10\",\n",
    "        \"–¥–≤–∏–≥–∞—Ç–µ–ª—å 1.5–∫–í—Ç\",\n",
    "        \"—Ç—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57\",\n",
    "        \"–Ω–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π\",\n",
    "        \"–∫–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    fuzzy_results = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        start_time = time.time()\n",
    "        results = fuzzy_engine.search(query)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        fuzzy_results[query] = results\n",
    "        \n",
    "        print(f\"\\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
    "        print(f\"   –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {search_time*1000:.1f}–º—Å\")\n",
    "        print(f\"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\")\n",
    "        \n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            print(f\"   {i}. {result['document']} (—Å—Ö–æ–∂–µ—Å—Ç—å: {result['combined_score']:.3f})\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    stats = fuzzy_engine.get_statistics()\n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    print(f\"–°—Ç–∞—Ç—É—Å: {stats['status']}\")\n",
    "    print(f\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {stats['total_documents']}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {stats['vocabulary_size']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –Ω–µ—á–µ—Ç–∫–æ–º –ø–æ–∏—Å–∫–µ: {e}\")\n",
    "    fuzzy_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 SemanticSearchEngine - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç BERT-–ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–º—ã—Å–ª–∞ –∑–∞–ø—Ä–æ—Å–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è SemanticSearchEngine\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SemanticSearchConfig.__init__() got an unexpected keyword argument 'max_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m semantic_config = \u001b[43mSemanticSearchConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –¥–≤–∏–∂–æ–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\u001b[39;00m\n\u001b[32m     15\u001b[39m     semantic_engine = SemanticSearchEngine(semantic_config)\n",
      "\u001b[31mTypeError\u001b[39m: SemanticSearchConfig.__init__() got an unexpected keyword argument 'max_results'"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "print(\"üß† –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è SemanticSearchEngine\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "semantic_config = SemanticSearchConfig(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    similarity_threshold=0.3,\n",
    "    max_results=5,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –¥–≤–∏–∂–æ–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    semantic_engine = SemanticSearchEngine(semantic_config)\n",
    "    \n",
    "    print(\"üîÑ –û–±—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...\")\n",
    "    print(\"‚ö†Ô∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ\")\n",
    "    start_time = time.time()\n",
    "    semantic_engine.fit(documents, document_ids)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {fit_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–≤–∫–ª—é—á–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ)\n",
    "    semantic_queries = [\n",
    "        \"–∫—Ä–µ–ø–µ–∂–Ω–æ–µ –∏–∑–¥–µ–ª–∏–µ –º10\",  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ \"–±–æ–ª—Ç –º10\"\n",
    "        \"—ç–ª–µ–∫—Ç—Ä–æ–º–æ—Ç–æ—Ä –ø–æ–ª—Ç–æ—Ä–∞ –∫–∏–ª–æ–≤–∞—Ç—Ç–∞\",  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ \"–¥–≤–∏–≥–∞—Ç–µ–ª—å 1.5–∫–í—Ç\"\n",
    "        \"–º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∞—è —Ç—Ä—É–±–∫–∞ 57–º–º\",  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ \"—Ç—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57\"\n",
    "        \"–≤–æ–¥—è–Ω–æ–π –Ω–∞—Å–æ—Å\",  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ \"–Ω–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π\"\n",
    "        \"—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≤–æ–¥\",  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ \"–∫–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query in semantic_queries:\n",
    "        start_time = time.time()\n",
    "        results = semantic_engine.search(query)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        semantic_results[query] = results\n",
    "        \n",
    "        print(f\"\\nüß† –ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
    "        print(f\"   –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {search_time*1000:.1f}–º—Å\")\n",
    "        print(f\"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\")\n",
    "        \n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            print(f\"   {i}. {result['document']} (—Å—Ö–æ–∂–µ—Å—Ç—å: {result['similarity_score']:.3f})\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    stats = semantic_engine.get_statistics()\n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    print(f\"–°—Ç–∞—Ç—É—Å: {stats['status']}\")\n",
    "    print(f\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {stats['total_documents']}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {stats['embedding_dimension']}\")\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {stats['model_name']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: {e}\")\n",
    "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: sentence-transformers, faiss-cpu\")\n",
    "    semantic_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 HybridSearchEngine - –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫\n",
    "\n",
    "–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—á–µ—Ç–∫–æ–≥–æ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è HybridSearchEngine\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HybridSearchConfig.__init__() got an unexpected keyword argument 'similarity_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m hybrid_config = \u001b[43mHybridSearchConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuzzy_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msemantic_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted_sum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫\u001b[39;00m\n\u001b[32m     16\u001b[39m     hybrid_engine = HybridSearchEngine(hybrid_config)\n",
      "\u001b[31mTypeError\u001b[39m: HybridSearchConfig.__init__() got an unexpected keyword argument 'similarity_threshold'"
     ]
    }
   ],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "print(\"‚ö° –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è HybridSearchEngine\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "hybrid_config = HybridSearchConfig(\n",
    "    fuzzy_weight=0.4,\n",
    "    semantic_weight=0.6,\n",
    "    combination_strategy=\"weighted_sum\",\n",
    "    similarity_threshold=0.2,\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫\n",
    "    hybrid_engine = HybridSearchEngine(hybrid_config)\n",
    "    \n",
    "    print(\"üîÑ –û–±—É—á–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...\")\n",
    "    start_time = time.time()\n",
    "    hybrid_engine.fit(documents, document_ids)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {fit_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤\n",
    "    comparison_queries = [\n",
    "        \"–±–æ–ª—Ç –º10\",\n",
    "        \"–¥–≤–∏–≥–∞—Ç–µ–ª—å 1.5–∫–í—Ç\",\n",
    "        \"—Ç—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    hybrid_results = {}\n",
    "    \n",
    "    for query in comparison_queries:\n",
    "        start_time = time.time()\n",
    "        results = hybrid_engine.search(query)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        hybrid_results[query] = results\n",
    "        \n",
    "        print(f\"\\n‚ö° –ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
    "        print(f\"   –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {search_time*1000:.1f}–º—Å\")\n",
    "        print(f\"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\")\n",
    "        \n",
    "        for i, result in enumerate(results[:3], 1):\n",
    "            print(f\"   {i}. {result['document']} (–≥–∏–±—Ä–∏–¥–Ω—ã–π —Å–∫–æ—Ä: {result['hybrid_score']:.3f})\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    stats = hybrid_engine.get_statistics()\n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:\")\n",
    "    print(f\"–°—Ç–∞—Ç—É—Å: {stats['status']}\")\n",
    "    print(f\"–°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {stats['combination_strategy']}\")\n",
    "    print(f\"–í–µ—Å–∞: –Ω–µ—á–µ—Ç–∫–∏–π={stats['weights']['fuzzy']}, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π={stats['weights']['semantic']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}\")\n",
    "    hybrid_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "\n",
    "–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fuzzy_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfuzzy_results\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m semantic_results \u001b[38;5;129;01mand\u001b[39;00m hybrid_results:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\u001b[39;00m\n\u001b[32m      7\u001b[39m     comparison_data = []\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m comparison_queries:\n",
      "\u001b[31mNameError\u001b[39m: name 'fuzzy_results' is not defined"
     ]
    }
   ],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "print(\"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if fuzzy_results and semantic_results and hybrid_results:\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "    comparison_data = []\n",
    "    \n",
    "    for query in comparison_queries:\n",
    "        if query in fuzzy_results and query in semantic_results and query in hybrid_results:\n",
    "            fuzzy_top = fuzzy_results[query][0] if fuzzy_results[query] else None\n",
    "            semantic_top = semantic_results.get(query, [{}])[0] if semantic_results.get(query) else None\n",
    "            hybrid_top = hybrid_results[query][0] if hybrid_results[query] else None\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'query': query,\n",
    "                'fuzzy_result': fuzzy_top['document'] if fuzzy_top else 'N/A',\n",
    "                'fuzzy_score': fuzzy_top['combined_score'] if fuzzy_top else 0,\n",
    "                'semantic_result': semantic_top.get('document', 'N/A') if semantic_top else 'N/A',\n",
    "                'semantic_score': semantic_top.get('similarity_score', 0) if semantic_top else 0,\n",
    "                'hybrid_result': hybrid_top['document'] if hybrid_top else 'N/A',\n",
    "                'hybrid_score': hybrid_top['hybrid_score'] if hybrid_top else 0\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"\\nüîç –ó–∞–ø—Ä–æ—Å: '{row['query']}'\")\n",
    "        print(f\"   –ù–µ—á–µ—Ç–∫–∏–π:     {row['fuzzy_result'][:50]}... (—Å–∫–æ—Ä: {row['fuzzy_score']:.3f})\")\n",
    "        print(f\"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π: {row['semantic_result'][:50]}... (—Å–∫–æ—Ä: {row['semantic_score']:.3f})\")\n",
    "        print(f\"   –ì–∏–±—Ä–∏–¥–Ω—ã–π:    {row['hybrid_result'][:50]}... (—Å–∫–æ—Ä: {row['hybrid_score']:.3f})\")\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_queries))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, comparison_df['fuzzy_score'], width, label='–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫', alpha=0.8)\n",
    "    ax.bar(x, comparison_df['semantic_score'], width, label='–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫', alpha=0.8)\n",
    "    ax.bar(x + width, comparison_df['hybrid_score'], width, label='–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('–ó–∞–ø—Ä–æ—Å—ã')\n",
    "    ax.set_ylabel('–°–∫–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏')\n",
    "    ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(comparison_queries, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {comparison_df['fuzzy_score'].mean():.3f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {comparison_df['semantic_score'].mean():.3f}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {comparison_df['hybrid_score'].mean():.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –ù–µ –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "–°–∏—Å—Ç–µ–º–∞ SAMe –º–æ–∂–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π –ú–¢–†."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 RegexParameterExtractor - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ regex\n",
    "print(\"üîß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è RegexParameterExtractor\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    regex_extractor = RegexParameterExtractor()\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    parameter_samples = [\n",
    "        \"–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –¥–∏–∞–º–µ—Ç—Ä 10–º–º –¥–ª–∏–Ω–∞ 50–º–º\",\n",
    "        \"–î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π 4–∫–í—Ç 1500–æ–±/–º–∏–Ω –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ 380–í\",\n",
    "        \"–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è 57√ó3.5 –¥–∏–∞–º–µ—Ç—Ä 57–º–º —Ç–æ–ª—â–∏–Ω–∞ —Å—Ç–µ–Ω–∫–∏ 3.5–º–º\",\n",
    "        \"–ö–∞–±–µ–ª—å –í–í–ì 3√ó2.5–º–º¬≤ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ 0.66–∫–í —Å–µ—á–µ–Ω–∏–µ 2.5–º–º¬≤\",\n",
    "        \"–ù–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –ø–æ–¥–∞—á–∞ 12.5–º¬≥/—á –Ω–∞–ø–æ—Ä 20–º –º–æ—â–Ω–æ—Å—Ç—å 1.1–∫–í—Ç\",\n",
    "        \"–ú–∞–Ω–æ–º–µ—Ç—Ä 0-16 –∫–≥—Å/—Å–º¬≤ —Ä–µ–∑—å–±–∞ –ú20√ó1.5 –∫–ª–∞—Å—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ 1.5\",\n",
    "        \"–ü–æ–¥—à–∏–ø–Ω–∏–∫ 6205 2RS –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏–∞–º–µ—Ç—Ä 25–º–º –Ω–∞—Ä—É–∂–Ω—ã–π 52–º–º\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\")\n",
    "    extraction_results = []\n",
    "    \n",
    "    for i, text in enumerate(parameter_samples, 1):\n",
    "        parameters = regex_extractor.extract_parameters(text)\n",
    "        extraction_results.append({\n",
    "            'text': text,\n",
    "            'parameters': parameters\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{i}. –¢–µ–∫—Å—Ç: '{text}'\")\n",
    "        print(f\"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(parameters)}\")\n",
    "        \n",
    "        for param in parameters:\n",
    "            print(f\"   - {param.name}: {param.value} {param.unit or ''} ({param.parameter_type.value})\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n",
    "    total_params = sum(len(r['parameters']) for r in extraction_results)\n",
    "    param_types = {}\n",
    "    \n",
    "    for result in extraction_results:\n",
    "        for param in result['parameters']:\n",
    "            param_type = param.parameter_type.value\n",
    "            param_types[param_type] = param_types.get(param_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\")\n",
    "    print(f\"–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç: {total_params/len(parameter_samples):.1f}\")\n",
    "    print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:\")\n",
    "    for param_type, count in param_types.items():\n",
    "        print(f\"  - {param_type}: {count}\")\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    if param_types:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        types = list(param_types.keys())\n",
    "        counts = list(param_types.values())\n",
    "        \n",
    "        bars = ax.bar(types, counts, alpha=0.8)\n",
    "        ax.set_xlabel('–¢–∏–ø—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')\n",
    "        ax.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
    "        ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Ç–∏–ø–∞–º')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                   str(count), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö\n",
    "    extractor_stats = regex_extractor.get_statistics()\n",
    "    print(f\"\\nüîç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–µ:\")\n",
    "    print(f\"–í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {extractor_stats['total_patterns']}\")\n",
    "    print(f\"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã: {', '.join(extractor_stats['supported_types'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}\")\n",
    "    extraction_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ParameterParser - –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "print(\"‚öôÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è ParameterParser\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–∞—Ä—Å–µ—Ä–∞\n",
    "    parser_config = ParameterParserConfig(\n",
    "        use_regex=True,\n",
    "        use_ml=False,  # –û—Ç–∫–ª—é—á–∞–µ–º ML –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "        min_confidence=0.5,\n",
    "        remove_duplicates=True\n",
    "    )\n",
    "    \n",
    "    parameter_parser = ParameterParser(parser_config)\n",
    "    \n",
    "    # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞—à–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(\"üîÑ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –ú–¢–†...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    sample_catalog = documents[:15]\n",
    "    batch_results = parameter_parser.parse_batch(sample_catalog)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "    parsed_data = []\n",
    "    for i, (text, params) in enumerate(zip(sample_catalog, batch_results)):\n",
    "        parsed_data.append({\n",
    "            'id': i + 1,\n",
    "            'text': text,\n",
    "            'param_count': len(params),\n",
    "            'parameters': params\n",
    "        })\n",
    "    \n",
    "    parsed_df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    print(f\"\\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ–∑–∏—Ü–∏–π: {len(parsed_df)}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {parsed_df['param_count'].mean():.1f}\")\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {parsed_df['param_count'].max()}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    top_parsed = parsed_df.nlargest(5, 'param_count')\n",
    "    \n",
    "    print(\"\\nüèÜ –¢–æ–ø-5 –ø–æ–∑–∏—Ü–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\")\n",
    "    for _, row in top_parsed.iterrows():\n",
    "        print(f\"\\n{row['id']}. {row['text'][:60]}...\")\n",
    "        print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {row['param_count']}\")\n",
    "        for param in row['parameters'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "            print(f\"   - {param.name}: {param.value} {param.unit or ''}\")\n",
    "        if len(row['parameters']) > 3:\n",
    "            print(f\"   ... –∏ –µ—â–µ {len(row['parameters']) - 3}\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞\n",
    "    parser_stats = parameter_parser.get_statistics()\n",
    "    print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞:\")\n",
    "    print(f\"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {parser_stats['total_processed']}\")\n",
    "    print(f\"–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {parser_stats['total_parameters_extracted']}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {parser_stats.get('avg_processing_time', 0)*1000:.1f}–º—Å\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞—Ä—Å–µ—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}\")\n",
    "    parsed_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    "–°–∏—Å—Ç–µ–º–∞ SAMe –º–æ–∂–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ExcelExporter - –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel\n",
    "\n",
    "–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel\n",
    "print(\"üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è ExcelExporter\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "    export_config = ExcelExportConfig(\n",
    "        include_statistics=True,\n",
    "        include_metadata=True,\n",
    "        auto_adjust_columns=True,\n",
    "        add_filters=True,\n",
    "        highlight_high_similarity=True\n",
    "    )\n",
    "    \n",
    "    excel_exporter = ExcelExporter(export_config)\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "    if fuzzy_results or semantic_results or hybrid_results:\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "        export_results = {}\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "        for query, results in fuzzy_results.items():\n",
    "            if query not in export_results:\n",
    "                export_results[query] = []\n",
    "            for result in results[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3\n",
    "                export_results[query].append({\n",
    "                    'document_id': result['document_id'],\n",
    "                    'document': result['document'],\n",
    "                    'similarity_score': result['combined_score'],\n",
    "                    'search_method': 'fuzzy',\n",
    "                    'rank': result['rank']\n",
    "                })\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "        for query, results in hybrid_results.items():\n",
    "            if query not in export_results:\n",
    "                export_results[query] = []\n",
    "            for result in results[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3\n",
    "                export_results[query].append({\n",
    "                    'document_id': result['document_id'],\n",
    "                    'document': result['document'],\n",
    "                    'similarity_score': result['hybrid_score'],\n",
    "                    'search_method': 'hybrid',\n",
    "                    'rank': result['rank']\n",
    "                })\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "        output_path = \"data/output/same_demo_results.xlsx\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        print(f\"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ Excel —Ñ–∞–π–ª–∞: {output_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        excel_exporter.export_search_results(\n",
    "            results=export_results,\n",
    "            output_path=output_path,\n",
    "            metadata={\n",
    "                'system': 'SAMe Demo',\n",
    "                'version': '1.0',\n",
    "                'export_date': datetime.now().isoformat(),\n",
    "                'catalog_size': len(documents),\n",
    "                'queries_processed': len(export_results)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        export_time = time.time() - start_time\n",
    "        print(f\"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {export_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "        total_results = sum(len(results) for results in export_results.values())\n",
    "        print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞:\")\n",
    "        print(f\"–ó–∞–ø—Ä–æ—Å–æ–≤: {len(export_results)}\")\n",
    "        print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_results}\")\n",
    "        print(f\"–§–∞–π–ª: {output_path}\")\n",
    "        print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(output_path) / 1024:.1f} KB\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "        print(f\"\\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞:\")\n",
    "        for query, results in list(export_results.items())[:3]:\n",
    "            print(f\"  –ó–∞–ø—Ä–æ—Å '{query}': {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "            for i, result in enumerate(results[:2], 1):\n",
    "                print(f\"    {i}. {result['document'][:40]}... ({result['search_method']}, {result['similarity_score']:.3f})\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä - –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "\n",
    "–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã SAMe –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä\n",
    "print(\"üöÄ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å–∏—Å—Ç–µ–º—ã SAMe\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def run_full_pipeline(user_queries: List[str]):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\"\"\"\n",
    "    \n",
    "    pipeline_results = {\n",
    "        'preprocessing': {},\n",
    "        'search_results': {},\n",
    "        'extracted_parameters': {},\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # –®–∞–≥ 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "        print(\"\\n1Ô∏è‚É£ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤...\")\n",
    "        if 'text_preprocessor' in locals():\n",
    "            preprocessed_queries = []\n",
    "            for query in user_queries:\n",
    "                result = text_preprocessor.preprocess_text(query)\n",
    "                preprocessed_queries.append(result['final_text'])\n",
    "                pipeline_results['preprocessing'][query] = result\n",
    "            print(f\"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(user_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "        else:\n",
    "            preprocessed_queries = user_queries\n",
    "            print(\"   ‚ö†Ô∏è  –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\")\n",
    "        \n",
    "        # –®–∞–≥ 2: –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤\n",
    "        print(\"\\n2Ô∏è‚É£ –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤...\")\n",
    "        if 'hybrid_engine' in locals() and hybrid_engine.is_fitted:\n",
    "            for i, (original_query, processed_query) in enumerate(zip(user_queries, preprocessed_queries)):\n",
    "                search_results = hybrid_engine.search(processed_query)\n",
    "                pipeline_results['search_results'][original_query] = search_results\n",
    "                print(f\"   üîç '{original_query}': –Ω–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  –ü–æ–∏—Å–∫ –ø—Ä–æ–ø—É—â–µ–Ω (–¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\")\n",
    "        \n",
    "        # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        print(\"\\n3Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\")\n",
    "        if 'parameter_parser' in locals():\n",
    "            for query, results in pipeline_results['search_results'].items():\n",
    "                query_params = []\n",
    "                for result in results[:3]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "                    params = parameter_parser.parse_parameters(result['document'])\n",
    "                    query_params.extend(params)\n",
    "                pipeline_results['extracted_parameters'][query] = query_params\n",
    "                print(f\"   üîß '{query}': –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(query_params)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ (–ø–∞—Ä—Å–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\")\n",
    "        \n",
    "        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        total_time = time.time() - total_start_time\n",
    "        pipeline_results['statistics'] = {\n",
    "            'total_processing_time': total_time,\n",
    "            'queries_processed': len(user_queries),\n",
    "            'total_results_found': sum(len(results) for results in pipeline_results['search_results'].values()),\n",
    "            'total_parameters_extracted': sum(len(params) for params in pipeline_results['extracted_parameters'].values()),\n",
    "            'avg_time_per_query': total_time / len(user_queries) if user_queries else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}\")\n",
    "    \n",
    "    return pipeline_results\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "user_test_queries = [\n",
    "    \"–Ω—É–∂–µ–Ω –±–æ–ª—Ç –º10 –¥–ª–∏–Ω–æ–π 50–º–º\",\n",
    "    \"—ç–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å –º–æ—â–Ω–æ—Å—Ç—å—é 1.5 –∫–∏–ª–æ–≤–∞—Ç—Ç–∞\",\n",
    "    \"—Å—Ç–∞–ª—å–Ω–∞—è —Ç—Ä—É–±–∞ –¥–∏–∞–º–µ—Ç—Ä–æ–º 57 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤\",\n",
    "    \"—Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π –Ω–∞—Å–æ—Å –¥–ª—è –≤–æ–¥—ã\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:\")\n",
    "for i, query in enumerate(user_test_queries, 1):\n",
    "    print(f\"{i}. '{query}'\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "pipeline_results = run_full_pipeline(user_test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "\n",
    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "print(\"üìà –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if pipeline_results and pipeline_results['statistics']:\n",
    "    stats = pipeline_results['statistics']\n",
    "    \n",
    "    print(f\"\\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats['total_processing_time']:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['queries_processed']}\")\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {stats['total_results_found']}\")\n",
    "    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {stats['total_parameters_extracted']}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—Ä–æ—Å: {stats['avg_time_per_query']*1000:.1f}–º—Å\")\n",
    "    \n",
    "    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º\n",
    "    print(f\"\\nüîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º:\")\n",
    "    \n",
    "    for query in user_test_queries:\n",
    "        print(f\"\\nüìù –ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
    "        \n",
    "        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        if query in pipeline_results['preprocessing']:\n",
    "            preproc = pipeline_results['preprocessing'][query]\n",
    "            print(f\"   –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: '{preproc['final_text']}'\")\n",
    "            print(f\"   –°–∂–∞—Ç–∏–µ: {len(query)} ‚Üí {len(preproc['final_text'])} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        \n",
    "        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞\n",
    "        if query in pipeline_results['search_results']:\n",
    "            search_results = pipeline_results['search_results'][query]\n",
    "            print(f\"   –ù–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤: {len(search_results)}\")\n",
    "            \n",
    "            if search_results:\n",
    "                best_result = search_results[0]\n",
    "                print(f\"   –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '{best_result['document'][:50]}...'\")\n",
    "                print(f\"   –°–∫–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏: {best_result.get('hybrid_score', 0):.3f}\")\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "        if query in pipeline_results['extracted_parameters']:\n",
    "            params = pipeline_results['extracted_parameters'][query]\n",
    "            print(f\"   –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(params)}\")\n",
    "            \n",
    "            if params:\n",
    "                unique_param_names = set(p.name for p in params)\n",
    "                print(f\"   –¢–∏–ø—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {', '.join(list(unique_param_names)[:3])}\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "    summary_data = []\n",
    "    for query in user_test_queries:\n",
    "        row = {'query': query}\n",
    "        \n",
    "        if query in pipeline_results['search_results']:\n",
    "            results = pipeline_results['search_results'][query]\n",
    "            row['results_count'] = len(results)\n",
    "            row['best_score'] = results[0].get('hybrid_score', 0) if results else 0\n",
    "        else:\n",
    "            row['results_count'] = 0\n",
    "            row['best_score'] = 0\n",
    "        \n",
    "        if query in pipeline_results['extracted_parameters']:\n",
    "            row['parameters_count'] = len(pipeline_results['extracted_parameters'][query])\n",
    "        else:\n",
    "            row['parameters_count'] = 0\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\nüìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    if len(summary_df) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        ax1.bar(range(len(summary_df)), summary_df['results_count'], alpha=0.7)\n",
    "        ax1.set_xlabel('–ó–∞–ø—Ä–æ—Å—ã')\n",
    "        ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')\n",
    "        ax1.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º')\n",
    "        ax1.set_xticks(range(len(summary_df)))\n",
    "        ax1.set_xticklabels([f'Q{i+1}' for i in range(len(summary_df))])\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        ax2.bar(range(len(summary_df)), summary_df['best_score'], alpha=0.7, color='orange')\n",
    "        ax2.set_xlabel('–ó–∞–ø—Ä–æ—Å—ã')\n",
    "        ax2.set_ylabel('–õ—É—á—à–∏–π —Å–∫–æ—Ä —Å—Ö–æ–∂–µ—Å—Ç–∏')\n",
    "        ax2.set_title('–ö–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')\n",
    "        ax2.set_xticks(range(len(summary_df)))\n",
    "        ax2.set_xticklabels([f'Q{i+1}' for i in range(len(summary_df))])\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "–ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã SAMe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –í—ã–≤–æ–¥—ã –ø–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã SAMe\n",
      "==================================================\n",
      "\n",
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥—É–ª–∏:\n",
      "\n",
      "‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "   - TextCleaner –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "   - TextNormalizer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "   - Lemmatizer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è SpaCy –º–æ–¥–µ–ª—å)\n",
      "   - FuzzySearchEngine –Ω–µ –æ–±—É—á–µ–Ω\n",
      "   - SemanticSearchEngine –Ω–µ –æ–±—É—á–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è sentence-transformers)\n",
      "   - HybridSearchEngine –Ω–µ –æ–±—É—á–µ–Ω\n",
      "   - RegexParameterExtractor –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "   - ExcelExporter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "\n",
      "üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_mtr_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00missue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–¢–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ú–¢–†: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43msample_mtr_data\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–†–∞–∑–º–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(user_test_queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_mtr_data' is not defined"
     ]
    }
   ],
   "source": [
    "# –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã\n",
    "print(\"üéØ –í—ã–≤–æ–¥—ã –ø–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã SAMe\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥—É–ª–∏:\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –º–æ–¥—É–ª–∏ —Ä–∞–±–æ—Ç–∞–ª–∏\n",
    "working_modules = []\n",
    "issues = []\n",
    "\n",
    "if 'text_cleaner' in locals():\n",
    "    working_modules.append(\"üßπ TextCleaner - –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\")\n",
    "else:\n",
    "    issues.append(\"TextCleaner –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "\n",
    "if 'text_normalizer' in locals():\n",
    "    working_modules.append(\"üîß TextNormalizer - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
    "else:\n",
    "    issues.append(\"TextNormalizer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "\n",
    "if 'lemmatizer' in locals():\n",
    "    working_modules.append(\"üìö Lemmatizer - –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å SpaCy\")\n",
    "else:\n",
    "    issues.append(\"Lemmatizer –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è SpaCy –º–æ–¥–µ–ª—å)\")\n",
    "\n",
    "if 'fuzzy_engine' in locals() and fuzzy_engine.is_fitted:\n",
    "    working_modules.append(\"üîç FuzzySearchEngine - –Ω–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫\")\n",
    "else:\n",
    "    issues.append(\"FuzzySearchEngine –Ω–µ –æ–±—É—á–µ–Ω\")\n",
    "\n",
    "if 'semantic_engine' in locals() and semantic_engine.is_fitted:\n",
    "    working_modules.append(\"üß† SemanticSearchEngine - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫\")\n",
    "else:\n",
    "    issues.append(\"SemanticSearchEngine –Ω–µ –æ–±—É—á–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è sentence-transformers)\")\n",
    "\n",
    "if 'hybrid_engine' in locals() and hybrid_engine.is_fitted:\n",
    "    working_modules.append(\"‚ö° HybridSearchEngine - –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫\")\n",
    "else:\n",
    "    issues.append(\"HybridSearchEngine –Ω–µ –æ–±—É—á–µ–Ω\")\n",
    "\n",
    "if 'regex_extractor' in locals():\n",
    "    working_modules.append(\"üîß RegexParameterExtractor - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "else:\n",
    "    issues.append(\"RegexParameterExtractor –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "\n",
    "if 'excel_exporter' in locals():\n",
    "    working_modules.append(\"üìä ExcelExporter - —ç–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "else:\n",
    "    issues.append(\"ExcelExporter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "for module in working_modules:\n",
    "    print(f\"   {module}\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\n‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   - {issue}\")\n",
    "\n",
    "print(f\"\\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\")\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ú–¢–†: {len(sample_mtr_data)}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞: {len(documents) if 'documents' in locals() else 0}\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(user_test_queries)}\")\n",
    "\n",
    "if pipeline_results and pipeline_results['statistics']:\n",
    "    stats = pipeline_results['statistics']\n",
    "    print(f\"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats['total_processing_time']:.2f}—Å\")\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {stats['total_results_found']}\")\n",
    "    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {stats['total_parameters_extracted']}\")\n",
    "\n",
    "print(f\"\\nüöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:\")\n",
    "print(\"   1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: spacy, sentence-transformers, faiss-cpu\")\n",
    "print(\"   2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ SpaCy –º–æ–¥–µ–ª—å: python -m spacy download ru_core_news_lg\")\n",
    "print(\"   3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ\")\n",
    "print(\"   4. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ –ú–¢–†\")\n",
    "print(\"   5. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—Ä–æ–≥–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤–∞—à–µ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏\")\n",
    "print(\"   6. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "\n",
    "print(f\"\\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã SAMe –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "print(f\"üìö –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ –ø–∞–ø–∫–µ docs/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "same-PgMDtfn9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
