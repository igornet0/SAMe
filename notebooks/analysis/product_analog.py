# %% [markdown]
# # –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–µ
# 
# **Product Analog Search System for Nomenclature Data**
# 
# –≠—Ç–æ—Ç notebook —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤/–ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã SAMe (Similar Articles Matching Engine).
# 
# ## –¶–µ–ª–∏:
# - –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã
# - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤
# - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
# - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ–∏—Å–∫–∞
# - –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# 
# ---

# %% [markdown]
# ## 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç—ã

# %%
# –°–∏—Å—Ç–µ–º–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import sys
import os
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple
import time
from datetime import datetime
import re
from collections import defaultdict, Counter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 20)
pd.set_option('display.width', None)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º SAMe
sys.path.append(os.path.abspath('../../src'))
sys.path.append(os.path.abspath('../..'))

print("‚úÖ –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
print(f"üìÅ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
print(f"üïê –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %% [markdown]
# ## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥—É–ª–µ–π SAMe

# %%
# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π SAMe
try:
    from same.data_manager import data_helper
    from same.text_processing.text_cleaner import TextCleaner, CleaningConfig
    from same.text_processing.lemmatizer import Lemmatizer, LemmatizerConfig
    from same.text_processing.normalizer import TextNormalizer, NormalizerConfig
    from same.text_processing.preprocessor import TextPreprocessor, PreprocessorConfig
    from same.search_engine.fuzzy_search import FuzzySearchEngine, FuzzySearchConfig
    from same.search_engine.semantic_search import SemanticSearchEngine, SemanticSearchConfig
    from same.search_engine.hybrid_search import HybridSearchEngine, HybridSearchConfig
    from same.parameter_extraction.regex_extractor import RegexParameterExtractor
    print("‚úÖ –ú–æ–¥—É–ª–∏ SAMe —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π SAMe: {e}")
    print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥—É–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ src/same/")

# %% [markdown]
# ## 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö

# %%
# –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
try:
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º
    dataset_path = data_helper["datasets"] / "main/–í—ã–≥—Ä—É–∑–∫–∞_–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞_–±–µ–∑_—É–¥–∞–ª–µ–Ω–Ω—ã—Ö_17_07_25.xlsx"
    if not dataset_path.exists():
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–∞–π–ª
        dataset_path = data_helper["datasets"] / "main/main_dataset.xlsx"
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
    data = pd.read_excel(dataset_path)
    
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {data.shape[0]} —Å—Ç—Ä–æ–∫, {data.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    print("üîß –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
    data = pd.DataFrame({
        '–ö–æ–¥': ['–ù–ò-001', '–ù–ò-002', '–ù–ò-003', '–ù–ò-004', '–ù–ò-005'],
        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': [
            '–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π',
            '–ë–æ–ª—Ç —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π –ú12√ó60 DIN 933',
            '–í–∏–Ω—Ç –ú8√ó30 —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫–æ–º',
            '–ì–∞–π–∫–∞ –ú10 —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è –ì–û–°–¢ 5915-70',
            '–®–∞–π–±–∞ –ø–ª–æ—Å–∫–∞—è 10 –ì–û–°–¢ 11371-78'
        ],
        '–ì—Ä—É–ø–ø–∞': ['–ö—Ä–µ–ø–µ–∂'] * 5,
        '–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã': ['–ú–∞—Ç–µ—Ä–∏–∞–ª—ã'] * 5
    })

# %%
# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
print("üìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
print(f"–°—Ç–æ–ª–±—Ü—ã: {list(data.columns)}")
print(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:")
print(data.info())

print(f"\nüîç –ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π:")
print(data.head())

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–æ–ª–±–µ—Ü —Å –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏
name_columns = [col for col in data.columns if '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' in col.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in col.lower()]
if name_columns:
    main_name_column = name_columns[0]
    print(f"\nüìù –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–æ–ª–±–µ—Ü —Å –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏: '{main_name_column}'")
else:
    main_name_column = data.columns[1] if len(data.columns) > 1 else data.columns[0]
    print(f"\nüìù –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–æ–ª–±–µ—Ü: '{main_name_column}'")

# %%
# –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
print("üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:")
print(f"–ü—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å—Ç–æ–ª–±—Ü–µ: {data[main_name_column].isnull().sum()}")
print(f"–î—É–±–ª–∏–∫–∞—Ç—ã: {data.duplicated().sum()}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ '{main_name_column}': {data[main_name_column].nunique()}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π
name_lengths = data[main_name_column].dropna().str.len()
print(f"\nüìè –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π:")
print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {name_lengths.mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"–ú–µ–¥–∏–∞–Ω–∞: {name_lengths.median():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"–ú–∏–Ω/–ú–∞–∫—Å: {name_lengths.min()}/{name_lengths.max()} —Å–∏–º–≤–æ–ª–æ–≤")

# –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π:")
sample_names = data[main_name_column].dropna().sample(min(5, len(data))).tolist()
for i, name in enumerate(sample_names, 1):
    print(f"{i}. {name[:80]}{'...' if len(name) > 80 else ''}")

# %% [markdown]
# ## 3. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

# %%
# –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã
print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã...")

def check_spacy_model(model_name: str = "ru_core_news_sm") -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ SpaCy –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    """
    try:
        import spacy
        spacy.load(model_name)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å SpaCy '{model_name}' –Ω–∞–π–¥–µ–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return True
    except OSError:
        print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å SpaCy '{model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print(f"üì• –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É:")
        print(f"   python -m spacy download {model_name}")
        print(f"üí° –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'ru_core_news_lg' –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
        return False
    except ImportError:
        print(f"‚ùå SpaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install spacy")
        return False

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏)
TECHNICAL_ABBREVIATIONS = {
    '—ç–ª': '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π', '—ç–ª–µ–∫—Ç—Ä': '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π',
    '–º–µ—Ö': '–º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–π', '–≥–∏–¥—Ä': '–≥–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫–∏–π',
    '–ø–Ω–µ–≤–º': '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '–∞–≤—Ç': '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π',
    '—Ä—É—á': '—Ä—É—á–Ω–æ–π', '—Å—Ç–∞—Ü': '—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–π',
    '–ø–µ—Ä–µ–Ω': '–ø–µ—Ä–µ–Ω–æ—Å–Ω–æ–π', '–º–æ–±–∏–ª': '–º–æ–±–∏–ª—å–Ω—ã–π',
    '–Ω–µ—Ä–∂': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', '–æ—Ü–∏–Ω–∫': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π'
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏)
TECHNICAL_PATTERNS = [
    r'\b\d+[.,]?\d*\s*[–∞-—è—ë]*[–≤—Ç|–∞|–≤|–æ–º|–º–º|—Å–º|–º|–∫–≥|–≥|–ª|–º–ª]\b',  # –†–∞–∑–º–µ—Ä—ã –∏ –µ–¥–∏–Ω–∏—Ü—ã
    r'\b[–º–º]\d+[x√ó]\d+\b',  # –†–∞–∑–º–µ—Ä—ã —Ç–∏–ø–∞ –ú10√ó50
    r'\b[–≥–æ—Å—Ç|din|iso]\s*\d+[-]?\d*[-]?\d*\b',  # –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã
    r'\b\d+[.,]\d*\s*[–∫–≤—Ç|–∫–≤|–º–≤—Ç|–≤—Ç]\b',  # –ú–æ—â–Ω–æ—Å—Ç—å
    r'\b\d+\s*–æ–±[/.]–º–∏–Ω\b',  # –û–±–æ—Ä–æ—Ç—ã
    r'\b\d+[.,]\d*\s*[–º–ø–∞|–∫–ø–∞|–ø–∞|–±–∞—Ä]\b',  # –î–∞–≤–ª–µ–Ω–∏–µ
    r'\b\d+[.,]\d*\s*[–º–º|—Å–º|–º]\b'  # –†–∞–∑–º–µ—Ä—ã
]

def enhanced_simple_preprocess(text):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    –¢–µ–ø–µ—Ä—å —ç—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å pickle
    """
    if pd.isna(text) or not text:
        return ""
    
    text = str(text).strip()
    original_text = text.lower()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    preserved_terms = []
    for pattern in TECHNICAL_PATTERNS:
        matches = re.findall(pattern, original_text)
        preserved_terms.extend(matches)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
    for abbr, full_form in TECHNICAL_ABBREVIATIONS.items():
        original_text = re.sub(rf'\b{abbr}\b', full_form, original_text)
    
    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö
    original_text = re.sub(r'[^–∞-—è—ëa-z\w\s\d.,()√óx/-]', ' ', original_text)
    original_text = re.sub(r'\s+', ' ', original_text)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    if preserved_terms:
        original_text = original_text + ' ' + ' '.join(preserved_terms)
    
    return original_text.strip()

def create_enhanced_simple_preprocess():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    –¢–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    return enhanced_simple_preprocess

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å SpaCy –º–æ–¥–µ–ª–∏
print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ SpaCy –º–æ–¥–µ–ª–µ–π...")
spacy_available = check_spacy_model("ru_core_news_sm")

if not spacy_available:
    # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å
    print("\nüîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å...")
    spacy_available = check_spacy_model("ru_core_news_lg")
    if spacy_available:
        spacy_model = "ru_core_news_lg"
    else:
        spacy_model = "ru_core_news_sm"
        print("\n‚ö†Ô∏è SpaCy –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞.")
else:
    spacy_model = "ru_core_news_sm"

# –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
enhanced_simple_preprocess = create_enhanced_simple_preprocess()
print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–∑–¥–∞–Ω–∞")

# %%
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã
print("\n‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π...")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def create_safe_config(config_class, **kwargs):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∏–∑ SAMe —Å–∏—Å—Ç–µ–º—ã
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    # –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ SAMe
    supported_params = {
        'CleaningConfig': {
            'remove_html', 'remove_special_chars', 'remove_extra_spaces', 
            'remove_numbers', 'preserve_technical_terms', 'custom_patterns'
        },
        'NormalizerConfig': {
            'standardize_units', 'normalize_abbreviations', 'unify_technical_terms',
            'remove_brand_names', 'standardize_numbers'
        },
        'LemmatizerConfig': {
            'model_name', 'preserve_technical_terms', 'custom_stopwords',
            'min_token_length', 'preserve_numbers'
        },
        'PreprocessorConfig': {
            'cleaning_config', 'lemmatizer_config', 'normalizer_config',
            'save_intermediate_steps', 'batch_size'
        },
        'FuzzySearchConfig': {
            'tfidf_max_features', 'tfidf_ngram_range', 'tfidf_min_df', 'tfidf_max_df',
            'cosine_threshold', 'fuzzy_threshold', 'levenshtein_threshold', 'similarity_threshold',
            'cosine_weight', 'fuzzy_weight', 'levenshtein_weight',
            'max_candidates', 'top_k_results', 'max_results', 'use_stemming'
        },
        'SemanticSearchConfig': {
            'model_name', 'embedding_dim', 'index_type', 'nlist', 'nprobe',
            'similarity_threshold', 'top_k_results', 'max_results',
            'batch_size', 'normalize_embeddings', 'use_gpu'
        },
        'HybridSearchConfig': {
            'fuzzy_config', 'semantic_config', 'fuzzy_weight', 'semantic_weight',
            'min_fuzzy_score', 'min_semantic_score', 'max_candidates_per_method',
            'final_top_k', 'max_results', 'similarity_threshold', 'combination_strategy',
            'enable_parallel_search', 'max_workers'
        }
    }
    
    config_name = config_class.__name__
    
    if config_name in supported_params:
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        safe_kwargs = {k: v for k, v in kwargs.items() 
                      if k in supported_params[config_name]}
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        filtered_out = set(kwargs.keys()) - set(safe_kwargs.keys())
        if filtered_out:
            print(f"‚ö†Ô∏è {config_name}: –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã: {filtered_out}")
    else:
        safe_kwargs = kwargs
    
    try:
        return config_class(**safe_kwargs)
    except TypeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è {config_name}: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return config_class()

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
print("üìã –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (—Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
cleaning_config = create_safe_config(
    CleaningConfig,
    remove_html=True,
    remove_special_chars=True,
    remove_extra_spaces=True,
    remove_numbers=False,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–∞ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    preserve_technical_terms=True,  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ SAMe
    custom_patterns=[]  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
normalizer_config = create_safe_config(
    NormalizerConfig,
    standardize_units=True,      # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è
    normalize_abbreviations=True, # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
    unify_technical_terms=True,  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
    remove_brand_names=False,    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±—Ä–µ–Ω–¥—ã
    standardize_numbers=True     # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
lemmatizer_config = create_safe_config(
    LemmatizerConfig,
    model_name=spacy_model,
    preserve_technical_terms=True,  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    min_token_length=2,            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    preserve_numbers=True,         # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    custom_stopwords=set()         # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (–ø—É—Å—Ç–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ)
)

print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã —Å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ SAMe:")
print("   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ (–ì–û–°–¢, DIN, ISO)")
print("   ‚Ä¢ –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π")
print("   ‚Ä¢ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π")
print("   –≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ.")

# %%
# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏...")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
text_cleaner = None
text_normalizer = None
lemmatizer = None
preprocessor = None
preprocessing_errors = []

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—á–∏—Å—Ç–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞
try:
    text_cleaner = TextCleaner(cleaning_config)
    print("‚úÖ TextCleaner —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    error_msg = f"TextCleaner: {str(e)}"
    preprocessing_errors.append(error_msg)
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è TextCleaner: {e}")

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    text_normalizer = TextNormalizer(normalizer_config)
    print("‚úÖ TextNormalizer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    error_msg = f"TextNormalizer: {str(e)}"
    preprocessing_errors.append(error_msg)
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è TextNormalizer: {e}")

# –°–æ–∑–¥–∞–Ω–∏–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ SpaCy –¥–æ—Å—Ç—É–ø–µ–Ω)
if spacy_available:
    try:
        lemmatizer = Lemmatizer(lemmatizer_config)
        print("‚úÖ Lemmatizer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        error_msg = f"Lemmatizer: {str(e)}"
        preprocessing_errors.append(error_msg)
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Lemmatizer: {e}")
        spacy_available = False  # –û—Ç–∫–ª—é—á–∞–µ–º SpaCy –µ—Å–ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
else:
    print("‚ö†Ô∏è Lemmatizer –ø—Ä–æ–ø—É—â–µ–Ω (SpaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
if any([text_cleaner, text_normalizer, lemmatizer]):
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ SAMe
        # PreprocessorConfig –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –∞ –Ω–µ –±—É–ª–µ–≤—ã —Ñ–ª–∞–≥–∏
        preprocessor_config = create_safe_config(
            PreprocessorConfig,
            cleaning_config=cleaning_config if text_cleaner else None,
            normalizer_config=normalizer_config if text_normalizer else None,
            lemmatizer_config=lemmatizer_config if lemmatizer else None,
            save_intermediate_steps=True,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            batch_size=1000               # –†–∞–∑–º–µ—Ä batch –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            # –£–¥–∞–ª–µ–Ω—ã –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: enable_cleaning, enable_normalization,
            # enable_lemmatization, show_progress, handle_errors_gracefully, preserve_original
        )
        
        preprocessor = TextPreprocessor(preprocessor_config)
        print("‚úÖ TextPreprocessor —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ process_text vs preprocess_text
        if hasattr(preprocessor, 'preprocess_text'):
            preprocess_method_name = 'preprocess_text'
        elif hasattr(preprocessor, 'process_text'):
            preprocess_method_name = 'process_text'
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
            preprocess_method_name = None
        
        print(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥: {preprocess_method_name}")
        
    except Exception as e:
        error_msg = f"TextPreprocessor: {str(e)}"
        preprocessing_errors.append(error_msg)
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è TextPreprocessor: {e}")
        preprocessor = None

# –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
print(f"\nüìä –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:")
print(f"   TextCleaner: {'‚úÖ' if text_cleaner else '‚ùå'}")
print(f"   TextNormalizer: {'‚úÖ' if text_normalizer else '‚ùå'}")
print(f"   Lemmatizer: {'‚úÖ' if lemmatizer else '‚ùå'}")
print(f"   TextPreprocessor: {'‚úÖ' if preprocessor else '‚ùå'}")

if preprocessing_errors:
    print(f"\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ ({len(preprocessing_errors)}):")
    for error in preprocessing_errors:
        print(f"   ‚Ä¢ {error}")
    print(f"\nüí° –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
if preprocessor:
    def final_preprocess_function(text):
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ SAMe
            if hasattr(preprocessor, 'preprocess_text'):
                result = preprocessor.preprocess_text(str(text))
                # –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏
                if isinstance(result, dict):
                    return result.get('final_normalized', 
                           result.get('final_text',
                           result.get('lemmatized', str(text))))
                else:
                    return str(result)
            elif hasattr(preprocessor, 'process_text'):
                result = preprocessor.process_text(str(text))
                if isinstance(result, dict):
                    return result.get('final_text', str(text))
                else:
                    return str(result)
            else:
                # Fallback –µ—Å–ª–∏ –º–µ—Ç–æ–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
                return enhanced_simple_preprocess(text)
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –ª—é–±–æ–π –æ—à–∏–±–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
            return enhanced_simple_preprocess(text)
    
    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å —É–º–Ω—ã–º fallback")
else:
    final_preprocess_function = enhanced_simple_preprocess
    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
simple_preprocess = enhanced_simple_preprocess

print(f"\nüîß –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ—Ç–æ–≤–∞:")
print(f"   –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: final_preprocess_function")
print(f"   Fallback —Ñ—É–Ω–∫—Ü–∏—è: enhanced_simple_preprocess")
print(f"   –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: simple_preprocess (–¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞)")

# –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
print(f"\nüìã –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:")
print(f"‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ SAMe:")
print(f"   ‚Ä¢ –û—á–∏—Å—Ç–∫–∞ HTML –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤")
print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
print(f"   ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è")
print(f"   ‚Ä¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π")
print(f"   ‚Ä¢ –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
print(f"   ‚Ä¢ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —á–∏—Å–µ–ª")
print(f"   ‚Ä¢ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")

print(f"\n‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ SAMe:")
print(f"   ‚Ä¢ –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ (–ì–û–°–¢, DIN, ISO)")
print(f"   ‚Ä¢ –ù–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π")
print(f"   ‚Ä¢ –ù–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑–º–µ—Ä–µ–Ω–∏–π")
print(f"   ‚Ä¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤")

print(f"\nüí° –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π:")
print(f"   ‚Ä¢ –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ—Å—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏")
print(f"   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö")
print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–µ—Ä–µ–∑ regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã")
print(f"   ‚Ä¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π")

# %%
def create_safe_final_preprocess_function():
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    import re
    
    def safe_final_preprocess(text):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ async –ø—Ä–æ–±–ª–µ–º"""
        try:
            text = str(text).lower().strip()
            
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            text = re.sub(r'\s+', ' ', text)  
            text = re.sub(r'[^\w\s\-\.\,\(\)√ó¬∞]', '', text)  
            
            return text.strip()
        except Exception:
            return str(text).lower().strip()
    
    return safe_final_preprocess

final_preprocess_function = create_safe_final_preprocess_function()
print("‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è final_preprocess_function —Å–æ–∑–¥–∞–Ω–∞")

if 'enhanced_simple_preprocess' not in globals():
    enhanced_simple_preprocess = final_preprocess_function

print("üîß Async –ø—Ä–æ–±–ª–µ–º–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ - –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å notebook")

# %%
# –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
print("\nüß™ –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏...")

# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π
test_samples = [
    "–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70 –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π —Å —à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤–∫–æ–π",
    "–î–≤–∏–≥–∞—Ç–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π 4–∫–í—Ç 1500 –æ–±/–º–∏–Ω 380–í IP54",
    "–ù–∞—Å–æ—Å —Ü–µ–Ω—Ç—Ä–æ–±–µ–∂–Ω—ã–π Q=50–º¬≥/—á H=32–º N=7.5–∫–í—Ç",
    "–ö–∞–±–µ–ª—å –í–í–ì–Ω–≥-LS 3√ó2.5 –º–º¬≤ 0.66/1–∫–í",
    "–ü–æ–¥—à–∏–ø–Ω–∏–∫ —à–∞—Ä–∏–∫–æ–≤—ã–π 6205-2RS —Ä–∞–∑–º–µ—Ä 25√ó52√ó15–º–º",
    "–ö–ª–∞–ø–∞–Ω —à–∞—Ä–æ–≤–æ–π DN50 PN16 –Ω–µ—Ä–∂. —Å—Ç–∞–ª—å 316L",
    "–†–µ–¥—É–∫—Ç–æ—Ä —á–µ—Ä–≤—è—á–Ω—ã–π i=40 –ú–Ω–æ–º=1.5–∫–í—Ç"
]

print(f"\nüìù –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {len(test_samples)} –ø—Ä–∏–º–µ—Ä–∞—Ö:")
print("=" * 80)

validation_results = []
processing_times = []

for i, sample in enumerate(test_samples, 1):
    print(f"\n{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:")
    print(f"   {sample}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    start_time = time.time()
    try:
        processed = final_preprocess_function(sample)
        processing_time = time.time() - start_time
        processing_times.append(processing_time)
        
        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {processed}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.4f} —Å–µ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        quality_score = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if re.search(r'\d+', processed):  # –ß–∏—Å–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
            quality_score += 1
        if len(processed) > len(sample) * 0.5:  # –ù–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω
            quality_score += 1
        if processed.strip():  # –ù–µ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            quality_score += 1
        
        validation_results.append({
            'original': sample,
            'processed': processed,
            'processing_time': processing_time,
            'quality_score': quality_score,
            'success': True
        })
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        validation_results.append({
            'original': sample,
            'processed': '',
            'processing_time': 0,
            'quality_score': 0,
            'success': False,
            'error': str(e)
        })

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
print(f"\n\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
print("=" * 50)

successful_tests = sum(1 for r in validation_results if r['success'])
avg_processing_time = np.mean(processing_times) if processing_times else 0
avg_quality_score = np.mean([r['quality_score'] for r in validation_results])

print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {successful_tests}/{len(test_samples)} ({successful_tests/len(test_samples)*100:.1f}%)")
print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_processing_time:.4f} —Å–µ–∫")
print(f"üéØ –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞: {avg_quality_score:.1f}/3")

if successful_tests == len(test_samples):
    print(f"\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
elif successful_tests > len(test_samples) * 0.8:
    print(f"\n‚ö†Ô∏è –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–∞ —Å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏.")
else:
    print(f"\n‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É.")

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
if avg_processing_time > 0.1:
    print(f"   ‚Ä¢ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã—Å–æ–∫–æ–µ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ batch-–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
if avg_quality_score < 2:
    print(f"   ‚Ä¢ –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
if not spacy_available:
    print(f"   ‚Ä¢ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SpaCy –º–æ–¥–µ–ª—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏")
if preprocessing_errors:
    print(f"   ‚Ä¢ –û–±–Ω–æ–≤–∏—Ç–µ –º–æ–¥—É–ª–∏ SAMe –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")

print(f"\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö.")

# %%
print("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º async –ø—Ä–æ–±–ª–µ–º—ã...")

# –°–æ–∑–¥–∞–µ–º –°–ò–ù–•–†–û–ù–ù–£–Æ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
def create_safe_preprocessor():
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    import re
    
    def safe_preprocess(text):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ async"""
        try:
            text = str(text).lower().strip()
            
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
            text = re.sub(r'\s+', ' ', text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
            text = re.sub(r'[^\w\s\-\.\,\(\)√ó]', '', text)  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            
            return text.strip()
        except Exception:
            return str(text).lower().strip()
    
    return safe_preprocess

# –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
safe_preprocess_function = create_safe_preprocessor()
print("‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–∑–¥–∞–Ω–∞")

# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã
processed_data = data.copy()

# –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
initial_count = len(processed_data)
processed_data = processed_data.dropna(subset=[main_name_column])
cleaned_count = len(processed_data)

if initial_count != cleaned_count:
    print(f"üßπ –£–¥–∞–ª–µ–Ω–æ –ø—É—Å—Ç—ã—Ö –∑–∞–ø–∏—Å–µ–π: {initial_count - cleaned_count}")

print(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {cleaned_count} –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π...")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
processing_stats = {
    'total_processed': 0,
    'successful_full_processing': 0,
    'fallback_processing': 0,
    'errors': 0,
    'processing_times': [],
    'original_lengths': [],
    'processed_lengths': []
}

processed_names = []
batch_size = 1000
start_time = time.time()
print()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
for idx, name in enumerate(processed_data[main_name_column]):
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
    if idx % batch_size == 0 and idx > 0:
        elapsed = time.time() - start_time
        rate = idx / elapsed
        eta = (cleaned_count - idx) / rate if rate > 0 else 0
        print(
            f"\rüìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{cleaned_count} ({idx/cleaned_count*100:.1f}%) | "
            f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫ | ETA: {eta:.0f} —Å–µ–∫",
            end='', flush=True
        )
    
    item_start_time = time.time()
    original_length = len(str(name))
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ë–ï–ó–û–ü–ê–°–ù–£–Æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_name = safe_preprocess_function(name)
        
        # –í—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º–∏
        processing_stats['successful_full_processing'] += 1
        
        processed_names.append(processed_name)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        processing_time = time.time() - item_start_time
        processing_stats['processing_times'].append(processing_time)
        processing_stats['original_lengths'].append(original_length)
        processing_stats['processed_lengths'].append(len(processed_name))
        processing_stats['total_processed'] += 1
        
    except Exception as e:
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        processed_names.append(str(name).lower().strip())
        processing_stats['errors'] += 1
        
        if processing_stats['errors'] <= 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ {idx}: {e}")

print()

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
processed_data['processed_name'] = processed_names

# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
total_time = time.time() - start_time
avg_processing_time = np.mean(processing_stats['processing_times']) if processing_stats['processing_times'] else 0
avg_original_length = np.mean(processing_stats['original_lengths'])
avg_processed_length = np.mean(processing_stats['processed_lengths'])
compression_ratio = avg_processed_length / avg_original_length if avg_original_length > 0 else 1

print(f"\n‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥")
print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
print(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processing_stats['total_processed']}")
print(f"   –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {processing_stats['successful_full_processing']}")
print(f"   –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {processing_stats['fallback_processing']}")
print(f"   –û—à–∏–±–∫–∏: {processing_stats['errors']}")
print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {processing_stats['total_processed']/total_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø–∏—Å—å: {avg_processing_time*1000:.2f} –º—Å")
print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–∞: {compression_ratio:.2f}")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º:")
print("=" * 80)

sample_indices = np.random.choice(len(processed_data), min(3, len(processed_data)), replace=False)

for i, idx in enumerate(sample_indices, 1):
    original = processed_data.iloc[idx][main_name_column]
    processed = processed_data.iloc[idx]['processed_name']
    
    print(f"{i}. –ò—Å—Ö–æ–¥–Ω—ã–π ({len(original)} —Å–∏–º–≤–æ–ª–æ–≤):")
    print(f"   {original}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π ({len(processed)} —Å–∏–º–≤–æ–ª–æ–≤):")
    print(f"   {processed}")
    
    # –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    changes = []
    if len(processed) < len(original) * 0.8:
        changes.append("–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ")
    if original.lower() != processed:
        changes.append("–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥–∏—Å—Ç—Ä–∞")
    if re.search(r'\d', processed) and re.search(r'\d', original):
        changes.append("—Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
    
    if changes:
        print(f"   üìã –ò–∑–º–µ–Ω–µ–Ω–∏—è: {', '.join(changes)}")
    print()

print(f"\nüéØ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤!")


# %% [markdown]
# ## 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤

# %%
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
print("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤...")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
documents = processed_data['processed_name'].tolist()
original_names = processed_data[main_name_column].tolist()
document_ids = processed_data.index.tolist()

print(f"üìö –ö–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)} –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π")

# 1. –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ (Fuzzy Search)
try:
    fuzzy_config = create_safe_config(
        FuzzySearchConfig,
        similarity_threshold=0.3,      # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (alias –¥–ª—è cosine_threshold)
        top_k_results=10,             # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        tfidf_max_features=5000,      # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        tfidf_ngram_range=(1, 3),     # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        cosine_threshold=0.3,         # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        fuzzy_threshold=60,           # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        max_candidates=100,           # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        use_stemming=False            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    )
    fuzzy_engine = FuzzySearchEngine(fuzzy_config)
    
    print("üîß –û–±—É—á–µ–Ω–∏–µ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...")
    fuzzy_engine.fit(documents)
    print("‚úÖ –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
    fuzzy_engine = None

# 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    semantic_config = create_safe_config(
        SemanticSearchConfig,
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        similarity_threshold=0.5,     # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        top_k_results=10,            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        batch_size=32,               # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        normalize_embeddings=True,   # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        use_gpu=False                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    )
    semantic_engine = SemanticSearchEngine(semantic_config)
    
    print("üß† –û–±—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞...")
    semantic_engine.fit(documents)
    print("‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
    print("üí° –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (transformers, sentence-transformers)")
    semantic_engine = None

# 3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –æ–±–∞ –¥–≤–∏–∂–∫–∞)
if fuzzy_engine and semantic_engine:
    try:
        hybrid_config = create_safe_config(
            HybridSearchConfig,
            fuzzy_weight=0.4,           # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            semantic_weight=0.6,        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            final_top_k=10,            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            max_candidates_per_method=50,  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            combination_strategy="weighted_sum",  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            enable_parallel_search=True,  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            # –£–¥–∞–ª–µ–Ω –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: enable_reranking
        )
        hybrid_engine = HybridSearchEngine(hybrid_config)
        hybrid_engine.fit(documents)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –≤—ã–∑–æ–≤ fit - –Ω–µ –ø–µ—Ä–µ–¥–∞–µ–º –¥–≤–∏–∂–∫–∏
        print("‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        hybrid_engine = None
else:
    hybrid_engine = None
    print("‚ö†Ô∏è –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç—Ä–µ–±—É—é—Ç—Å—è –æ–±–∞ –¥–≤–∏–∂–∫–∞)")

# –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤
available_engines = {}
if fuzzy_engine:
    available_engines['fuzzy'] = fuzzy_engine
if semantic_engine:
    available_engines['semantic'] = semantic_engine
if hybrid_engine:
    available_engines['hybrid'] = hybrid_engine

print(f"\nüéØ –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏: {list(available_engines.keys())}")

# –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
print(f"\nüìã –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤ SAMe:")
print(f"\n‚úÖ FuzzySearchConfig - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
print(f"   ‚Ä¢ tfidf_max_features, tfidf_ngram_range, tfidf_min_df, tfidf_max_df")
print(f"   ‚Ä¢ cosine_threshold, fuzzy_threshold, levenshtein_threshold")
print(f"   ‚Ä¢ cosine_weight, fuzzy_weight, levenshtein_weight")
print(f"   ‚Ä¢ max_candidates, top_k_results, similarity_threshold")
print(f"   ‚Ä¢ use_stemming (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å notebook)")

print(f"\n‚úÖ SemanticSearchConfig - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
print(f"   ‚Ä¢ model_name, embedding_dim, index_type, nlist, nprobe")
print(f"   ‚Ä¢ similarity_threshold, top_k_results, batch_size")
print(f"   ‚Ä¢ normalize_embeddings, use_gpu")

print(f"\n‚úÖ HybridSearchConfig - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
print(f"   ‚Ä¢ fuzzy_weight, semantic_weight, final_top_k")
print(f"   ‚Ä¢ min_fuzzy_score, min_semantic_score, max_candidates_per_method")
print(f"   ‚Ä¢ combination_strategy, enable_parallel_search, max_workers")

print(f"\nüí° –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:")
print(f"   ‚Ä¢ HybridSearchEngine —Å–æ–∑–¥–∞–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –¥–≤–∏–∂–∫–æ–≤")
print(f"   ‚Ä¢ –ú–µ—Ç–æ–¥ fit() –¥–ª—è HybridSearchEngine –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ–ª—å–∫–æ documents")
print(f"   ‚Ä¢ –í—Å–µ –¥–≤–∏–∂–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∞–ª–∏–∞—Å—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
print(f"   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

# %% [markdown]
# ## 5. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

# %%
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

def search_analogs(query: str, engine_type: str = 'fuzzy', top_k: int = 5) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞)
        engine_type: –¢–∏–ø –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ ('fuzzy', 'semantic', 'hybrid')
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    if engine_type not in available_engines:
        print(f"‚ùå –î–≤–∏–∂–æ–∫ '{engine_type}' –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return []
    
    engine = available_engines[engine_type]
    
    try:
        if 'final_preprocess_function' in globals():
            processed_query = final_preprocess_function(query)
        else:
            processed_query = str(query).lower().strip()
    except Exception:
        processed_query = str(query).lower().strip()
    
    # –ü–æ–∏—Å–∫
    try:
        results = engine.search(processed_query, top_k=top_k)
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        enriched_results = []
        for result in results:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ document_id
            doc_idx = result.get('document_id', result.get('index', result.get('doc_id')))
            
            if doc_idx is not None and doc_idx < len(processed_data):
                row = processed_data.iloc[doc_idx]
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 3: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ score
                score = 0.0
                for score_field in ['score', 'similarity_score', 'combined_score', 'hybrid_score', 'cosine_score', 'fuzzy_score']:
                    if score_field in result:
                        score = result[score_field]
                        break
                
                enriched_result = {
                    'score': float(score),
                    'original_name': row[main_name_column],
                    'processed_name': result.get('document', ''),
                    'code': row.get('–ö–æ–¥', ''),
                    'group': row.get('–ì—Ä—É–ø–ø–∞', ''),
                    'type': row.get('–í–∏–¥–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã', ''),
                    'index': doc_idx
                }
                enriched_results.append(enriched_result)
        
        return enriched_results
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []


def display_search_results(query: str, results: List[Dict], engine_type: str):
    """
    –ö—Ä–∞—Å–∏–≤–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ ({engine_type.upper()})")
    print(f"üìù –ó–∞–ø—Ä–æ—Å: '{query}'")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(results)} –∞–Ω–∞–ª–æ–≥–æ–≤")
    print("=" * 80)
    
    for i, result in enumerate(results, 1):
        print(f"{i}. üì¶ {result['original_name']}")
        print(f"   üè∑Ô∏è  –ö–æ–¥: {result['code']}")
        print(f"   üìÇ –ì—Ä—É–ø–ø–∞: {result['group']}")
        print(f"   üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.3f}")
        print()

def compare_engines(query: str, top_k: int = 5):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
    """
    print(f"\nüÜö –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤")
    print(f"üìù –ó–∞–ø—Ä–æ—Å: '{query}'")
    print("=" * 100)
    
    all_results = {}
    
    for engine_name in available_engines.keys():
        results = search_analogs(query, engine_name, top_k)
        all_results[engine_name] = results
        
        print(f"\nüîß {engine_name.upper()} ENGINE:")
        for i, result in enumerate(results[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
            print(f"  {i}. {result['original_name'][:60]}... (—Å–∫–æ—Ä: {result['score']:.3f})")
    
    return all_results

print("‚úÖ –§—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –≥–æ—Ç–æ–≤—ã")

# %% [markdown]
# ## 6. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

# %%
# –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
example_queries = [
    "C–≤–µ—Ç–∏–ª—å–Ω–∏–∫ LED –ø–∞–Ω–µ–ª—å 50W",
    "K–æ–ª—å—Ü–æ –∫—Ä–µ–ø–ª–µ–Ω–∏—è –≥—Ä—É–∑–∞ –¥–æ 3",
    "–ê–≤—Ç–æ–ª–∞–º–ø–æ—á–∫–∞ –ù7 24-70W",
    "–ê–≤—Ç–æ—ç–º–∞–ª—å Reoflex",
    "–ê–¥–∞–ø—Ç–µ—Ä –ø–∏—Ç–∞–Ω–∏—è"
]

if len(processed_data) > 0:
    # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    sample_products = processed_data[main_name_column].sample(min(3, len(processed_data))).tolist()
    example_queries.extend(sample_products)

print("üéØ –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
for i, query in enumerate(example_queries[:5], 1):
    print(f"{i}. {query}")

# %%
# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤")
print("=" * 50)

# –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
demo_engine = list(available_engines.keys())[0] if available_engines else None

if demo_engine:
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 2-3 –∑–∞–ø—Ä–æ—Å–æ–≤
    for query in example_queries[:2]:
        print(f"\n" + "="*60)
        results = search_analogs(query, demo_engine, top_k=5)
        display_search_results(query, results, demo_engine)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if results:
            print(f"üí° –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
            scores = [r['score'] for r in results]
            print(f"   –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {np.mean(scores):.3f}")
            print(f"   –†–∞–∑–±—Ä–æ—Å —Å–∫–æ—Ä–æ–≤: {np.std(scores):.3f}")
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            groups = [r['group'] for r in results if r['group']]
            if groups:
                group_counts = Counter(groups)
                print(f"   –ì—Ä—É–ø–ø—ã —Ç–æ–≤–∞—Ä–æ–≤: {dict(group_counts)}")
else:
    print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")

# %%
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
if len(available_engines) > 1:
    print("\nüÜö –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤")
    print("=" * 60)
    
    # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_query = example_queries[0]
    
    comparison_results = compare_engines(comparison_query, top_k=5)
    
    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(comparison_results) >= 2:
        engine_names = list(comparison_results.keys())
        engine1, engine2 = engine_names[0], engine_names[1]
        
        results1 = set(r['index'] for r in comparison_results[engine1])
        results2 = set(r['index'] for r in comparison_results[engine2])
        
        intersection = results1.intersection(results2)
        union = results1.union(results2)
        
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π:")
        print(f"   {engine1}: {len(results1)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print(f"   {engine2}: {len(results2)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print(f"   –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ: {len(intersection)} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞: {len(intersection)/len(union):.3f}")
        
else:
    print("\n‚ö†Ô∏è –î–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")

# %% [markdown]
# ## 7. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# %%
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")

try:
    parameter_extractor = RegexParameterExtractor()
    print("‚úÖ –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("\nüìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    
    # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    sample_names = processed_data[main_name_column].head(5).tolist()
    
    for i, name in enumerate(sample_names, 1):
        print(f"\n{i}. –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {name}")
        
        try:
            parameters = parameter_extractor.extract_parameters(name)
            
            if parameters:
                print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(parameters)}")
                for param in parameters[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                    unit_str = f" {param.unit}" if param.unit else ""
                    print(f"   - {param.name}: {param.value}{unit_str} (—Ç–∏–ø: {param.parameter_type})")
            else:
                print("   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
    parameter_extractor = None

# %%
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def search_analogs_with_parameters(query: str, engine_type: str = 'fuzzy', top_k: int = 5):
    """
    –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
    results = search_analogs(query, engine_type, top_k)
    
    if not parameter_extractor or not results:
        return results
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    try:
        query_params = parameter_extractor.extract_parameters(query)
        query_param_dict = {p.name: p.value for p in query_params}
    except:
        query_param_dict = {}
    
    # –û–±–æ–≥–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    enriched_results = []
    
    for result in results:
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
            item_params = parameter_extractor.extract_parameters(result['original_name'])
            item_param_dict = {p.name: p.value for p in item_params}
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            param_match_score = 0
            if query_param_dict and item_param_dict:
                common_params = set(query_param_dict.keys()).intersection(set(item_param_dict.keys()))
                if common_params:
                    matches = sum(1 for param in common_params 
                                if str(query_param_dict[param]).lower() == str(item_param_dict[param]).lower())
                    param_match_score = matches / len(common_params)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            result['parameters'] = item_params
            result['parameter_match_score'] = param_match_score
            result['parameter_count'] = len(item_params)
            
            enriched_results.append(result)
            
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –Ω–∏—Ö
            result['parameters'] = []
            result['parameter_match_score'] = 0
            result['parameter_count'] = 0
            enriched_results.append(result)
    
    return enriched_results

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
if parameter_extractor and available_engines:
    print("\nüéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print("=" * 60)
    
    demo_query = "–ë–æ–ª—Ç –ú10√ó50 –ì–û–°–¢ 7798-70"
    engine_name = list(available_engines.keys())[0]
    
    param_results = search_analogs_with_parameters(demo_query, engine_name, top_k=5)
    
    print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{demo_query}'")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(param_results)} –∞–Ω–∞–ª–æ–≥–æ–≤")
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    
    for i, result in enumerate(param_results, 1):
        print(f"\n{i}. üì¶ {result['original_name']}")
        print(f"   üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.3f}")
        print(f"   üîß –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {result['parameter_count']}")
        print(f"   ‚öñÔ∏è  –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {result['parameter_match_score']:.3f}")
        
        if result['parameters']:
            print(f"   üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            for param in result['parameters'][:3]:
                unit_str = f" {param.unit}" if param.unit else ""
                print(f"      - {param.name}: {param.value}{unit_str}")
else:
    print("\n‚ö†Ô∏è –ü–æ–∏—Å–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

# %% [markdown]
# ## 8. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

# %%
# –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤")
print("=" * 60)

if available_engines:
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    performance_stats = {}
    
    for engine_name, engine in available_engines.items():
        print(f"\nüîß –ê–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–∫–∞: {engine_name.upper()}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
        test_queries = example_queries[:3]
        search_times = []
        result_counts = []
        avg_scores = []
        
        for query in test_queries:
            start_time = time.time()
            results = search_analogs(query, engine_name, top_k=10)
            search_time = time.time() - start_time
            
            search_times.append(search_time)
            result_counts.append(len(results))
            
            if results:
                avg_scores.append(np.mean([r['score'] for r in results]))
            else:
                avg_scores.append(0)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        performance_stats[engine_name] = {
            'avg_search_time': np.mean(search_times),
            'avg_results_count': np.mean(result_counts),
            'avg_relevance_score': np.mean(avg_scores)
        }
        
        print(f"   ‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {np.mean(search_times):.3f} —Å–µ–∫")
        print(f"   üìä –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {np.mean(result_counts):.1f}")
        print(f"   üéØ –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {np.mean(avg_scores):.3f}")
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    if len(performance_stats) > 1:
        print(f"\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        print(f"{'–î–≤–∏–∂–æ–∫':<12} {'–í—Ä–µ–º—è (—Å–µ–∫)':<12} {'–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤':<12} {'–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å':<15}")
        print("-" * 55)
        
        for engine_name, stats in performance_stats.items():
            print(f"{engine_name:<12} {stats['avg_search_time']:<12.3f} "
                  f"{stats['avg_results_count']:<12.1f} {stats['avg_relevance_score']:<15.3f}")
else:
    print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

# %%
# –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\nüéØ –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
print("=" * 50)

if available_engines and len(processed_data) > 0:
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º —Ç–æ–≤–∞—Ä–æ–≤
    if '–ì—Ä—É–ø–ø–∞' in processed_data.columns:
        group_distribution = processed_data['–ì—Ä—É–ø–ø–∞'].value_counts()
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º:")
        for group, count in group_distribution.head(10).items():
            percentage = (count / len(processed_data)) * 100
            print(f"   {group}: {count} ({percentage:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    engine_name = list(available_engines.keys())[0]
    unique_results = set()
    
    for query in example_queries[:5]:
        results = search_analogs(query, engine_name, top_k=5)
        for result in results:
            unique_results.add(result['index'])
    
    coverage = len(unique_results) / len(processed_data) * 100
    print(f"\nüéØ –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö: {len(unique_results)}")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(processed_data)}")
    print(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage:.1f}%")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:")
    
    if coverage < 20:
        print("   - –ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    if len(available_engines) == 1:
        print("   - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    
    if not parameter_extractor:
        print("   - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    
    print("   - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –¥–æ–º–µ–Ω–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("   - –î–æ–±–∞–≤—å—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")

else:
    print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è")

# %% [markdown]
# ## 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞
# 
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ production.

# %%
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
import pickle
from datetime import datetime
import json

print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞...")

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
models_dir = Path("../../models/analog_search")
models_dir.mkdir(parents=True, exist_ok=True)

# –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_version = f"v{timestamp}"

print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {models_dir}")
print(f"üè∑Ô∏è –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version}")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
system_data = {
    'version': model_version,
    'created_at': datetime.now().isoformat(),
    'dataset_info': {
        'total_records': len(processed_data) if 'processed_data' in locals() else 0,
        'main_column': main_name_column if 'main_name_column' in locals() else None,
        'columns': list(processed_data.columns) if 'processed_data' in locals() else []
    },
    'preprocessing_stats': processing_stats if 'processing_stats' in locals() else None,
    'available_engines': list(available_engines.keys()) if 'available_engines' in locals() else [],
    'spacy_available': spacy_available if 'spacy_available' in locals() else False,
    'preprocessing_errors': preprocessing_errors if 'preprocessing_errors' in locals() else []
}

# 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
configs_path = models_dir / f"configs_{model_version}.pkl"
try:
    configs = {
        'cleaning_config': cleaning_config if 'cleaning_config' in locals() else None,
        'normalizer_config': normalizer_config if 'normalizer_config' in locals() else None,
        'lemmatizer_config': lemmatizer_config if 'lemmatizer_config' in locals() else None,
        'preprocessor_config': preprocessor_config if 'preprocessor_config' in locals() else None
    }
    
    with open(configs_path, 'wb') as f:
        pickle.dump(configs, f)
    print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {configs_path.name}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")

# 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
preprocessing_path = models_dir / f"preprocessing_{model_version}.pkl"
try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–π –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π...")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é enhanced_simple_preprocess
    if 'enhanced_simple_preprocess' in locals():
        try:
            test_pickle = pickle.dumps(enhanced_simple_preprocess)
            print("‚úÖ enhanced_simple_preprocess —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå enhanced_simple_preprocess –Ω–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è: {e}")
    
    preprocessing_components = {
        'text_cleaner': text_cleaner if 'text_cleaner' in locals() else None,
        'text_normalizer': text_normalizer if 'text_normalizer' in locals() else None,
        'lemmatizer': lemmatizer if 'lemmatizer' in locals() else None,
        'preprocessor': preprocessor if 'preprocessor' in locals() else None,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–æ–±–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç—Å—è
        'enhanced_simple_preprocess': enhanced_simple_preprocess if 'enhanced_simple_preprocess' in locals() else None,
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        'function_info': {
            'enhanced_simple_preprocess_type': str(type(enhanced_simple_preprocess)) if 'enhanced_simple_preprocess' in locals() else None,
            'enhanced_simple_preprocess_module': getattr(enhanced_simple_preprocess, '__module__', None) if 'enhanced_simple_preprocess' in locals() else None,
            'enhanced_simple_preprocess_name': getattr(enhanced_simple_preprocess, '__name__', None) if 'enhanced_simple_preprocess' in locals() else None,
        }
    }
    
    # –ò—Å–∫–ª—é—á–∞–µ–º final_preprocess_function, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
    print("‚ö†Ô∏è final_preprocess_function –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)")
    
    with open(preprocessing_path, 'wb') as f:
        pickle.dump(preprocessing_components, f)
    print(f"‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {preprocessing_path.name}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    import traceback
    print(f"üìã –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")

# 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
if 'available_engines' in locals() and available_engines:
    engines_path = models_dir / f"search_engines_{model_version}.pkl"
    try:
        with open(engines_path, 'wb') as f:
            pickle.dump(available_engines, f)
        print(f"‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {engines_path.name}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        for engine_name, engine in available_engines.items():
            if hasattr(engine, 'save_model'):
                try:
                    engine_model_path = models_dir / f"{engine_name}_model_{model_version}.pkl"
                    engine.save_model(str(engine_model_path))
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {engine_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ: {engine_model_path.name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å {engine_name}: {e}")
                    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤: {e}")
else:
    print("‚ö†Ô∏è –ü–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

# 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
if 'processed_data' in locals() and len(processed_data) > 0:
    data_path = models_dir / f"processed_data_{model_version}.pkl"
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
        essential_columns = [main_name_column, 'processed_name'] if 'main_name_column' in locals() else []
        other_columns = [col for col in processed_data.columns if col not in essential_columns]
        all_columns = essential_columns + other_columns
        
        essential_data = processed_data[all_columns].copy()
        
        with open(data_path, 'wb') as f:
            pickle.dump(essential_data, f)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {data_path.name}")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(essential_data)} –∑–∞–ø–∏—Å–µ–π, {len(essential_data.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")

# 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º—ã
metadata_path = models_dir / f"system_metadata_{model_version}.json"
try:
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(system_data, f, ensure_ascii=False, indent=2, default=str)
    print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path.name}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

# 6. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ
readme_path = models_dir / f"README_{model_version}.md"
try:
    dataset_size = len(processed_data) if 'processed_data' in locals() else 'N/A'
    engines_list = ', '.join(available_engines.keys()) if 'available_engines' in locals() else 'N/A'
    
    readme_content = f"""# –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ - {model_version}

## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
- **–í–µ—Ä—Å–∏—è**: {model_version}
- **–°–æ–∑–¥–∞–Ω–∞**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **–ó–∞–ø–∏—Å–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ**: {dataset_size}
- **–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–≤–∏–∂–∫–∏**: {engines_list}

## –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏
- `configs_{model_version}.pkl` - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- `preprocessing_{model_version}.pkl` - –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
- `search_engines_{model_version}.pkl` - –ü–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏
- `processed_data_{model_version}.pkl` - –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- `system_metadata_{model_version}.json` - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã

## –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
```python
import pickle
from pathlib import Path

models_dir = Path("models/analog_search")
version = "{model_version}"

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
with open(models_dir / f"preprocessing_{{version}}.pkl", 'rb') as f:
    preprocessing = pickle.load(f)

with open(models_dir / f"search_engines_{{version}}.pkl", 'rb') as f:
    engines = pickle.load(f)

with open(models_dir / f"processed_data_{{version}}.pkl", 'rb') as f:
    data = pickle.load(f)
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ notebook `analog_search_production.ipynb` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.
"""
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    print(f"‚úÖ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {readme_path.name}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: {e}")

# –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
print(f"\nüéâ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"üìÅ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {models_dir}")
print(f"üè∑Ô∏è –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version}")
print(f"\nüìã –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
for file_path in models_dir.glob(f"*{model_version}*"):
    file_size = file_path.stat().st_size / (1024*1024)  # MB
    print(f"   ‚Ä¢ {file_path.name} ({file_size:.1f} MB)")

print(f"\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ notebook: analog_search_production.ipynb")
print(f"üîß –ü–µ—Ä–µ–¥–∞–π—Ç–µ –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏: {model_version}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ production notebook
latest_version_path = models_dir / "latest_version.txt"
try:
    with open(latest_version_path, 'w') as f:
        f.write(model_version)
    print(f"‚úÖ –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {latest_version_path.name}")
except Exception as e:
    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ—Ä—Å–∏—é: {e}")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π...")
try:
    test_preprocessing_path = models_dir / f"preprocessing_{model_version}.pkl"
    with open(test_preprocessing_path, 'rb') as f:
        test_components = pickle.load(f)
    
    test_function = test_components.get('enhanced_simple_preprocess')
    if test_function:
        test_result = test_function("–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
        print(f"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç: '{test_result}'")
    else:
        print(f"‚ö†Ô∏è –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
    function_info = test_components.get('function_info', {})
    if function_info:
        print(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {function_info}")
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

# %% [markdown]
# ## 9. –í—ã–≤–æ–¥—ã –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ

# %%
# –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã
print("üéØ –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´ –ü–û –°–ò–°–¢–ï–ú–ï –ü–û–ò–°–ö–ê –ê–ù–ê–õ–û–ì–û–í")
print("=" * 70)

print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data) if 'data' in locals() else 0}")
print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(processed_data) if 'processed_data' in locals() else 0}")
print(f"   –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–æ–ª–±–µ—Ü: {main_name_column if 'main_name_column' in locals() else 'N/A'}")

print(f"\nüîß –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
components_status = {
    '–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞': '‚úÖ' if 'preprocessor' in locals() and preprocessor else '‚ùå',
    '–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫': '‚úÖ' if 'fuzzy_engine' in locals() and fuzzy_engine else '‚ùå',
    '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫': '‚úÖ' if 'semantic_engine' in locals() and semantic_engine else '‚ùå',
    '–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫': '‚úÖ' if 'hybrid_engine' in locals() and hybrid_engine else '‚ùå',
    '–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤': '‚úÖ' if 'parameter_extractor' in locals() and parameter_extractor else '‚ùå'
}

for component, status in components_status.items():
    print(f"   {status} {component}")

print(f"\nüéØ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã:")
print(f"   üìù –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é")
print(f"   üîç –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∏—Å–∫–∞")
print(f"   üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
print(f"   ‚öñÔ∏è  –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
print(f"   üìã –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤")

print(f"\nüöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:")
print(f"   ‚Ä¢ –ü–æ–∏—Å–∫ –∑–∞–º–µ–Ω–∏—Ç–µ–ª–µ–π –¥–ª—è —Å–Ω—è—Ç—ã—Ö —Å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤")
print(f"   ‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ—Ç —Ä–∞–∑–Ω—ã—Ö –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤")
print(f"   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
print(f"   ‚Ä¢ –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
print(f"   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫–ª–∞–¥—Å–∫–∏—Ö –∑–∞–ø–∞—Å–æ–≤")

print(f"\nüìà –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è:")
print(f"   ‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∫–∞—Ç–∞–ª–æ–≥–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤")
print(f"   ‚Ä¢ –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö")
print(f"   ‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
print(f"   ‚Ä¢ API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å ERP-—Å–∏—Å—Ç–µ–º–∞–º–∏")
print(f"   ‚Ä¢ –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∫–æ–Ω–µ—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")

print(f"\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
print(f"üìö –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø—Ä–∞–≤–∫–∏ –ø–æ API –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ SAMe")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
if 'processed_data' in locals() and len(processed_data) > 0:
    try:
        output_path = Path("../../data/output/processed_nomenclature.csv")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        processed_data.to_csv(output_path, index=False)
        print(f"\nüíæ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {e}")

print(f"\nüïê –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


