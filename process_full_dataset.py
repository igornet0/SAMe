#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ main_dataset_processed.csv
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel —Ñ–æ—Ä–º–∞—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π —ç–∫—Å–ø–æ—Ä—Ç–∞ SAMe
"""

import os
import sys
import psutil
import argparse
import logging
from pathlib import Path
from datetime import datetime
import subprocess
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º SAMe
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π —ç–∫—Å–ø–æ—Ä—Ç–∞ SAMe
try:
    from src.same_api.export import ExcelExporter, ExcelExportConfig
    EXPORT_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"Warning: SAMe export modules not available. Missing: {e}")
    EXPORT_MODULES_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def check_system_resources():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
    # –ü–∞–º—è—Ç—å
    memory = psutil.virtual_memory()
    available_memory_gb = memory.available / (1024**3)
    total_memory_gb = memory.total / (1024**3)
    
    # CPU
    cpu_count = psutil.cpu_count()
    cpu_count_logical = psutil.cpu_count(logical=True)
    
    # –î–∏—Å–∫
    disk = psutil.disk_usage('.')
    available_disk_gb = disk.free / (1024**3)
    
    logger.info(f"System resources:")
    logger.info(f"  Memory: {available_memory_gb:.1f}GB available / {total_memory_gb:.1f}GB total")
    logger.info(f"  CPU: {cpu_count} cores ({cpu_count_logical} logical)")
    logger.info(f"  Disk: {available_disk_gb:.1f}GB available")
    
    return {
        'available_memory_gb': available_memory_gb,
        'total_memory_gb': total_memory_gb,
        'cpu_count': cpu_count,
        'cpu_count_logical': cpu_count_logical,
        'available_disk_gb': available_disk_gb
    }


def recommend_strategy(resources, dataset_size_mb):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    memory_gb = resources['available_memory_gb']
    cpu_count = resources['cpu_count']
    
    logger.info(f"Dataset size: {dataset_size_mb:.1f}MB")
    
    if memory_gb >= 8 and cpu_count >= 4:
        strategy = "parallel"
        chunk_size = 8000
        workers = min(cpu_count, 8)
        logger.info(f"üöÄ Recommended strategy: PARALLEL processing")
        logger.info(f"   Chunk size: {chunk_size}, Workers: {workers}")
    elif memory_gb >= 4:
        strategy = "large_dataset"
        chunk_size = 10000
        workers = 1
        logger.info(f"üîÑ Recommended strategy: LARGE DATASET processing")
        logger.info(f"   Chunk size: {chunk_size}")
    else:
        strategy = "emergency"
        chunk_size = 5000
        workers = 1
        logger.info(f"‚ö° Recommended strategy: EMERGENCY processing")
        logger.info(f"   Chunk size: {chunk_size}")
    
    return strategy, chunk_size, workers


def convert_csv_to_excel(csv_file: str, excel_file: str, metadata: dict = None) -> bool:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è CSV –≤ Excel —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞"""
    try:
        logger.info(f"Converting {csv_file} to Excel format...")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
        cmd = [
            sys.executable, "csv_to_excel_converter.py",
            csv_file,
            "-o", excel_file
        ]

        logger.info(f"Running Excel converter: {' '.join(cmd)}")

        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info("Excel conversion completed successfully!")
        logger.info(result.stdout)

        return True

    except subprocess.CalledProcessError as e:
        logger.error(f"Excel conversion failed: {e}")
        logger.error(f"STDOUT: {e.stdout}")
        logger.error(f"STDERR: {e.stderr}")

        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É pandas
        try:
            logger.info("Trying fallback conversion with pandas...")
            df = pd.read_csv(csv_file)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
            if len(df) > 1000000:
                logger.warning(f"Dataset too large ({len(df)} rows) for simple Excel export")
                return False

            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞', index=False)

            logger.info(f"‚úÖ Fallback Excel file created: {excel_file}")
            return True

        except Exception as fallback_error:
            logger.error(f"Fallback conversion also failed: {fallback_error}")
            return False

    except Exception as e:
        logger.error(f"Error converting to Excel: {e}")
        return False


def run_processing(input_file, output_csv, strategy, chunk_size, workers, threshold):
    """–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π"""

    if strategy == "parallel":
        cmd = [
            sys.executable, "parallel_dbscan_processor.py",
            input_file,
            "-o", output_csv,
            "-c", str(chunk_size),
            "-t", str(threshold),
            "-j", str(workers),
            "--overlap", "1000"
        ]
    elif strategy == "large_dataset":
        cmd = [
            sys.executable, "large_dataset_processor.py",
            input_file,
            "-o", output_csv,
            "-c", str(chunk_size),
            "-t", str(threshold),
            "--max-features", "3000",
            "--n-components", "50"
        ]
    else:  # emergency
        cmd = [
            sys.executable, "emergency_search.py",
            input_file,
            "-o", output_csv,
            "-t", str(threshold),
            "-l", str(chunk_size * 10),  # –î–ª—è emergency –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–∏–π –ª–∏–º–∏—Ç
            "-b", str(chunk_size)
        ]

    logger.info(f"Executing command: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info("Processing completed successfully!")
        logger.info(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Processing failed with error: {e}")
        logger.error(f"STDOUT: {e.stdout}")
        logger.error(f"STDERR: {e.stderr}")
        return False


def main():
    parser = argparse.ArgumentParser(description='Process full main_dataset_processed.csv optimally and save to Excel')
    parser.add_argument('input_file', nargs='?',
                       default='src/data/output/main_dataset_processed.csv',
                       help='Input CSV file (default: src/data/output/main_dataset_processed.csv)')
    parser.add_argument('-o', '--output', help='Output Excel file (.xlsx) or CSV file (.csv)')
    parser.add_argument('-t', '--threshold', type=float, default=0.5, help='Similarity threshold (default: 0.5)')
    parser.add_argument('-s', '--strategy', choices=['auto', 'parallel', 'large_dataset', 'emergency'],
                       default='auto', help='Processing strategy (default: auto)')
    parser.add_argument('--chunk-size', type=int, help='Override chunk size')
    parser.add_argument('--workers', type=int, help='Override number of workers')
    parser.add_argument('--keep-csv', action='store_true', help='Keep intermediate CSV file')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    input_file = args.input_file
    if not os.path.exists(input_file):
        logger.error(f"Input file not found: {input_file}")
        return 1
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
    if args.output:
        if args.output.endswith('.xlsx'):
            excel_file = args.output
            csv_file = args.output.replace('.xlsx', '.csv')
        else:
            excel_file = args.output.replace('.csv', '.xlsx')
            csv_file = args.output
    else:
        input_path = Path(input_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_file = f"full_dataset_results_{timestamp}.xlsx"
        csv_file = f"full_dataset_results_{timestamp}.csv"
    
    logger.info(f"üéØ Processing full dataset: {input_file}")
    logger.info(f"üìÑ CSV output: {csv_file}")
    logger.info(f"üìä Excel output: {excel_file}")
    logger.info(f"üéöÔ∏è  Similarity threshold: {args.threshold}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
    resources = check_system_resources()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size_mb = os.path.getsize(input_file) / (1024**2)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    if args.strategy == 'auto':
        strategy, chunk_size, workers = recommend_strategy(resources, file_size_mb)
    else:
        strategy = args.strategy
        chunk_size = args.chunk_size or 8000
        workers = args.workers or min(4, psutil.cpu_count())
        logger.info(f"Using manual strategy: {strategy}")
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã –≤—Ä—É—á–Ω—É—é
    if args.chunk_size:
        chunk_size = args.chunk_size
    if args.workers:
        workers = args.workers
    
    logger.info(f"Final parameters:")
    logger.info(f"  Strategy: {strategy}")
    logger.info(f"  Chunk size: {chunk_size}")
    logger.info(f"  Workers: {workers}")
    logger.info(f"  Threshold: {args.threshold}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    start_time = datetime.now()
    success = run_processing(input_file, csv_file, strategy, chunk_size, workers, args.threshold)
    processing_end_time = datetime.now()

    processing_duration = processing_end_time - start_time

    if success:
        logger.info(f"‚úÖ Data processing completed successfully!")
        logger.info(f"‚è±Ô∏è  Processing time: {processing_duration}")

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel
        metadata = {
            '–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏': start_time.strftime("%Y-%m-%d %H:%M:%S"),
            '–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏': str(processing_duration),
            '–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª': input_file,
            '–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏': strategy,
            '–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞': chunk_size,
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤': workers,
            '–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏': args.threshold,
            '–í–µ—Ä—Å–∏—è SAMe': '2.0'
        }

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel
        logger.info("üîÑ Converting results to Excel format...")
        excel_success = convert_csv_to_excel(csv_file, excel_file, metadata)

        end_time = datetime.now()
        total_duration = end_time - start_time

        if excel_success:
            logger.info(f"‚úÖ Full processing completed successfully!")
            logger.info(f"‚è±Ô∏è  Total time: {total_duration}")
            logger.info(f"üìÑ CSV results: {csv_file}")
            logger.info(f"üìä Excel results: {excel_file}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if os.path.exists(csv_file):
                try:
                    results_df = pd.read_csv(csv_file)
                    logger.info(f"üìä Results summary:")
                    logger.info(f"   Total relationships found: {len(results_df)}")

                    if 'Relation_Type' in results_df.columns:
                        relation_counts = results_df['Relation_Type'].value_counts()
                        for relation_type, count in relation_counts.items():
                            logger.info(f"   {relation_type}: {count}")

                    if 'Similarity_Score' in results_df.columns:
                        logger.info(f"   Average similarity: {results_df['Similarity_Score'].mean():.3f}")
                        logger.info(f"   Max similarity: {results_df['Similarity_Score'].max():.3f}")

                except Exception as e:
                    logger.warning(f"Could not read results for statistics: {e}")

            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π CSV —Ñ–∞–π–ª –µ—Å–ª–∏ Excel —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ –∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ keep-csv
            if not args.keep_csv:
                try:
                    if os.path.exists(csv_file) and csv_file != excel_file:
                        os.remove(csv_file)
                        logger.info(f"üóëÔ∏è  Removed intermediate CSV file: {csv_file}")
                except Exception as e:
                    logger.warning(f"Could not remove CSV file: {e}")
            else:
                logger.info(f"üìÑ CSV file kept: {csv_file}")

            return 0
        else:
            logger.warning(f"‚ö†Ô∏è  Processing completed but Excel conversion failed")
            logger.info(f"üìÑ CSV results available: {csv_file}")
            return 0
    else:
        logger.error(f"‚ùå Processing failed!")
        return 1


if __name__ == "__main__":
    exit(main())
