#!/usr/bin/env python3
"""
–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é –≤ DBSCAN
"""

import pandas as pd
import numpy as np
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
import re
from collections import Counter
import gc
import psutil
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def jaccard_similarity(text1: str, text2: str) -> float:
    """–ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ñ–∞–∫–∫–∞—Ä–∞"""
    if not text1 or not text2:
        return 0.0
    
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0


def determine_relation_type(similarity: float) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"""
    if similarity >= 0.9:
        return "–¥—É–±–ª—å"
    elif similarity >= 0.7:
        return "–∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.6:
        return "–±–ª–∏–∑–∫–∏–π –∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.4:
        return "–≤–æ–∑–º–æ–∂–Ω—ã–π –∞–Ω–∞–ª–æ–≥"
    elif similarity >= 0.2:
        return "–ø–æ—Ö–æ–∂–∏–π —Ç–æ–≤–∞—Ä"
    else:
        return "–Ω–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤"


def create_word_index(df: pd.DataFrame, text_column: str) -> Dict[str, List[int]]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    logger.info("Creating word index for fast search...")
    
    word_index = {}
    
    for idx, row in df.iterrows():
        text = normalize_text_fast(str(row.get(text_column, '')))
        words = text.split()
        
        for word in words:
            if word not in word_index:
                word_index[word] = []
            word_index[word].append(idx)
    
    logger.info(f"Word index created with {len(word_index)} unique words")
    return word_index


def find_candidates_fast(query_text: str, word_index: Dict[str, List[int]], 
                        df: pd.DataFrame, max_candidates: int = 100) -> List[int]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
    query_words = normalize_text_fast(query_text).split()
    
    if not query_words:
        return []
    
    # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    candidate_scores = Counter()
    
    for word in query_words:
        if word in word_index:
            for idx in word_index[word]:
                candidate_scores[idx] += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤
    candidates = [idx for idx, score in candidate_scores.most_common(max_candidates)]
    
    return candidates


def emergency_search(input_file: str, output_file: str, similarity_threshold: float = 0.4, 
                    max_records: int = None, batch_size: int = 1000):
    """–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –±–µ–∑ DBSCAN"""
    
    logger.info(f"üö® Emergency search started for: {input_file}")
    logger.info(f"Similarity threshold: {similarity_threshold}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("Loading data...")
    df = pd.read_csv(input_file)
    
    if max_records:
        df = df.head(max_records)
        logger.info(f"Limited to {max_records} records")
    
    logger.info(f"Loaded {len(df)} records")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –ø–æ–∏—Å–∫–∞
    text_column = 'Normalized_Name' if 'Normalized_Name' in df.columns else 'Raw_Name'
    logger.info(f"Using column: {text_column}")
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    word_index = create_word_index(df, text_column)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_results = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
    total_batches = (len(df) + batch_size - 1) // batch_size
    
    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, len(df))
        
        logger.info(f"Processing batch {batch_num + 1}/{total_batches}: records {start_idx}-{end_idx-1}")
        
        batch_results = []
        
        for idx in range(start_idx, end_idx):
            query_row = df.iloc[idx]
            query_text = str(query_row.get(text_column, ''))
            
            if not query_text or query_text == 'nan':
                continue
            
            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidates = find_candidates_fast(query_text, word_index, df, max_candidates=50)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            for candidate_idx in candidates:
                if candidate_idx == idx:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º —Ç–æ–≤–∞—Ä
                    continue
                
                candidate_row = df.iloc[candidate_idx]
                candidate_text = str(candidate_row.get(text_column, ''))
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
                similarity = jaccard_similarity(
                    normalize_text_fast(query_text),
                    normalize_text_fast(candidate_text)
                )
                
                if similarity >= similarity_threshold:
                    relation_type = determine_relation_type(similarity)
                    
                    result = {
                        '–ö–æ–¥': query_row.get('–ö–æ–¥', ''),
                        'Raw_Name': query_row.get('Raw_Name', ''),
                        'Candidate_Name': candidate_row.get('Raw_Name', ''),
                        'Similarity_Score': round(similarity, 4),
                        'Relation_Type': relation_type,
                        'Suggested_Category': candidate_row.get('–ì—Ä—É–ø–ø–∞', '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'),
                        'Final_Decision': '',
                        'Comment': f"–ñ–∞–∫–∫–∞—Ä —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}; –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫",
                        'Original_Category': query_row.get('–ì—Ä—É–ø–ø–∞', ''),
                        'Candidate_–ö–æ–¥': candidate_row.get('–ö–æ–¥', ''),
                        'Original_Code': query_row.get('–ö–æ–¥', ''),
                        'Search_Engine': 'EmergencySearch'
                    }
                    
                    batch_results.append(result)
        
        all_results.extend(batch_results)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        memory_gb = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 3)
        logger.info(f"Memory usage: {memory_gb:.1f}GB, Found {len(batch_results)} relationships in batch")
        
        if memory_gb > 6.0:  # –ï—Å–ª–∏ –ø–∞–º—è—Ç—å –±–æ–ª—å—à–µ 6GB, –¥–µ–ª–∞–µ–º —Å–±–æ—Ä–∫—É –º—É—Å–æ—Ä–∞
            gc.collect()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info(f"Saving {len(all_results)} results to {output_file}")
    
    if all_results:
        results_df = pd.DataFrame(all_results)
        results_df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"‚úÖ Results saved to: {output_file}")
    else:
        logger.warning("No results found!")
    
    logger.info(f"üéâ Emergency search completed! Found {len(all_results)} analog relationships")
    
    return output_file


def main():
    parser = argparse.ArgumentParser(description='Emergency analog search for large datasets')
    parser.add_argument('input_file', help='Input CSV file')
    parser.add_argument('-o', '--output', help='Output CSV file')
    parser.add_argument('-t', '--threshold', type=float, default=0.4, help='Similarity threshold')
    parser.add_argument('-l', '--limit', type=int, help='Limit number of records to process')
    parser.add_argument('-b', '--batch-size', type=int, default=1000, help='Batch size for processing')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    if args.output:
        output_file = args.output
    else:
        input_path = Path(args.input_file)
        output_file = f"emergency_search_results_{input_path.stem}.csv"
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
    emergency_search(
        input_file=args.input_file,
        output_file=output_file,
        similarity_threshold=args.threshold,
        max_records=args.limit,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()
